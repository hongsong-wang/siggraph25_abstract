<!DOCTYPE html>
<html>
<head>
<title>Siggraph 2025 collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br></p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>1
</span></div><div id = 'author'>Authors:<span id = 'author'>Lanjiong Li,Guanhua Zhao,Lingting Zhu,Zeyu Cai,Lequan Yu,Jian Zhang,Zeyu Wang
</span></div><div id = 'affiliation'><span id = 'affiliation'>The Hong Kong University of Science and Technology (Guangzhou),School of Electronic and Computer Engineering, Peking University,The University of Hong Kong,The Hong Kong University of Science and Technology (Guangzhou),The University of Hong Kong,School of Electronic and Computer Engineering, Peking University,The Hong Kong University of Science and Technology (Guangzhou)
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1060&sess=sess102">AssetDropper: Asset Extraction via Diffusion Models with Reward-Driven Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AssetDropper is a novel framework for extracting standardized assets from reference images, addressing challenges such as occlusion and distortion. Leveraging both synthetic and real-world datasets, along with a reward-driven feedback mechanism, it achieves state-of-the-art performance in asset extraction and provides designers with a versatile open-world asset palette.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>2
</span></div><div id = 'author'>Authors:<span id = 'author'>Jiaju Ma,Maneesh Agrawala
</span></div><div id = 'affiliation'><span id = 'affiliation'>Stanford University,Stanford University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1116&sess=sess102">MoVer: Motion Verification for Motion Graphics Animations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large vision-language models often fail to capture spatio-temporal details in text-to-animation tasks. We introduce MoVer, a verification system using first-order logic to check properties like timing and positioning in motion graphics animations. Integrated into an LLM pipeline, MoVer enables iterative refinement, significantly improving animation generation accuracy from 58.8% to 93.6%.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>3
</span></div><div id = 'author'>Authors:<span id = 'author'>Marcelo Sandoval-Castañeda,Bryan Russell,Josef Sivic,Gregory Shakhnarovich,Fabian David Caba Heilbron
</span></div><div id = 'affiliation'><span id = 'affiliation'>TTIC,Adobe Research,Adobe Research,TTIC,Adobe Research
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1476&sess=sess102">EditDuet: A Multi-Agent System for Video Non-Linear Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We automate video nonlinear editing using a multi-agent system. An Editor agent uses tools to create sequences from clips and instructions, while a Critic agent provides feedback in natural language. Our learning-based approach enhances agent communication. Evaluations with an LLM-as-a-judge metric and user studies show our system’s superior performance.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>4
</span></div><div id = 'author'>Authors:<span id = 'author'>Bo Yang,Ying Cao
</span></div><div id = 'affiliation'><span id = 'affiliation'>ShanghaiTech University,ShanghaiTech University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_531&sess=sess102">Order Matters: Learning Element Ordering for Graphic Design Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a Generative Order Learner (GOL) that optimizes element ordering for graphic design generation. Our approach learns a content-aware neural order, which can significantly improve graphic generation quality, generalize across different types of generative models and help design generators scale up greatly.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>5
</span></div><div id = 'author'>Authors:<span id = 'author'>Weitao You,Yinyu Lu,Zirui Ma,Nan Li,Mingxu Zhou,Xue Zhao,Pei Chen,Lingyun Sun
</span></div><div id = 'affiliation'><span id = 'affiliation'>Zhejiang University,Zhejiang University,Zhejiang University,Zhejiang University,Zhejiang University,Zhejiang University,Zhejiang University,Zhejiang University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1076&sess=sess102">DesignManager: An Agent-Powered Copilot for Designers to Integrate AI Design Tools into Creative Workflows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DesignManager is an AI-powered design support system that functions as an interactive copilot throughout the creative workflow. With node-based visualization of design evolution and conversational interaction modes, it helps designers track, modify, and branch their processes while providing context-aware assistance through an innovative agent framework.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>6
</span></div><div id = 'author'>Authors:<span id = 'author'>Xinrui Liu,Longxiulin Deng,Abe Davis
</span></div><div id = 'affiliation'><span id = 'affiliation'>Cornell University,Cornell University,Cornell University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1534&sess=sess102">Hybrid Tours: A Clip-based System for Authoring Long-take Touring Shots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Hybrid Tours, a hybrid approach to creating long-take shots by combining short video clips in a virtual interface. We show that Hybrid Tours makes capturing long-take touring shots much easier, and that clip-based authoring and reconstruction lead to higher-fidelity results at lower compute costs.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>7
</span></div><div id = 'author'>Authors:<span id = 'author'>Pascal Guehl,Rémi Allègre,Guillaume Gilet,Basile Sauvage,Marie-Paule Cani,Jean-Michel Dischler
</span></div><div id = 'affiliation'><span id = 'affiliation'>LIX - Ecole Polytechnique/CNRS,ICube, Université de Strasbourg,Université de Sherbrooke,ICube, Université de Strasbourg,LIX - Ecole Polytechnique/CNRS,ICube, Université de Strasbourg
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1206&sess=sess121">Multi-Dimensional Procedural Wave Noise</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a fast, wave-based procedural noise model enabling precise spectral control in any dimension. Using precomputed wave functions and inverse Fourier transforms, it supports Gaussian and non-Gaussian noises—including Gabor, Phasor, and novel recursive cellular patterns—making it ideal for compact, controllable, and animated solid textures in 2D, 3D, and time.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>8
</span></div><div id = 'author'>Authors:<span id = 'author'>Gilles Daviet,Tianchang Shen,Nicholas Sharp,David Levin,Gilles Daviet
</span></div><div id = 'affiliation'><span id = 'affiliation'>NVIDIA Corp,NVIDIA Corp,NVIDIA Corp,NVIDIA Corp,NVIDIA
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_104&sess=sess121">Neurally Integrated Finite Elements for Differentiable Elasticity on Evolving Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We train a network to map signed distance fields to the quadrature points and weights of non-conforming numerical integration rule in a Mixed Finite Element formulation, enabling differentiable elastic simulation over evolving domains. We demonstrate applications to image-guided material and topology optimization.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>9
</span></div><div id = 'author'>Authors:<span id = 'author'>Jiong Chen,Florian Schäfer,Mathieu DESBRUN
</span></div><div id = 'affiliation'><span id = 'affiliation'>INRIA Saclay,Georgia Institute of Technology,INRIA Saclay
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_944&sess=sess121">Lightning-fast Boundary Element Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce an inverse-LU preconditioner to solve for the typical asymmetric and dense matrices generated by boundary element methods (BEM). The computational efficiency and low memory requirements of our approach conspire to scale up to millions of degrees of freedom, with orders of magnitude speedups in solving times.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>10
</span></div><div id = 'author'>Authors:<span id = 'author'>Bailey Miller,Rohan Sawhney,Keenan Crane,Ioannis Gkioulekas
</span></div><div id = 'affiliation'><span id = 'affiliation'>Carnegie Mellon University,NVIDIA,Carnegie Mellon University,Carnegie Mellon University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_271&sess=sess121">Monte Carlo PDE simulation in participating media</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We solve partial differential equations in domains involving complex microparticle geometry that is impractical, or intractable, to model explicitly. Drawing inspiration from volume rendering, we treat the domain as a participating medium with stochastic microparticle geometry and develop a volumetric variant of the Monte Carlo walk on spheres algorithm.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>11
</span></div><div id = 'author'>Authors:<span id = 'author'>Paul Himmler,Tobias Günther
</span></div><div id = 'affiliation'><span id = 'affiliation'>Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU),Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU)
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1475&sess=sess121">Conformal First Passage for Epsilon-free Walk-on-Spheres</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel Monte Carlo approach to solve boundary integral equations with Dirichlet boundary conditions in two dimensions. While Walk-on-Spheres uses largest empty circles, which touch the boundary in only one point, we utilize semicircles and circle sectors that share one or two boundary edges resulting in shorter walks.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>12
</span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Huang,Jingwang Ling,Shuang Zhao,Feng Xu
</span></div><div id = 'affiliation'><span id = 'affiliation'>School of Software and BNRist, Tsinghua University,School of Software and BNRist, Tsinghua University,University of California Irvine,School of Software and BNRist, Tsinghua University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_138&sess=sess121">Guiding-Based Importance Sampling for Walk on Stars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Walk on stars (WoSt) has shown its power in being applied to Monte Carlo methods for solving PDEs but the sampling techniques in WoSt are not satisfactory, leading to high variance. Inspired by Monte Carlo rendering, we propose a guiding-based importance sampling method to reduce the variance of WoSt.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>13
</span></div><div id = 'author'>Authors:<span id = 'author'>Luozhou Wang,Ziyang Mai,Guibao Shen,Yixun Liang,Xin Tao,Pengfei Wan,Di Zhang,Yijun Li,Yingcong Chen
</span></div><div id = 'affiliation'><span id = 'affiliation'>Hong Kong University of Science and Technology, Guangzhou,Hong Kong University of Science and Technology, Guangzhou,Hong Kong University of Science and Technology, Guangzhou,Hong Kong University of Science and Technology,Kuaishou Technology,Kuaishou Technology,Kuaishou Technology,Adobe Research,Hong Kong University of Science and Technology, Guangzhou
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1099&sess=sess148">Motion Inversion for Video Customization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Motion Embeddings for video generation, enabling precise motion in video transfer across diverse scenes and objects. These embeddings disentangle motion from appearance, preserving original dynamics while adapting to new prompts. Experiments show that our method achieved high-quality, prompt-aligned video generation across a wide range of scenarios.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>14
</span></div><div id = 'author'>Authors:<span id = 'author'>Zinuo You,Stamatios Georgoulis,Anpei Chen,Siyu Tang,Dengxin Dai
</span></div><div id = 'affiliation'><span id = 'affiliation'>ETH Zürich,Huawei Research Zürich,ETH Zürich,ETH Zürich,Huawei Research Zürich
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1397&sess=sess148">GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>GaVS: Transform unstable shaky videos into smooth, professional-quality footage. We design novel 3D rednering technology that preserves the motion intent while eliminating shakes and distortions—no cropping, no distortion and workable under dynamics and intense motions. GaVS delivers natural-looking results validated by users as superiority. Capture life steadily!
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>15
</span></div><div id = 'author'>Authors:<span id = 'author'>Or Patashnik,Rinon Gal,Daniil Ostashev,Sergey Tulyakov,Kfir Aberman,Daniel Cohen-Or
</span></div><div id = 'affiliation'><span id = 'affiliation'>Tel Aviv University,Tel Aviv University,Snap,Snap,Snap,Tel Aviv University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_362&sess=sess148">Nested Attention: Semantic-aware Attention Values for Concept Personalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces Nested Attention, a mechanism that improves text-to-image personalization by injecting query-dependent subject features into cross-attention layers, achieving strong identity preservation and prompt alignment. The method maintains the model’s prior, enabling multi-subject generation across diverse domains.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>16
</span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Garibi,Shahar Yadin,Roni Paiss,Omer Tov,Shiran Zada,Ariel Ephrat,Tomer Michaeli,Inbar Mosseri,Tali Dekel
</span></div><div id = 'affiliation'><span id = 'affiliation'>Tel Aviv University,Technion - Israel Institute of Technology,DeepMind,DeepMind,DeepMind,DeepMind,Technion - Israel Institute of Technology,DeepMind,Weizmann Institute of Science
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_396&sess=sess148">TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>TokenVerse extracts complex visual elements from images by identifying semantic directions in per-token modulation space of DiT models for each word in the image caption. It's capable of combining concepts from multiple sources by adding corresponding directions, enabling flexible generation of new combinations including abstract concepts like lighting and poses.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>17
</span></div><div id = 'author'>Authors:<span id = 'author'>Rameen Abdal,Or Patashnik,Ivan Skorokhodov,Willi Menapace,Aliaksandr Siarohin,Sergey Tulyakov,Daniel Cohen-Or,Kfir Aberman
</span></div><div id = 'affiliation'><span id = 'affiliation'>Snap,Snap,Snap,Snap,Snap,Snap,Snap,Snap
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_415&sess=sess148">Dynamic Concepts Personalization from Single Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Personalizing text-to-video models is challenging because dynamic concepts require capturing both appearance and motion. We propose Set-and-Sequence, a framework that personalizes DiT-based video models by first learning an identity LoRA basis from unordered frames, then fine-tuning coefficients with motion residuals on full videos, enabling superior editability and compositionality for applications.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>18
</span></div><div id = 'author'>Authors:<span id = 'author'>Shiyi Zhang,Junhao Zhuang,Zhaoyang Zhang,Ying Shan,Yansong Tang
</span></div><div id = 'affiliation'><span id = 'affiliation'>Tsinghua University,Tencent,Tencent,Tencent,Tsinghua University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_685&sess=sess148">FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose FlexiAct, an image animation framework that transfers actions from a reference video to any target image, enabling variations in layout, viewpoint, and skeletal structure while maintaining identity consistency.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>19
</span></div><div id = 'author'>Authors:<span id = 'author'>Bernhard Braun,Jan Bender,Nils Thuerey
</span></div><div id = 'affiliation'><span id = 'affiliation'>Technical University Munich,RWTH Aachen University,Technical University Munich
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_516&sess=sess138">Adaptive Phase-Field-FLIP for Very Large Scale Two-Phase Fluid Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an algorithm for simulating large-scale, violently turbulent two-phase flows—such as breaking ocean waves, tsunamis, and asteroid impacts—at extreme resolutions of the coupled water-air velocity field. This is achieved by integrating a new multiphase FLIP variant with highly efficient dual particle–grid adaptivity and a novel adaptive Poisson solver.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>20
</span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Chen,Yixin Chen,Jonathan Panuelos,Otman Benchekroun,Yue Chang,Eitan Grinspun,Zhecheng Wang
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Toronto,University of Toronto,University of Toronto,University of Toronto,University of Toronto,University of Toronto,University of Toronto
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_179&sess=sess138">Fast Subspace Fluid Simulation with a Temporally-Aware Basis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel reduced-order fluid simulation technique leveraging Dynamic Mode Decomposition (DMD) to enable fast, memory-efficient, and user-controllable subspace simulation. Combining spatial ROM compression with spectral physical insights, our method excels in animation, real-time interaction, artistic control, and time-reversible fluid effects.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>21
</span></div><div id = 'author'>Authors:<span id = 'author'>Jingrui Xing,Bin Wang,Mengyu Chu,Baoquan Chen
</span></div><div id = 'affiliation'><span id = 'affiliation'>School of Intelligence Science and Technology, Peking University,Independent,Peking University,Peking University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_290&sess=sess138">Gaussian Fluids: A Grid-Free Fluid Solver based on Gaussian Spatial Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a grid-free fluid simulator featuring a novel Gaussian spatial representation (GSR) for velocity field. The advantages of GSR over traditional Lagrangian/Eulerian data structures are 4-folded: memory compactness, spatial adaptivity, vorticity preservation and continuous differentiability. Our method also greatly outperforms similar competitors in terms of quality and performance.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>22
</span></div><div id = 'author'>Authors:<span id = 'author'>Fumiya Narita,Nimiko Ochiai,Takashi Kanai,Ryoichi Ando
</span></div><div id = 'affiliation'><span id = 'affiliation'>The University of Tokyo,GAME FREAK Inc.,The University of Tokyo,Unaffiliated
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_485&sess=sess138">Quadtree Tall Cells for Eulerian Liquid Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel grid structure that extends tall cell methods for efficient deep water simulation. Unlike previous methods, our approach subdivides tall cells horizontally, allowing for more aggressive adaptivity. We demonstrate that this novel form of adaptivity delivers superior performance compared to traditional uniform tall cells.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>23
</span></div><div id = 'author'>Authors:<span id = 'author'>Duowen Chen,Junwei Zhou,Bo Zhu,Duowen Chen
</span></div><div id = 'affiliation'><span id = 'affiliation'>Georgia Institute of Technology,University of Michigan,Georgia Institute of Technology,Georgia Institute of Technology
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_117&sess=sess138">A Neural Particle Level Set Method for Dynamic Interface Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Neural PLS, a neural particle level-set method for tracking and evolving dynamic neural representations. Oriented particles serve as interface trackers and sampling seeders, enabling efficient evolution on a multi-resolution grid-hash structure. Our approach integrates traditional PLS and implicit neural representations, achieving superior performance in benchmarks and physical simulations.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>24
</span></div><div id = 'author'>Authors:<span id = 'author'>Naoki Agata,Takeo Igarashi
</span></div><div id = 'affiliation'><span id = 'affiliation'>The University of Tokyo,The University of Tokyo
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_579&sess=sess143">Motion Control via Metric-Aligning Motion Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metric-Aligning Motion Matching (MAMM) is a novel method for controlling motion sequences using sketches, labels, audio, or another motion sequence without requiring training or annotations. By aligning within-domain distances, MAMM provides a flexible and efficient solution for motion control across various control modalities.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>25
</span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Zheng,Ke Chen,Yuxin Yao,Zijiao Zeng,Xinwei Jiang,He Wang,Joan Lasenby,Xiaogang Jin
</span></div><div id = 'affiliation'><span id = 'affiliation'>Zhejiang University,Zhejiang University,University of Cambridge,Tencent Games Digital Content Technology Center,Tencent Games Digital Content Technology Center,UCL Centre for Artificial Intelligence, Department of Computer Science, University College London,University of Cambridge,Zhejiang University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_578&sess=sess143">AutoKeyframe: Autoregressive Keyframe Generation for Human Motion Synthesis and Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present AutoKeyframe, a novel framework that simultaneously accepts dense and sparse control signals for motion generation by generating keyframes directly. Our method reduces manual efforts for keyframing while maintaining precise controllability, using an autoregressive diffusion model and a new skeleton-based gradient guidance method for flexible spatial constraints.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>26
</span></div><div id = 'author'>Authors:<span id = 'author'>Arjun Lakshmipathy,Jessica Hodgins,Nancy Pollard,Arjun Lakshmipathy
</span></div><div id = 'affiliation'><span id = 'affiliation'>Carnegie Mellon University,Carnegie Mellon University,Carnegie Mellon University,Carnegie Mellon University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_113&sess=sess143">Kinematic Motion Retargeting for Contact-Rich Anthropomorphic Manipulations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a simple, but effective framework for kinematically retargeting contact-rich anthropomorphic hand-object manipulations by exploiting contact areas. We reliably retarget contact area data between diverse hands using a novel non-isometric shape matching process and generate high quality results using the retargeted contacts alongside a straightforward IK-based motion synthesis pipeline.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>27
</span></div><div id = 'author'>Authors:<span id = 'author'>Inbar Gat,Sigal Raab,Guy Tevet,Yuval Reshef,Amit Haim Bermano,Daniel Cohen-Or
</span></div><div id = 'affiliation'><span id = 'affiliation'>Tel Aviv University,Tel Aviv University,Tel Aviv University,Tel Aviv University,Tel Aviv University,Tel Aviv University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_292&sess=sess143">AnyTop: Character Animation Diffusion with Any Topology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AnyTop generates motion for diverse character skeletons using only skeletal structure as input. This diffusion model incorporates topology information and textual joint descriptions to learn semantic correspondences across different skeletons. It generalizes with minimal training examples and supports joint correspondence, temporal segmentation, and motion editing tasks.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>28
</span></div><div id = 'author'>Authors:<span id = 'author'>Kemeng Huang,Xinyu Lu,Huancheng Lin,Taku Komura,Minchen Li,Kemeng Huang
</span></div><div id = 'affiliation'><span id = 'affiliation'>Carnegie Mellon University,TransGP,Carnegie Mellon University,The University of Hong Kong,Carnegie Mellon University,University of Hong Kong, Carnegie Mellon University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_101&sess=sess143">Advancing GPU IPC for Stiff Affine-Deformable Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a GPU-optimized IPC framework achieving up to 10× speedup across soft, stiff, and hybrid simulations. Key innovations include a connectivity-enhanced MAS preconditioner, a parallel-friendly inexact strain limiting energy, and a hash-based two-level reduction strategy for fast Hes-sian assembly and efficient affine-deformable coupling.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>29
</span></div><div id = 'author'>Authors:<span id = 'author'>Lei Lan,Zixuan Lu,Chun Yuan,Weiwei Xu,Hao Su,Huamin Wang,Chenfanfu Jiang,Yin Yang
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Utah,University of Utah,University of Utah,State Key Lab of CAD&CG, Zhejiang University, China,UCSD,Style3D Research,UCLA,University of Utah
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_715&sess=sess143">JGS2: Near Second-order Converging Jacobi/Gauss-Seidel for GPU Elastodynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a new GPU simulation algorithm, which converges as fast as global Newton's method and as efficient as Jacobi method.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>30
</span></div><div id = 'author'>Authors:<span id = 'author'>Alvin Shi,Haomiao Wu,Theodore Kim
</span></div><div id = 'affiliation'><span id = 'affiliation'>Yale University,Yale University,Yale University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1058&sess=sess143">Hyper-Dimensional Deformation Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ever feel like three dimensions isn't quite enough? We performed the analysis necessary to simulate the motion of deformables in four spatial dimensions! Along the way, we developed techniques for generating simulation-ready hyper-meshes, analyzing hyper-dimensional deformation energies, and detecting and responding to collision scenarios for softbodies in any dimension.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>31
</span></div><div id = 'author'>Authors:<span id = 'author'>Chengxu Zuo,Jiawei Huang,Xiao Jiang,Yuan Yao,Xiangren Shi,Rui Cao,Xinyu Yi,Feng Xu,Shihui Guo,Yipeng Qin
</span></div><div id = 'affiliation'><span id = 'affiliation'>Xiamen University,Xiamen University,Xiamen University,Xiamen University,Bournemouth University,Xiamen University,Tsinghua University,Tsinghua University,Xiamen University,Cardiff University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1351&sess=sess139">Transformer IMU Calibrator: Dynamic On-body IMU Calibration for Inertial Motion Capture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes a dynamic calibration system for inertial motion capture, which can dynamically remove non-static IMU drift and sensor-body offset during usage, enable user-friendly calibration (without T-pose and IMU heading reset), and ensure long-term robustness.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>32
</span></div><div id = 'author'>Authors:<span id = 'author'>Rafael Wampfler,Chen Yang,Dillon Elste,Nikola Kovacevic,Philine Witzig,Markus Gross
</span></div><div id = 'affiliation'><span id = 'affiliation'>ETH Zürich,ETH Zürich,ETH Zürich,ETH Zürich,ETH Zürich,ETH Zürich
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1516&sess=sess139">A Platform for Interactive AI Character Experiences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a platform for creating believable, conversational digital characters that combine conversational AI, speech, animation, memory, personality, and emotions. Demonstrated through Digital Einstein, our system enables interactive, story-driven experiences and generalizes to any character, making immersive, AI-powered character experiences more accessible than ever.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>33
</span></div><div id = 'author'>Authors:<span id = 'author'>Fengqi LIU,Longji Huang,Zhengyu Huang,Zeyu Wang
</span></div><div id = 'affiliation'><span id = 'affiliation'>The Hong Kong University of Science and Technology (Guangzhou),The Hong Kong University of Science and Technology (Guangzhou),The Hong Kong University of Science and Technology (Guangzhou),The Hong Kong University of Science and Technology (Guangzhou)
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1090&sess=sess139">Learning to Draw Is Learning to See: Analyzing Eye Tracking Patterns for Assisted Observational Drawing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an image-to-image drawing setup capturing eye tracking and stroke data across 156 drawings from 10 artists. Our findings reveal consistent fixation patterns, strong gaze–stroke correlations, and structured drawing sequences, offering new insights into professional observation strategies and observation-guided assistive drawing system design.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>34
</span></div><div id = 'author'>Authors:<span id = 'author'>Zhiming Hu,Daniel Haeufle,Syn Schmitt,Andreas Bulling
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Stuttgart,University of Tuebingen,University of Stuttgart,University of Stuttgart
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_754&sess=sess139">HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended Reality Exploiting Eye-Hand-Head Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present HOIGaze – a novel approach for gaze estimation during hand-object interactions in extended reality. HOIGaze features: 1) a novel hierarchical framework that first recognises attended hand and then estimates gaze; 2) a new gaze estimator combining CNN, GCN, and cross-modal Transformers; and 3) a novel eye-head coordination loss.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>35
</span></div><div id = 'author'>Authors:<span id = 'author'>DongHeun Han,Byungmin Kim,RoUn Lee,KyeongMin Kim,Hyoseok Hwang,HyeongYeop Kang
</span></div><div id = 'affiliation'><span id = 'affiliation'>Kyung Hee University,Korea University,Kyung Hee University,Korea University,Kyung Hee University,Korea University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1125&sess=sess139">ForceGrip: Reference-Free Curriculum Learning for Realistic Grip Force Control in VR Hand Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>ForceGrip is a reference-free reinforcement learning-based agent for realistic VR hand manipulation. It uses a progressive curriculum (finger positioning, intention adaptation, dynamic stabilization) and physics simulation to convert VR controller inputs into faithful grip forces. In user studies, participants achieved higher realism and precise control than competitors, ensuring immersive interaction.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>36
</span></div><div id = 'author'>Authors:<span id = 'author'>Qingqin Liu,Ziqi Fang,Jiayi Wu,Shaoyu Cai,Jianhui Yan,Tiande Mo,Shuk Ching CHAN,Kening Zhu
</span></div><div id = 'affiliation'><span id = 'affiliation'>School of Creative Media, City University of Hong Kong,School of Creative Media, City University of Hong Kong,School of Creative Media, City University of Hong Kong,National University of Singapore,School of Creative Media, City University of Hong Kong,Hong Kong Productivity Council,Hong Kong Productivity Council,City University of Hong Kong
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_761&sess=sess139">VirCHEW Reality: On-Face Kinesthetic Feedback for Enhancing Food-Intake Experience in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents VirCHEW Reality, a face-worn haptic device for virtual food intake in VR. It uses pneumatic actuation to simulate food textures, enhancing the chewing experience. User studies demonstrated its effectiveness in providing distinct kinesthetic feedback and improving virtual eating experiences, with applications in dining, healthcare, and entertainment.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>37
</span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Violante,Andréas Meuleman,Alban Gauthier,Fredo Durand,Thibault Groueix,George Drettakis
</span></div><div id = 'affiliation'><span id = 'affiliation'>INRIA, Université Côte d'Azur,INRIA, Université Côte d'Azur,INRIA, Université Côte d'Azur,Massachusetts Institute of Technology (MIT),Adobe Research,INRIA, Université Côte d'Azur
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1022&sess=sess105">Splat and Replace: 3D Reconstruction with Repetitive Elements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We leverage repetitions in 3D scenes to improve reconstruction in low-quality parts due to poor coverage and occlusions. Our methods segments the repetitions, registers them together, and optimizes a shared representation with multi-view information flowing from all repetitions, improving the reconstruction of each individual repetition.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>38
</span></div><div id = 'author'>Authors:<span id = 'author'>Sara Sabour,Lily Goli,George Kopanas,Mark Matthews,Dmitry Lagun,Leonidas Guibas,Alec Jacobson,David Fleet,Andrea Tagliasacchi,Sara Sabour,Lily Goli
</span></div><div id = 'affiliation'><span id = 'affiliation'>Google Inc - Deepmind,University of Toronto,Runway,Google Inc - Deepmind,Google Inc - Deepmind,Google Inc - Deepmind,University of Toronto,Google Inc - Deepmind,Google Inc - Deepmind,Google Inc - Deepmind, University of Toronto,University of Toronto, Waabi AI
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_123&sess=sess105">SpotLessSplats: Ignoring Distractors in 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Gaussian Splatting (3DGS) enables fast 3D reconstruction and rendering but struggles with real-world captures due to transient elements and lighting changes. We introduce SpotLessSplats, which leverages semantic features from foundation models and robust optimization to remove transient effects, achieving state-of-the-art qualitative and quantitative reconstruction quality on casual scene captures.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>39
</span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Zhang,Nicolas Roussel,Thomas Muller,Tizian Zeltner,Merlin Nimier-David,Fabrice Rousselle,Wenzel Jakob
</span></div><div id = 'affiliation'><span id = 'affiliation'>EPFL,EPFL,NVIDIA Research,NVIDIA Research,NVIDIA Research,NVIDIA Research,EPFL
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_872&sess=sess105">Radiance Surfaces: Optimizing Surface Representations with a 5D Radiance Field Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a simple and fast method to reconstruct radiance surfaces by directly supervising the radiance field via image projection.Unlike volumetric approaches, we move alpha blending and ray marching from image formation into loss computation.This simple modification enables high-quality surface reconstruction while preserving baseline efficiency and robustness.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>40
</span></div><div id = 'author'>Authors:<span id = 'author'>Junkai Huang,Saswat Subhajyoti Mallick,Alejandro Amat,Marc Ruiz Olle,Albert Mosella-Montoro,Bernhard Kerbl,Francisco Vicente Carrasco,Fernando De la Torre
</span></div><div id = 'affiliation'><span id = 'affiliation'>Carnegie Mellon University,Carnegie Mellon University,Carnegie Mellon University,Carnegie Mellon University,Carnegie Mellon University,Carnegie Mellon University,Carnegie Mellon University,Carnegie Mellon University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1268&sess=sess105">Echoes of the Coliseum: Towards 3D Live streaming of Sports Events</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a revolutionary method for experiencing live sports in stunning 3D, redefining the way games are seen, through immersive, interactive replays. Alongside, we release a large-scale synthetic dataset built to benchmark realism, motion, and human interaction in dynamic scenes, to fuel the next wave of research in 3D streaming.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>41
</span></div><div id = 'author'>Authors:<span id = 'author'>Akshat Dave,Tianyi Zhang,Aaron Young,Ramesh Raskar,Wolfgang Heidrich,Ashok Veeraraghavan,Akshat Dave
</span></div><div id = 'affiliation'><span id = 'affiliation'>Massachusetts Institute of Technology Media Lab,Rice University,Massachusetts Institute of Technology Media Lab,Massachusetts Institute of Technology Media Lab,King Abdullah University of Science and Technology,Rice University,Massachusetts Institute of Technology (MIT)
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_111&sess=sess105">NeST: Neural Stress Tensor Tomography by leveraging 3D Photoelasticity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>NeST enables non-destructive 3D stress analysis of transparent objects using the polarization of light. Traditional 2D methods require destructively slicing the object. Instead, we reconstruct the entire 3D stress field by jointly handling phase unwrapping and tensor tomography using neural implicit representations and inverse rendering, enabling novel 3D stress visualizations.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>42
</span></div><div id = 'author'>Authors:<span id = 'author'>Peng Li,Zeyong Wei,Honghua Chen,Xuefeng Yan,Mingqiang Wei
</span></div><div id = 'affiliation'><span id = 'affiliation'>Nanjing University of Aeronautics and Astronautics,Nanjing University of Aeronautics and Astronautics,Nanjing University of Aeronautics and Astronautics,Nanjing University of Aeronautics and Astronautics,Nanjing University of Aeronautics and Astronautics
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_812&sess=sess104">Revisiting Tradition and Beyond: A Customized Bilateral Filtering Framework for Point Cloud Denoising</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To combine deep learning's generalization with traditional methods' interpretability, we propose CustomBF—a hybrid framework that customizes bilateral filter components per point. By addressing key limitations of the classic bilateral filter, CustomBF achieves robust, interpretable, and effective point cloud denoising across diverse scenarios.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>43
</span></div><div id = 'author'>Authors:<span id = 'author'>Nathan King,Haozhe Su,Mridul Aanjaneya,Steven Ruuth,Christopher Batty,Nathan King
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Waterloo,LightSpeed Studios,Rutgers University,Simon Fraser University,University of Waterloo,University of Waterloo
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_122&sess=sess104">A Closest Point Method for PDEs on Manifolds with Interior Boundary Conditions for Geometry Processing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Geometry processing often requires the solution of PDEs with boundary conditions on the manifold’s interior. However, input manifolds can take many forms, each requiring specialized discretizations. Instead, we develop a unified framework for general manifold representations by extending the closest point method to handle interior boundary conditions.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>44
</span></div><div id = 'author'>Authors:<span id = 'author'>Zhuodong Li,Fei Hou,Wencheng Wang,Xuquan Lu,Ying He
</span></div><div id = 'affiliation'><span id = 'affiliation'>Institute of Software, Chinese Academy of Sciences,Institute of Software, Chinese Academy of Sciences,Institute of Software, Chinese Academy of Sciences,The University of Western Australia,Nanyang Technological University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1124&sess=sess104">A Divide-and-Conquer Approach for Global Orientation of Non-Watertight Scene-Level Point Clouds with 0-1 Integer Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a divide-and-conquer approach for orienting large-scale, non-watertight point clouds. The scene is first segmented into blocks, and normal orientations are estimated independently within each block. These local orientations are then globally unified through a graph-based formulation, solved via 0-1 integer optimization. Experiments demonstrate the robustness of our method.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>45
</span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Scrivener,Daniel Cui,Ellis Coldren,Mazdak Abulnaga,Mikhail Bessmeltsev,Edward Chien
</span></div><div id = 'affiliation'><span id = 'affiliation'>Boston University,Boston University,Boston University,Massachusetts Institute of Technology (MIT),Universite de Montreal,Boston University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1241&sess=sess104">Faraday Cage Estimation of Normals for Point Clouds and Ribbon Sketches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel method for normal estimation of unoriented point clouds and VR ribbon sketches that leverages a modeling of the Faraday cage effect. Our method is uniquely robust to the presence of interior structures and artifacts, producing superior surfacing output when combined with Poisson Surface Reconstruction.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>46
</span></div><div id = 'author'>Authors:<span id = 'author'>Max Kohlbrenner,Marc Alexa
</span></div><div id = 'affiliation'><span id = 'affiliation'>Technical University of Berlin,Technical University of Berlin
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1267&sess=sess104">A Polyhedral Construction of Empty Spheres in Discrete Distance Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spheres that are disjoint from a given union of spheres can be computing by solving a convex hull problem. This can be exploited for contouring discretely sampled signed distance functions.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>47
</span></div><div id = 'author'>Authors:<span id = 'author'>Yucheol Jung,Hyomin Kim,Hyejeong Yoon,Yoonha Hwang,Seungyong Lee
</span></div><div id = 'affiliation'><span id = 'affiliation'>POSTECH,POSTECH,POSTECH,POSTECH,POSTECH
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_791&sess=sess104">Variable Shared Template for Consistent Non-rigid ICP</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel ICP framework that jointly optimizes a shared template and instance-wise deformations. Our approach automatically captures common shape features from input shapes, achieving state-of-the-art accuracy and consistency while eliminating the need to carefully select a preset template mesh.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>48
</span></div><div id = 'author'>Authors:<span id = 'author'>Diyang Zhang,Zhendong Wang,Zegao Liu,Xinming Pei,Weiwei Xu,Huamin Wang
</span></div><div id = 'affiliation'><span id = 'affiliation'>Style3D Research,Style3D Research,Style3D Research,State Key Laboratory of CAD & CG, Zhejiang University,State Key Laboratory of CAD & CG, Zhejiang University,Style3D Research
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_289&sess=sess151">Physics-inspired Estimation of Optimal Cloth Mesh Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a method to estimate optimal cloth mesh resolution based on material stiffness and boundary conditions like shirring or stitching, and dynamic wrinkles from motion-induced collisions. To ensure smooth resolution transitions, we calculate transition distances and generate a mesh sizing map, enhancing realism, efficiency, and versatility for garment design.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>49
</span></div><div id = 'author'>Authors:<span id = 'author'>Chengzhu He,Zhendong Wang,Zhaorui Meng,Junfeng Yao,Shihui Guo,Huamin Wang
</span></div><div id = 'affiliation'><span id = 'affiliation'>Xiamen University,Style3D Research,Xiamen University,Xiamen University,Xiamen University,Style3D Research
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_298&sess=sess151">Automated Task Scheduling for Cloth and Deformable Body Simulations in Heterogeneous Computing Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces an automated scheduling framework to optimize cloth and deformable simulations across heterogeneous computing devices. Using an enhanced HEFT algorithm and asynchronous iteration methods, our approach minimizes communication delays and maximizes parallelism. our experiments demonstrate superior frame rates over single-unit solutions for real-time and resource-constrained environments.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>50
</span></div><div id = 'author'>Authors:<span id = 'author'>Zixuan Lu,Ziheng Liu,Lei Lan,Huamin Wang,Yuko Ishiwaka,Chenfanfu Jiang,Kui Wu,Yin Yang
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Utah,University of Utah,University of Utah,Style3D Research,SoftBank,UCLA,LightSpeed Studios,University of Utah
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_714&sess=sess151">High-performance CPU Cloth Simulation Using Domain-decomposed Projective Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a CPU-based cloth simulation algorithm that partitions garment models into domains that can be processed by each individual CPU core. Using projective dynamics with domain-level parallelization, this method achieves high performance comparable to  GPU methods and runs about an order faster than existing CPU approaches.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>51
</span></div><div id = 'author'>Authors:<span id = 'author'>Tao Huang,Haoyang Shi,Mengdi Wang,Yuxing Qiu,Yin Yang,Kui Wu
</span></div><div id = 'affiliation'><span id = 'affiliation'>LightSpeed Studios,University of Utah,LightSpeed Studios,LightSpeed Studios,University of Utah,LightSpeed Studios
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_730&sess=sess151">Real-Time Knit Deformation and Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we introduce the first real-time framework that integrates yarn-level simulation with fiber-level rendering. The whole system provides real-time performance and has been evaluated through various application scenarios, including knit simulation for small patches and full garments and yarn-level relaxation in the design pipeline.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>52
</span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Eris Zhang,Doug James,Danny Kaufman
</span></div><div id = 'affiliation'><span id = 'affiliation'>Stanford University,Stanford University,Adobe
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1019&sess=sess151">Progressive Dynamics++: A Framework for Stable, Continuous, and Consistent Animation Across Resolution and Time</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a general framework, Progressive Dynamics++, for constructing a family of progressive dynamics integration methods that advance physical simulation states forward in both time and spatial resolution. We analyze requirements for stable, continuous, and consistent level-of-detail animation and introduce a novel, stable method that significantly improves temporal continuity.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>53
</span></div><div id = 'author'>Authors:<span id = 'author'>Yue Chang,Mengfei Liu,Zhecheng Wang,Peter Yichen Chen,Eitan Grinspun
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Toronto,University of Toronto,University of Toronto,MIT CSAIL,University of Toronto
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_172&sess=sess151">Lifting the Winding Number: Precise Discontinuities in Neural Fields for Physics Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We designed a neural field capable of capturing a diverse family of discontinuities, enabling the simulation of cuts in thin-walled deformable structures. By lifting input coordinates using generalized winding numbers, our approach models discontinuities explicitly and flexibly, supporting accurate, real-time simulations with dynamic cut updates and user-interactive cut shape design.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>54
</span></div><div id = 'author'>Authors:<span id = 'author'>Minseok Chae,Chun Chen,Seung-Woo Nam,Yoonchan Jeong
</span></div><div id = 'affiliation'><span id = 'affiliation'>Seoul National University,Seoul National University,Seoul National University,Seoul National University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_843&sess=sess159">Light Pipe Holographic Display: Bandwidth-preserved Kaleidoscopic Guiding for AR Glasses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present and analyze a holographic augmented reality display with the bandwidth-preserved guiding method using a light pipe. We propose the use of light pipe to spatially relocate the light engine from the image combiner at the front-module, enabling enhanced weight distribution and obstruction-free view while preserving the wavefront bandwidth.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>55
</span></div><div id = 'author'>Authors:<span id = 'author'>Florian Schiffers,Grace Kuo,Nathan Matsuda,Douglas Lanman,Oliver Cossairt,Florian Schiffers,Oliver Cossairt
</span></div><div id = 'affiliation'><span id = 'affiliation'>Amazon Prime Video,Meta Reality Labs,Meta Reality Labs,Meta Reality Labs,Meta Reality Labs,Northwestern University; Reality Labs Research, Meta,Northwestern University, Facebook
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_103&sess=sess159">HoloChrome: Polychromatic Illumination for Speckle Reduction in Holographic Near-Eye Displays</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>HoloChrome introduces a novel holographic display method by multiplexing multiple wavelengths and two spatial light modulators to enhance image quality. By moving beyond standard three-color primary systems, it significantly reduces speckle noise while preserving natural depth cues while achieving more accurate color reproduction.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>56
</span></div><div id = 'author'>Authors:<span id = 'author'>Peter Michael,Zekun Hao,Serge Belongie,Abe Davis,Peter Michael
</span></div><div id = 'affiliation'><span id = 'affiliation'>Cornell University,Cornell Tech,University of Copenhagen,Cornell University,Cornell University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_124&sess=sess159">Noise-Coded Illumination for Forensic and Photometric Video Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video forensics, which focuses on identifying fake or manipulated video, is becoming increasingly difficult with the development of more advanced video editing techniques. We show how coding near-imperceptible, noise-like modulations into the illumination of a scene can create information asymmetry that favors forensic verification of video captured from that scene.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>57
</span></div><div id = 'author'>Authors:<span id = 'author'>Jipeng Sun,Kaixuan Wei,Thomas Eboli,Congli Wang,Cheng Zheng,Zhihao Zhou,Arka Majumdar,Wolfgang Heidrich,Felix Heide
</span></div><div id = 'affiliation'><span id = 'affiliation'>Princeton University,King Abdullah University of Science and Technology (KAUST),Université Paris-Saclay,Princeton University,Princeton University,University of Washington,University of Washington,King Abdullah University of Science and Technology (KAUST),Princeton University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_986&sess=sess159">Collaborative On-Sensor Array Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a collaborative metalens array comprising over 100-million nanopillars for broadband imaging. The proposed array camera is only a few millimeters flat and employs a non-generative reconstruction method, which performs favorably and without hallucinations, irrespective of the scene illumination spectrum.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>58
</span></div><div id = 'author'>Authors:<span id = 'author'>Juhyeon Kim,Craig Benko,Magnus Wrenninge,Ryusuke Villemin,Zeb Barber,Wojciech Jarosz,Adithya Pediredla
</span></div><div id = 'affiliation'><span id = 'affiliation'>Dartmouth College,Aurora Innovation,Aurora Innovation,Aurora Innovation,Aurora Innovation,Dartmouth College,Dartmouth College
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_247&sess=sess159">A Monte Carlo Rendering Framework for Simulating Optical Heterodyne Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a general spectral-domain simulation framework for optical heterodyne detection (OHD), extending path integral rendering to capture power spectral density of OHD. Unlike existing domain-specific tools, our approach supports diverse scenes and applications. We validate it against real-world data from FMCW lidar, blood flow velocimetry, and wind Doppler lidar.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>59
</span></div><div id = 'author'>Authors:<span id = 'author'>Suyeon Choi,Brian Chao,Jacqueline Yang,Manu Gopakumar,Gordon Wetzstein
</span></div><div id = 'affiliation'><span id = 'affiliation'>Stanford University,Stanford University,Stanford University,Stanford University,Stanford University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_525&sess=sess159">Gaussian Wave Splatting for Computer-Generated Holography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We develop novel and efficient computer-generated holography algorithms, dubbed Gaussian Wave Splatting, that transform Gaussian-based scene representations into holograms. We derive a closed-form 2D Gaussian-to-hologram transform supporting occlusions and alpha blending, along with an efficient, easily parallelizable Fourier-domain approximation of this process, implemented with custom CUDA kernels.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>60
</span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Ma,Hengyu Meng,Ziwei Wu,Zeyu Wang,Clea von Chamier-Waite
</span></div><div id = 'affiliation'><span id = 'affiliation'>The Hong Kong University of Science and Technology (Guangzhou),The Hong Kong University of Science and Technology (Guangzhou),The Hong Kong University of Science and Technology,The Hong Kong University of Science and Technology (Guangzhou),The Hong Kong University of Science and Technology (Guangzhou)
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=artpl_132&sess=sess107">Becoming Space: Exploring Agential Materiality Through AI-Generated Metamorphosis in Artistic Practice</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>"Becoming Space" is an installation that explores the agency of AI, discourse, and material intersections through AI-generated forms and 3D printing. Inspired by Ovid's Metamorphoses, it explores human-animal transformations using CLIP-guided diffusion models and stereolithography. The installation reveals limitations in AI's physical form interpretation---which is dominated by discourse---while demonstrating "intra-action" between language, algorithms, machines, and materials. This work tries to discuss authorship and material agency through the entanglement of matters.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>61
</span></div><div id = 'author'>Authors:<span id = 'author'>Troy TianYu LIN,Boyan Zheng,Haichuan Lin,Wen You,Kang Zhang,Chen Liang
</span></div><div id = 'affiliation'><span id = 'affiliation'>Hong Kong University of Science and Technology, Guangzhou,Independent Researcher,Hong Kong University of Science and Technology, Guangzhou,Hong Kong University of Science and Technology, Guangzhou,Hong Kong University of Science and Technology, Guangzhou,Hong Kong University of Science and Technology, Guangzhou
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=artpl_179&sess=sess107">Floating Strokes: A Spatial Interpretation and Modeling Method of Chinese Calligraphy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transforming 2D Chinese calligraphy into 3D forms deepens how traditional art is understood and experienced, combining cultural heritage with modern technology. This approach adds spatial depth, opening new possibilities for digital art, preservation, and interactive design.By merging computational modeling with artistic expression, the work explores how technology can reinterpret historical artforms, encouraging cross-disciplinary dialogue and inspiring new ways to preserve and evolve intangible cultural heritage.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>62
</span></div><div id = 'author'>Authors:<span id = 'author'>Tatuki Hayama,Kotaro Uchibe
</span></div><div id = 'affiliation'><span id = 'affiliation'>Keio University,Hosoo Co.,Ltd
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=artpl_180&sess=sess107">wave2weave: A Procedural Weave Data Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a procedural data generation method for Jacquard weaving that uses matrix computations to create textiles with complex shaded patterns and a triple-layer structure. Employing this method, the authors creatively applied noise functions to weave design and produced a textile artwork in collaboration with a traditional craft technique.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>63
</span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Han,Junfeng Lyu,Kuan Sheng,Minghao Que,Qixuan Zhang,Lan Xu,Feng Xu
</span></div><div id = 'affiliation'><span id = 'affiliation'>Tsinghua University,Tsinghua University,ShanghaiTech University,Tsinghua University,ShanghaiTech University,ShanghaiTech University,Tsinghua University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_164&sess=sess110">Facial Appearance Capture at Home with Patch-Level Reflectance Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given a single co-located smartphone video captured in a dim room as the input, our method can reconstruct high-quality facial assets within the distribution modeled by a diffusion prior trained on Light Stage scans, which can be exported to common graphics engines like Blender for photo-realistic rendering.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>64
</span></div><div id = 'author'>Authors:<span id = 'author'>Yiqian Wu,Malte Prinzler,Xiaogang Jin,Siyu Tang
</span></div><div id = 'affiliation'><span id = 'affiliation'>ETH Zürich,ETH Zürich,State Key Lab of CAD and CG, Zhejiang University,ETH Zürich
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_666&sess=sess110">Text-based Animatable 3D Avatars with Morphable Model Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AnimPortrait3D is a novel method for text-based, realistic, animatable 3DGS avatar generation with morphable model alignment. To address ambiguities in diffusion predictions during 3D distillation, we introduce key strategies: initializing a 3D avatar with robust appearance and geometry, and leveraging a ControlNet to ensure accurate alignment with the underlying model.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>65
</span></div><div id = 'author'>Authors:<span id = 'author'>Yisheng He,Xiaodong Gu,Xiaodan Ye,Chao Xu,Zhengyi Zhao,Yuan Dong,Weihao Yuan,Zilong Dong,Liefeng Bo
</span></div><div id = 'affiliation'><span id = 'affiliation'>Alibaba Group,Alibaba Group,Alibaba Group,Alibaba Group,Alibaba Group,Alibaba Group,Alibaba Group,Alibaba Group,Alibaba Group
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_829&sess=sess110">LAM: Large Avatar Model for One-shot Animatable Gaussian Head</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LAM is an innovative Large Avatar Model for animatable Gaussian head reconstruction from a single image in seconds. Our Gaussian heads are immediately animatable and renderable without additional networks or post-processing. This allows seamless integration into existing rendering pipelines, ensuring real-time animation and rendering across various platforms, including mobile phones.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>66
</span></div><div id = 'author'>Authors:<span id = 'author'>Tingting Liao,Yujian Zheng,Yuliang Xiu,Adilbek Karmanov,Liwen Hu,Leyang Jin,Hao Li
</span></div><div id = 'affiliation'><span id = 'affiliation'>Mohamed bin Zayed University of Artificial Intelligence,Mohamed bin Zayed University of Artificial Intelligence,Westlake University,Mohamed bin Zayed University of Artificial Intelligence,Pinscreen,Mohamed bin Zayed University of Artificial Intelligence,Mohamed bin Zayed University of Artificial Intelligence
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_745&sess=sess110">SOAP: Style-Omniscient Animatable Portraits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>SOAP awakens the 3D princess from 2D stylized photos. Unlike other works that directly drive the 2D photos, SOAP reconstructs well-rigged 3D avatars, with detailed geometry and all-around texture, from just a single stylized picture.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>67
</span></div><div id = 'author'>Authors:<span id = 'author'>Chengan He,Junxuan Li,Tobias Kirschstein,Artem Sevastopolskiy,Shunsuke Saito,Qingyang Tan,Javier Romero,Chen Cao,Holly Rushmeier,Giljoo Nam
</span></div><div id = 'affiliation'><span id = 'affiliation'>Yale University,Meta Codec Avatars Lab,Technical University of Munich,Technical University of Munich,Meta Codec Avatars Lab,Meta Codec Avatars Lab,Meta Codec Avatars Lab,Meta Codec Avatars Lab,Yale University,Meta Codec Avatars Lab
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1203&sess=sess110">3DGH: 3D Head Generation with Composable Hair and Face</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present 3DGH, a generative model that creates realistic 3D human heads with composable hair and face components. By modeling both the separation and correlation between hair and face in a generative paradigm, it enables high-quality, full-head synthesis and flexible 3D hairstyle editing with strong visual consistency and realism.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>68
</span></div><div id = 'author'>Authors:<span id = 'author'>Youyang Du,Lu Wang,Beibei Wang
</span></div><div id = 'affiliation'><span id = 'affiliation'>Shandong University,Shandong University,Nanjing University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1437&sess=sess110">Facial Microscopic Structures Synthesis from a Single Unconstrained Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our framework can efficiently synthesize facial microstructure from an unconstrained facial image via differentiable optimization. We propose neural wrinkle simulation for differentiable microstructure parameterization, and direction distribution similarity to align features with blurry image patches. Our framework is also compatible with existing facial reconstruction methods for detail enhancement.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>69
</span></div><div id = 'author'>Authors:<span id = 'author'>Giulio Viganò,Maks Ovsjanikov,Simone Melzi
</span></div><div id = 'affiliation'><span id = 'affiliation'>Università di Milano Bicocca,Centre National de la Recherche Scientifique - Laboratoire d'informatique de l'École Polytechnique (LIX),Università di Milano Bicocca
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1494&sess=sess126">NAM: Neural Adjoint Maps for refinement of shape correspondences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Neural Adjoint Maps, a novel representation for correspondences between 3D shapes. Built on and extending the functional map framework, our approach enables accurate, non-linear refinement of shape matching across meshes and point clouds, setting a new standard in diverse scenarios and applications like graphics and medical imaging.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>70
</span></div><div id = 'author'>Authors:<span id = 'author'>Yuezhi Yang,Haitao Yang,George Kiyohiro Nakayama,Xiangru Huang,Leonidas Guibas,Qixing Huang
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Texas at Austin,University of Texas at Austin,Stanford University,Westlake University,Stanford University,University of Texas at Austin
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_527&sess=sess126">GenAnalysis: Joint Shape Analysis by Learning Man-Made Shape Generators with Deformation Regularizations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present GenAnalysis, an implicit shape generation framework enabling joint shape matching and consistent segmentation by enforcing as-affine-as-possible (AAAP) deformations via regularization loss in latent space. It enables shape analysis via extracting and analysing shape variations in the tangent space. Experiments on ShapeNet demonstrate improved performance over existing methods.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>71
</span></div><div id = 'author'>Authors:<span id = 'author'>Changhao Li,Yu Xin,Xiaowei Zhou,Ariel Shamir,Hao Zhang,Ligang Liu,Ruizhen Hu
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Science and Technology of China,University of Science and Technology of China,State Key Laboratory of CAD & CG, Zhejiang University,Reichman University,Simon Fraser University,University of Science and Technology of China,Shenzhen University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_258&sess=sess126">MASH: Masked Anchored SpHerical Distances for 3D Shape Representation and Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Masked Anchored SpHerical Distances (MASH), a novel multi-view and parametrized representation of 3D shapes. MASH is versatilefor multiple applications including surface reconstruction, shape generation, completion, and blending, achieving superior performance thanks to its unique representation encompassing both implicit and explicit features.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>72
</span></div><div id = 'author'>Authors:<span id = 'author'>Chunyi Sun,Junlin Han,Runjia Li,Weijian Deng,Dylan Campbell,Stephen Gould
</span></div><div id = 'affiliation'><span id = 'affiliation'>Australian National University,University of Oxford,University of Oxford,Australian National University,Australian National University,Australian National University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_817&sess=sess126">Unsupervised Decomposition of 3D Shapes into Expressive and Editable Extruded Profile Primitives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D2EP transforms 3D shapes into expressive, editable primitives by extruding 2D profiles along 3D curves. This approach creates compact, interpretable representations that support intuitive editing and flexible redesign. It delivers high fidelity and efficiency, outperforming existing methods across digital design, asset creation, and customization workflows.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>73
</span></div><div id = 'author'>Authors:<span id = 'author'>Si-Tong Wei,Rui-Huan Wang,Chuan-Zhi Zhou,Baoquan Chen,Peng-Shuai Wang
</span></div><div id = 'affiliation'><span id = 'affiliation'>Peking University,Peking University,Peking University,Peking University,Peking University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_203&sess=sess126">OctGPT: Octree-based Multiscale Autoregressive Models for 3D Shape Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>OctGPT is a novel multiscale autoregressive model for 3D shape generation. It introduces hierarchical serialized octree representation, octree-based transformer with 3D RoPE and token-parallel generation schemes. OctGPT significantly accelerates convergence, achieves performance rivaling or surpassing state-of-the-art diffusion models, and supports text/sketch/image-conditioned generation and scene-level synthesis.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>74
</span></div><div id = 'author'>Authors:<span id = 'author'>Longwen Zhang,Qixuan Zhang,Haoran Jiang,Yinuo Bai,Wei Yang,Lan Xu,Jingyi Yu
</span></div><div id = 'affiliation'><span id = 'affiliation'>ShanghaiTech University,ShanghaiTech University,ShanghaiTech University,ShanghaiTech University,Huazhong University of Science and Technology,ShanghaiTech University,ShanghaiTech University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_357&sess=sess126">BANG: Dividing 3D Assets via Generative Exploded Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>BANG introduces Generative Exploded Dynamics, a novel method that dynamically decomposes 3D objects into meaningful, volumetric parts through smooth, controllable exploded views. Bridging intuitive human understanding and generative AI, it enables precise part-level manipulation, semantic comprehension, and versatile applications in 3D creation, visualization, and printing workflows.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>75
</span></div><div id = 'author'>Authors:<span id = 'author'>Karen Furuta,Jingjing Li,Tatsuki Fushimi,Yoichi Ochiai
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Tsukuba,University of Tsukuba,University of Tsukuba,University of Tsukuba
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=artpl_142&sess=sess106">Discipline Together with the Self in Kendo: Exploring "Qi" through Mixed Reality and Autoethnography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work explores “qi” in kendo through mixed reality and autoethnography, blending tradition and technology. By animating digital humans with “qi”, it frames martial arts as art. The project invites reflection on selfhood and offers fresh insights at the intersection of culture, embodiment, and digital experience.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>76
</span></div><div id = 'author'>Authors:<span id = 'author'>Néill O'Dwyer,Enda Bates,Nicholas Johnson
</span></div><div id = 'affiliation'><span id = 'affiliation'>Trinity College Dublin,Trinity College Dublin,Trinity College Dublin
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=artpl_169&sess=sess106">Reimagining Beckett=E2=80=99s Not I in Virtual Reality: The Metahuman as a Digital Double of the Actor</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This practice-based project reimagines Beckett’s Not I in virtual reality, marrying minimalist theatre with immersive technology. A lone, disembodied Metahuman mouth exploits VR’s intense presence while subverting customary embodiment and audience agency. Integrating performing avatars, the work probes authenticity, identity, and authorship, demonstrating how “subtractive dramaturgy” thrives in an additive medium. Findings advance performance studies, XR design, and digital humanities by showing how technology reshapes creativity, embodiment, and storytelling.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>77
</span></div><div id = 'author'>Authors:<span id = 'author'>Yihua Li,Hongyue Chen,Yiqing Li,Yetong Xin
</span></div><div id = 'affiliation'><span id = 'affiliation'>New York University,English Literature and Literary Theory at the University of Freiburg,New York University,Harvard University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=artpl_172&sess=sess106">PoeSpin: A Human-AI Dance to Poetry System for Movement-Based Verse Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>PoeSpin is a human-AI cocreating system. By transforming pole dance movements into poetry through AI, we challenge both traditional prejudices against this art form and conventional approaches to human-AI creativity. This work demonstrates how computational systems can preserve the deeply human aspects of artistic expression while creating new possibilities for cross-modal artistic collaboration, suggesting pathways for more inclusive and expressive forms of human-AI co-creation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>78
</span></div><div id = 'author'>Authors:<span id = 'author'>Kei Iwasaki,Yoshinori Dobashi
</span></div><div id = 'affiliation'><span id = 'affiliation'>Saitama University,Hokkaido University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_722&sess=sess113">Spherical Lighting with Spherical Harmonics Hessian</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce spherical harmonics Hessian and solid spherical harmonics, a variant of spherical harmonics, to compute the spherical harmonics Hessian efficiently and accurately to the computer graphics community. These mathematical tools are used to develop an analytical representation of the Hessian matrix of spherical harmonics coefficients for spherical lights.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>79
</span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Huang,Shaokun Zheng,Kun Xu,Yoshifumi Kitamura,Jiaping Wang
</span></div><div id = 'affiliation'><span id = 'affiliation'>International Digital Economy Academy,Tsinghua University,Tsinghua University,Tohoku University,International Digital Economy Academy
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_230&sess=sess113">Guided Lens Sampling for Efficient Monte Carlo Circle-of-Confusion Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A guided lens sampling technique that improves Monte Carlo rendering of depth-of-field by projecting a global 3D radiance field into lens space via bipolar-cone projection. This method efficiently targets high-contribution regions, significantly reducing noise and improving convergence for circle-of-confusion effects in production rendering.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>80
</span></div><div id = 'author'>Authors:<span id = 'author'>Zhimin Fan,Chen Wang,Yiming Wang,Boxuan Li,Yuxuan Guo,Ling-Qi Yan,Yanwen Guo,Jie Guo
</span></div><div id = 'affiliation'><span id = 'affiliation'>Nanjing University,Nanjing University,Nanjing University,Nanjing University,Nanjing University,University of California Santa Barbara,Nanjing University,Nanjing University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_114&sess=sess113">Bernstein Bounds for Caustics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We derive vertex position and irradiance bounds for each triangle tuple, introducing a bounding property of rational functions on the Bernstein basis, to significantly reduce the search domain when systematically simulating specular light transport.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>81
</span></div><div id = 'author'>Authors:<span id = 'author'>Jeffrey Liu,Daqi Lin,Markus Kettunen,Chris Wyman,Ravi Ramamoorthi
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Illinois Urbana-Champaign,NVIDIA,NVIDIA,NVIDIA,NVIDIA
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_435&sess=sess113">Reservoir Splatting for Temporal Path Resampling and Motion Blur</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce reservoir splatting, a technique preserving exact primary hits during temporal ReSTIR. This approach makes temporal path resampling more robust under motion, especially for regions with high-frequency detail. We further demonstrate how reservoir splatting naturally enables ReSTIR support for both motion blur and depth of field.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>82
</span></div><div id = 'author'>Authors:<span id = 'author'>Xuejun Hu,Jinfan Lu,Kun Xu
</span></div><div id = 'affiliation'><span id = 'affiliation'>Tsinghua University,Tsinghua University,Tsinghua University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_426&sess=sess113">Kernel Predicting Neural Shadow Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel shadow method named kernel predicting neural shadow mapping. By modeling soft shadow values as pixelwise local filtering from basic hard shadow values, we trained a neural network to predict local filter weights, achieving accurate and temporally-stable soft shadows with good generalizability.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>83
</span></div><div id = 'author'>Authors:<span id = 'author'>Naoto Shirashima,Hideki Todo,Yuki Yamaoka,Shizuo Kaji,Kunihiko Kobayashi,Haruna Shimotahira,Yonghao Yue
</span></div><div id = 'affiliation'><span id = 'affiliation'>AGU,Takushoku University,AGU,Kyushu University,AGU,AGU,AGU
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_207&sess=sess113">Stroke Transfer for Participating Media</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a stroke-based method for transforming dynamic 3D scenes with smoke, fire, or clouds into painterly animations. Learning from user-provided exemplars, our system transfers stroke styles—color, width, length, and orientation—while preserving motion and structure. This enables expressive and coherent renderings of complex volumetric media.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>84
</span></div><div id = 'author'>Authors:<span id = 'author'>Venkataram Edavamadathil Sivaram,Ravi Ramamoorthi,Tzu-Mao Li
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of California San Diego,University of California San Diego,University of California San Diego
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_632&sess=sess113">Modeling and Rendering Glow Discharge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a physically-based model for simulating and rendering glow discharge, a luminous plasma effect seen in neon lights and gas discharge lamps. The model captures particle interactions and emission dynamics, integrates into volume rendering systems, and enables realistic, interactive visualizations of complex light phenomena.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>85
</span></div><div id = 'author'>Authors:<span id = 'author'>Karran Pandey,Anita Hu,Clement Fuji Tsang,Or Perel,Karan Singh,Maria Shugrina
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Toronto,NVIDIA,NVIDIA,NVIDIA,University of Toronto,NVIDIA
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1011&sess=sess144">Painting with 3D Gaussian Splat Brushes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the first interactive system for painting with 3D Gaussian splat brushes. With our tool, artists can sample volumetric fragments from real-world Gaussian splat captures and paint with them in real time. Our tool seamlessly deforms sampled splats along painted strokes, introducing realistic transitions between seams with diffusion inpainting.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>86
</span></div><div id = 'author'>Authors:<span id = 'author'>Kenneth Chen,Nathan Matsuda,Jon McElvain,Yang Zhao,Thomas Wan,Qi Sun,Alexandre Chapiro
</span></div><div id = 'affiliation'><span id = 'affiliation'>New York University,Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta,New York University,Reality Labs Research, Meta
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_312&sess=sess144">What is HDR? Perceptual Impact of Luminance and Contrast in Immersive Displays</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We studied preferences for different contrasts and peak luminances in HDR. To do this, we collected a new HDR video dataset, developed tone mappers,  and built an HDR haploscope that can reproduce high luminance and contrast. Data was fit to a model which is used for applications like display design.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>87
</span></div><div id = 'author'>Authors:<span id = 'author'>Avinab Saha,Yu-Chih Chen,Jean-Charles Bazin,Christian Häne,Ioannis Katsavounidis,Alexandre Chapiro,Alan Bovik
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Texas Austin,University of Texas Austin,Reality Labs, Meta,Reality Labs, Meta,Reality Labs, Meta,Reality Labs, Meta,University of Texas Austin
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_489&sess=sess144">FaceExpressions-70k: A Dataset of Perceived Expression Differences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce FaceExpressions-70k, a large-scale dataset comprising 70,500 crowdsourced comparisons of facial expressions collected from over 1,000 participants. It supports the training of perceptual models for expression differences and helps guide decisions on acceptable latency and sampling rates for facial expressions when driving a face avatar.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>88
</span></div><div id = 'author'>Authors:<span id = 'author'>Seonghyeon Kim,Chang Wook Seo,Kwanggyoon Seo,Seung Han Song,Junyong Noh
</span></div><div id = 'affiliation'><span id = 'affiliation'>KAIST, Visual Media Lab,Anigma Technologies,KAIST, Visual Media Lab,Chungnam National University Hospital,KAIST, Visual Media Lab
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_574&sess=sess144">A Deep Learning-based Virtual Oculoplastic Surgery Simulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A novel deep learning system enhances realistic virtual oculoplastic surgery simulations.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>89
</span></div><div id = 'author'>Authors:<span id = 'author'>Sophie Kergaßner,Taimoor Tariq,Piotr Didyk
</span></div><div id = 'affiliation'><span id = 'affiliation'>Università della Svizzera italiana,Università della Svizzera italiana,Università della Svizzera italiana
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_251&sess=sess144">Towards Understanding Depth Perception in Foveated Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We demonstrate that stereoacuity is remarkably resilient to foveated rendering and remains unaffected with up to 2× stronger foveation than commonly used. To this end, we design a psychovisual experiment and derive a simple perceptual model that determines the amount of foveation that does not affect stereoacuity.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>90
</span></div><div id = 'author'>Authors:<span id = 'author'>Edward Lu,Anthony Rowe
</span></div><div id = 'affiliation'><span id = 'affiliation'>Carnegie Mellon University,Carnegie Mellon University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1248&sess=sess144">QUASAR: Quad-based Adaptive Streaming And Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces an improved quad-based geometry streaming method for remote rendering that reduces bandwidth demands through temporal compression and supports QoE-driven adaptation. It achieves high-quality visuals, captures disocclusion events, uses 15× less data than SOTA, and reduces bandwidth down to 100 Mbps, enabling real-time, low-latency rendering on lightweight headsets.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>91
</span></div><div id = 'author'>Authors:<span id = 'author'>Zhaofeng Luo,Zhitong Cui,Shijian Luo,Mengyu Chu,Minchen Li
</span></div><div id = 'affiliation'><span id = 'affiliation'>Carnegie Mellon University,Carnegie Mellon University,Zhejiang University,Peking University,Carnegie Mellon University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_318&sess=sess144">VR-Doh: Hands-on 3D Modeling in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>VR-Doh, an intuitive VR-based 3D modeling system that lets you sculpt and manipulate soft objects and edit 3D Gaussian Splatting scenes in real time. Combining physics-based simulation and expressive interaction, VR-Doh empowers both novices and experts to create rich, deformable, simulation-ready models with natural hand-based input.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>92
</span></div><div id = 'author'>Authors:<span id = 'author'>Yue Chang,Otman Benchekroun,Maurizio M. Chiaramonte,Peter Yichen Chen,Eitan Grinspun
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Toronto,University of Toronto,Meta Reality Labs Research,MIT CSAIL,University of Toronto
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_139&sess=sess140">Shape Space Spectra</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce shape-space eigenanalysis to compute eigenfunctions across continuously-parameterized shape families. These eigenfunctions are obtained by minimizing a variational principle. To handle eigenvalue dominance swaps at points of multiplicity, we incorporate dynamic reordering during optimization. The method is discretization-agnostic and differentiable, enabling applications in sound synthesis, locomotion, and elastodynamic simulation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>93
</span></div><div id = 'author'>Authors:<span id = 'author'>Theo Braune,Mark Gillespie,Yiying Tong,Mathieu Desbrun
</span></div><div id = 'affiliation'><span id = 'affiliation'>Centre National de la Recherche Scientifique - Laboratoire d'informatique de l'École Polytechnique (LIX),Inria Saclay,Michigan State University,Inria Saclay
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_946&sess=sess140">Discrete Torsion of Connection Forms on Simplicial Meshes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although discrete connections are ubiquitous in vector field design, their torsion remains unstudied. We extend the existing toolbox to control the torsion of discrete connections: we introduce a new discrete Levi-Civita connection and define torsion as a measure of deviation from this reference, so torsion becomes a simple linear constraint.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>94
</span></div><div id = 'author'>Authors:<span id = 'author'>Haikuan Zhu,Hongbo Li,Hsueh-Ti Derek Liu,Wenping Wang,Jing Hua,Zichun Zhong
</span></div><div id = 'affiliation'><span id = 'affiliation'>Wayne State University,Wayne State University,Roblox,Texas A&M University,Wayne State University,Wayne State University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_706&sess=sess140">Designing 3D Anisotropic Frame Fields with Odeco Tensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our method proposes a novel computational design framework for designing anisotropic tensor fields. It enables flexible control over scalings without requiring users to specify orientations explicitly. We apply these anisotropic tensor fields to various applications, such as anisotropic meshing, structural mechanics, and fabrication.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>95
</span></div><div id = 'author'>Authors:<span id = 'author'>Yiling Pan,Zhixin Xu,Bin Wang,Bailin Deng
</span></div><div id = 'affiliation'><span id = 'affiliation'>Tsinghua University,Tsinghua University,Tsinghua University,Cardiff University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_593&sess=sess140">Piecewise Ruled Approximation for Freeform Mesh Surfaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a method to approximate arbitrary freeform surface meshes with piecewise ruled surfaces. Our approach optimizes mesh shape and ruling direction field simultaneously, extracts patch topology, and refines ruling positions and orientations. The technique effectively approximates diverse freeform shapes and has potential applications in architecture and engineering.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>96
</span></div><div id = 'author'>Authors:<span id = 'author'>Ana Dodik,Isabella Yu,Kartik Chandra,Jonathan Ragan-Kelley,Joshua Tenenbaum,Vincent Sitzmann,Justin Solomon
</span></div><div id = 'affiliation'><span id = 'affiliation'>Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT),Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT),Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT),Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT),Massachusetts Institute of Technology (MIT),Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT),Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT)
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1425&sess=sess140">Meschers: Geometry Processing of Impossible Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Meschers are a mesh representation for Escheresque geometry. They allow us to solve partial differential equations on the surface of an impossible object, meaning that we can find impossible shortest paths, perform mescher smoothing, and even inverse render a mescher from an image.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>97
</span></div><div id = 'author'>Authors:<span id = 'author'>Yuou Sun,Bailin Deng,Juyong Zhang,Yuou Sun
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Science and Technology of China,Cardiff University,University of Science and Technology of China,University of Science and Technology of China
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_112&sess=sess140">End-to-end Surface Optimization for Light Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing freeform surfaces to reflect or refract light to achieve target light distributions is a challenging inverse problem. We propose an end-to-end optimization strategy using a novel differentiable rendering model driven by image errors, combined with face-based optimal transport initialization and geometric constraints, to achieve high-quality final physical results.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>98
</span></div><div id = 'author'>Authors:<span id = 'author'>Guying Lin,Lei Yang,Congyi Zhang,Hao Pan,Yuhan Ping,Guodong Wei,Taku Komura,John Keyser,Wenping Wang,Guying Lin
</span></div><div id = 'affiliation'><span id = 'affiliation'>Carnegie Mellon University,The University of Hong Kong,The University of British Columbia,Tsinghua University,The University of Hong Kong,The University of Hong Kong,The University of Hong Kong,Texas A&M University,Texas A&M University,Carnegie Mellon University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_119&sess=sess140">Patch-Grid: An Efficient and Feature-Preserving Neural Implicit Surface Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Patch-Grid, a unified neural implicit representation that efficiently represents complex shapes, preserves sharp features, and handles open boundaries and thin geometric details. By decomposing shapes into patches encapsulated by adaptive feature grids and merging them through localized CSG operations, Patch-Grid demonstrates superior robustness, efficiency, and accuracy.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>99
</span></div><div id = 'author'>Authors:<span id = 'author'>Dewen Guo,Zhendong Wang,Zegao Liu,Sheng Li,Guoping Wang,Yin Yang,Huamin Wang
</span></div><div id = 'affiliation'><span id = 'affiliation'>Peking University,Style3D Research,Style3D Research,Peking University,Peking University,University of Utah,Style3D Research
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_293&sess=sess114">Fast Physics-Based Modeling of Knots and Ties using Templates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a physics-based modeling system for knots and ties using pipe-like parametric templates, defined by Bézier curves and adaptive radii for flexible, intersection-free shapes. Our method maps cloth regions from UV space into 3D knot forms via a penetration-free initialization and supports quasistatic simulation with efficient collision handling.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>100
</span></div><div id = 'author'>Authors:<span id = 'author'>Zizhou Huang,Chrystiano Araújo,Andrew Kunz,Denis Zorin,Daniele Panozzo,Victor Zordan
</span></div><div id = 'affiliation'><span id = 'affiliation'>New York University,Roblox,Roblox,New York University,New York University,Roblox
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_102&sess=sess114">Intersection-Free Garment Retargeting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce an automatic tool to retarget artist-designed garments on a standard mannequin to possibly non-human avatars with unrealistic characteristics, which widely appear in games and animations. We preserve the geometrical features in the original design, guarantee intersection-free, and fit the garment adaptively to the avatars.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>101
</span></div><div id = 'author'>Authors:<span id = 'author'>Anran Qi,Nico Pietroni,Maria Korosteleva,Olga Sorkine-Hornung,Adrien Bousseau
</span></div><div id = 'affiliation'><span id = 'affiliation'>INRIA, Université Côte d'Azur,University of Technology Sydney,ETH Zurich,ETH Zurich,INRIA, Université Côte d'Azur
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_811&sess=sess114">Rags2Riches: Computational Garment Reuse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the first algorithm to automatically compute sewing patterns forupcycling existing garments into new designs. Our algorithm takes as inputtwo garment designs along with their corresponding sewing patterns anddetermines how to cut one of them to match the other by following garmentreuse principles.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>102
</span></div><div id = 'author'>Authors:<span id = 'author'>Yuki Tatsukawa,Anran Qi,I-Chao Shen,Takeo Igarashi
</span></div><div id = 'affiliation'><span id = 'affiliation'>The University of Tokyo,INRIA, Université Côte d'Azur,The University of Tokyo,The University of Tokyo
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_355&sess=sess114">GarmentImage: Raster Encoding of Garment Sewing Patterns with Diverse Topologies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Garment sewing patterns often rely on vector formats, which struggle with discontinuities and unseen topologies. GarmentImage instead encodes geometry, topology, and placement into multi-channel grids, enabling smooth transitions and better generalization. Using simple CNNs, it works well in pattern exploration, prompt-to-pattern generation, and image-to-pattern prediction.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>103
</span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Li,Chang Yu,Wenxin Du,Ying Jiang,Tianyi Xie,Yunuo Chen,Yin Yang,Chenfanfu Jiang
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of California Los Angeles,University of California Los Angeles,University of California Los Angeles,University of California Los Angeles,University of California Los Angeles,University of California Los Angeles,University of Utah,University of California Los Angeles
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_660&sess=sess114">Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion Prior and Differentiable Physics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Dress-1-to-3 to reconstruct physics-plausible, simulation-ready separated garments from an in-the-wild image. Starting with the image, our approach combines a pre-trained image-to-sewing pattern generation model with a pre-trained multi-view diffusion model. The sewing pattern is refined using a differentiable garment simulator based on the generated multi-view images.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>104
</span></div><div id = 'author'>Authors:<span id = 'author'>Ren Li,Cong Cao,Corentin Dumery,Yingxuan You,Hao Li,Pascal Fua
</span></div><div id = 'affiliation'><span id = 'affiliation'>EPFL,MBZUAI,EPFL,EPFL,MBZUAI,EPFL
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_478&sess=sess114">Single View Garment Reconstruction Using Diffusion Mapping Via Pattern Coordinates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel method for accurate 3D garment reconstruction from single-view images, bridging 2D and 3D representations. Our mapping model creates connections among image pixels, UV coordinates, and 3D geometry, resulting in realistic garments with intricate details and enabling downstream applications like garment retargeting and texture editing.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>105
</span></div><div id = 'author'>Authors:<span id = 'author'>Alon Feldman,Mirela Ben-Chen
</span></div><div id = 'affiliation'><span id = 'affiliation'>Technion – Israel Institute of Technology,Technion – Israel Institute of Technology
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_779&sess=sess112">On Planar Shape Interpolation With Logarithmic Metric Blending</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Logarithmic metric blending enables smooth interpolation between planar shapes while bounding both conformal and area distortions. By blending symmetric positive definite metrics in the log domain, our method geometrically interpolates distortions. This leads to natural transitions that outperform existing techniques in applications such as shape morphing and animation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>106
</span></div><div id = 'author'>Authors:<span id = 'author'>Shibo Liu,Ligang Liu,Xiao-Ming Fu
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Science and Technology of China,University of Science and Technology of China,University of Science and Technology of China
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_744&sess=sess112">Closed-form Generalized Winding Numbers of Rational Parametric Curves for Robust Containment Queries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We derive closed-form expressions for GWNs of rational parametric curves for robust containment queries.Our closed-form expression enables efficient computation of GWN, even if the query points are located on the rational curve. We also derive the derivatives of GWN for other applications.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>107
</span></div><div id = 'author'>Authors:<span id = 'author'>Elie Michel,Alec Jacobson,Siddhartha Chaudhuri,Jean-Marc Thiery
</span></div><div id = 'affiliation'><span id = 'affiliation'>Adobe Research,Adobe Research,Adobe Research,Adobe Research
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1370&sess=sess112">Variational Green and Biharmonic Coordinates for 2D Polynomial Cages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present analytical formulas for evaluating Green and biharmonic 2D coordinates and their gradients and Hessians, for 2D cages made of polynomial arcs.We present results of 2D image deformations by direct interaction with the cage and through variational solvers.We demonstrate the flexibility
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>108
</span></div><div id = 'author'>Authors:<span id = 'author'>Shibo Liu,Tielin Dai,Ligang Liu,Xiao-Ming Fu
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Science and Technology of China,University of Science and Technology of China,University of Science and Technology of China,University of Science and Technology of China
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_747&sess=sess112">Polynomial 2D Biharmonic Coordinates for High-order Cages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose polynomial 2D biharmonic coordinates for closed high-order cages containing polynomial curves of any order by extending the classical2D biharmonic coordinates using high-order BEM. When applying our coordinateto 2D cage-based deformation, users manipulate the \Beziercontrol points to quickly generate the desired conformal deformation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>109
</span></div><div id = 'author'>Authors:<span id = 'author'>Dong Xiao,Renjie Chen
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Science and Technology of China,University of Science and Technology of China
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_319&sess=sess112">Flexible 3D Cage-based Deformation via Green Coordinates on B=C3=A9zier Patches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work constructs Green coordinates for cages composed of Bézier patches, which enables flexible deformations with curved boundaries. The high-order structure also allows us to create a compact curved cage for the input models. Additionally, this work proposes a global projection technique for precise linear reproduction.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>110
</span></div><div id = 'author'>Authors:<span id = 'author'>Michal Edelstein,Hsueh-Ti Derek Liu,Mirela Ben-Chen
</span></div><div id = 'affiliation'><span id = 'affiliation'>Technion – Israel Institute of Technology,Roblox,Technion - Israel Institute of Technology
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_491&sess=sess112">CageNet: A Meta-Framework for Learning on Wild Meshes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a framework for learning on in-the-wild meshes containing non-manifold elements, multiple components, and interior structures. Our approach uses cages and generalized barycentric coordinates to parametrize and learn volumetric functions, demonstrated by segmentation and skinning weights, achieving state-of-the-art results on wild meshes.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>111
</span></div><div id = 'author'>Authors:<span id = 'author'>Isaac Joseph Clarke,Raul Masu,Theo Papatheodorou
</span></div><div id = 'affiliation'><span id = 'affiliation'>Hong Kong University of Science and Technology (Guangzhou),Hong Kong University of Science and Technology, Guangzhou,Hong Kong University of Science and Technology, Guangzhou
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=artpl_170&sess=sess108">Speculative AI Re-enactment of the Figurists' Encounters With the I Ching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Instead of pursuing the concern of AI displacing artists, we emphasise a role for artists in reshaping technology and branching it in new directions. A role that places us less as a user of AI technology, waiting for its creative outputs, but as a maker of what AI can be, perhaps leading us towards an AI that is as unnatural, occult, and esoteric, as it is practical.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>112
</span></div><div id = 'author'>Authors:<span id = 'author'>Jia SUN,Zheng WEI,Pan HUI
</span></div><div id = 'affiliation'><span id = 'affiliation'>The Hong Kong University of Science and Technology (Guangzhou),The Hong Kong University of Science and Technology,The Hong Kong University of Science and Technology (Guangzhou)
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=artpl_187&sess=sess108">Algorithmic Miner: Humanity in Service - An AI-Driven VR Journey into Machine Logic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Algorithmic Miner uses VR to reveal the hidden labor behind AI systems. By immersing participants in data annotation tasks, it critically reflects on exploitation, automation, and techno-capitalism, prompting new discussions on ethical, human-centered design in interactive systems.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>113
</span></div><div id = 'author'>Authors:<span id = 'author'>Ana María Zapata Guzmán,Ludovica Schaerf,Darío Negueruela del Castillo,Iacopo Neri
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Zurich,University of Zurich,University of Zurich,University of Zurich
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=artpl_199&sess=sess108">Digital F(r)ictions: Reimagining Colombian Art and its Territory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Both a critique and celebration of digital representation, this project offers multiple perspectives beyond technological homogenization. Through exploring digital f(r)ictions and multiplicities, we reject singular viewpoints in favor of interconnected truths. Our work with AI and Colombian art raises questions about bias, agency, and authenticity in cultural production, prompting reflection on AI's influence on collective imaginaries.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>114
</span></div><div id = 'author'>Authors:<span id = 'author'>Chong Zeng,Yue Dong,Pieter Peers,Hongzhi Wu,Xin Tong
</span></div><div id = 'affiliation'><span id = 'affiliation'>State Key Lab of CAD and CG, Zhejiang University,Microsoft Research Asia,College of William & Mary,State Key Lab of CAD and CG, Zhejiang University,Microsoft Research Asia
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_160&sess=sess115">RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present RenderFormer, a neural rendering pipeline that directly renders an image from a triangle-based representation of scene with full global illumination effects, and that does not require per-scene training or finetuning.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>115
</span></div><div id = 'author'>Authors:<span id = 'author'>Junke Zhu,Zehan Wu,Qixing Zhang,Cheng Liao,Zhangjin Huang
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Science and Technology of China,Tencent Technology,University of Science and Technology of China,Tencent Technology,University of Science and Technology of China
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1332&sess=sess115">WishGI: Lightweight Static Global Illumination Baking via Spherical Harmonics Fitting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our work is a lightweight static global illumination baking solution that achieves competitive lighting effects while using only approximately 5% of the memory required by mainstream industry techniques. By adopting a vertex-probe structure, we ensure excellent runtime performance, making it suitable for low-end devices.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>116
</span></div><div id = 'author'>Authors:<span id = 'author'>Shaohua Mo,Chuankun Zheng,Zihao Lin,Dianbing Xi,Qi Ye,Rui Wang,Hujun Bao,Yuchi Huo
</span></div><div id = 'affiliation'><span id = 'affiliation'>State Key Lab of CAD&CG, Zhejiang University,State Key Lab of CAD&CG, Zhejiang University,State Key Lab of CAD&CG, Zhejiang University,State Key Lab of CAD&CG, Zhejiang University,Zhejiang University,State Key Lab of CAD&CG, Zhejiang University,State Key Lab of CAD&CG, Zhejiang University,State Key Lab of CAD&CG, Zhejiang University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1080&sess=sess115">Dual-Band Feature Fusion for Neural Global Illumination with Multi-Frequency Reflections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a neural global illumination method capable of capturing multi-frequency reflections in dynamic scenes by leveraging object-centric feature grids and a novel dual-band fusion module. Our approach produces high-quality, realistic rendering effects and outperforms state-of-the-art techniques in both visual quality and computational efficiency.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>117
</span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Zhou,Chao Li,Zhenyuan Zhang,Mingcong Tang,Zibin Li,Shuhang Luan,Zhangjin Huang
</span></div><div id = 'affiliation'><span id = 'affiliation'>Tencent,Tencent,Tencent,Tencent,Tencent,Tencent,University of Science and Technology of China
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1413&sess=sess115">Gaussian Compression for Precomputed Indirect Illumination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a Gaussian fitting compression method for light field probes, reducing storage and memory demands in large scenes. Using low-bit adaptive Gaussians and GPU-accelerated decompression, our technique replaces traditional PCA-based approaches, achieving 1:50 compression ratios. Real-time cascaded light field textures eliminate redundant baking, preserving visual quality and rendering speed.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>118
</span></div><div id = 'author'>Authors:<span id = 'author'>Wenyou Wang,Rex West,Toshiya Hachisuka
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Waterloo,Aoyama Gakuin University,University of Waterloo
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_468&sess=sess115">Segment-based Light Transport Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel segment-based framework for light transport simulation, efficiently assembling paths from disconnected segments. Our method includes innovative segment sampling techniques and corresponding estimation strategies. To demonstrate its strengths, we propose a robust bidirectional path filtering prototype, achieving superior rendering quality and faster convergence than state-of-the-art methods.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>119
</span></div><div id = 'author'>Authors:<span id = 'author'>Pedro Figueiredo,Qihao He,Steve Bako,Nima Khademi Kalantari
</span></div><div id = 'affiliation'><span id = 'affiliation'>Texas A&M University,Texas A&M University,Aurora Innovation,Texas A&M University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1339&sess=sess115">Neural Importance Sampling of Many Lights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural approach for estimating spatially varying light selection distributions to improve importance sampling in Monte Carlo rendering. To efficiently manage hundreds or thousands of lights, we integrate our neural approach with light hierarchy techniques, where the network predicts cluster-level distributions and existing methods sample lights within clusters.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>120
</span></div><div id = 'author'>Authors:<span id = 'author'>Sarah Taylor,Salvador Medina,Jonathan Windle,Erica Alcusa Sáez,Iain Matthews
</span></div><div id = 'affiliation'><span id = 'affiliation'>Epic Games,Epic Games,Epic Games,Epic Games,Epic Games
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_866&sess=sess150">xADA: Controllable and Expressive Audio-Driven Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce xADA, a generative model for creating expressive and realistic animation of the face, tongue, and head directly from speech audio.The animation maps directly onto MetaHuman compatible rig controls enabling integration into industry-standard content creation pipelines.xADA generalizes across languages, and voice styles, and can animate non-verbal sounds.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>121
</span></div><div id = 'author'>Authors:<span id = 'author'>Anindita Ghosh,Bing Zhou,Rishabh Dabral,Jian Wang,Vladislav Golyanik,Christian Theobalt,Philipp Slusallek,Chuan Guo
</span></div><div id = 'affiliation'><span id = 'affiliation'>DFKI,Snap Inc.,Max Planck Institute for Informatics,Snap Inc.,Max Planck Institute for Informatics,Max Planck Institute for Informatics,DFKI,Snap Inc.
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1165&sess=sess150">DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a framework for generating music-driven synchronized two-person dance animations with close interactions. Our system represents the two-person motion sequence as a cohesive entity, performs hierarchical encoding of the motion sequence into discrete tokens, and utilizes dual generative masked transformers to generate realistic and coordinated dance motions.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>122
</span></div><div id = 'author'>Authors:<span id = 'author'>Zhiping Qiu,Yitong Jin,Yuan Wang,Yi Shi,Chao Tan,Chongwu Wang,Xiaobing Li,Feng Yu,Tao Yu,Qionghai Dai
</span></div><div id = 'affiliation'><span id = 'affiliation'>Central Conservatory of Music,Central Conservatory of Music,Central Conservatory of Music,Central Conservatory of Music,Weilan Tech,Central Conservatory of Music,Central Conservatory of Music,Central Conservatory of Music,Tsinghua University,Tsinghua University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1381&sess=sess150">ELGAR: Expressive Cello Performance Motion Generation for Audio Rendition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating string instrument performances with intricate movements and complex interactions poses significant challenges. To address these, we present ELGAR—the first diffusion-based framework for whole-body instrument performance motion generation solely from audio. We further contribute innovative losses, metrics, and dataset, marking a novel attempt with promising results for this emerging task.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>123
</span></div><div id = 'author'>Authors:<span id = 'author'>Bohong Chen,Yumeng Li,Youyi Zheng,Yao-Xiang Ding,Kun Zhou
</span></div><div id = 'affiliation'><span id = 'affiliation'>State Key Laboratory of CAD & CG, Zhejiang University,State Key Lab of CAD and CG, Zhejiang University,State Key Lab of CAD and CG, Zhejiang University,State Key Lab of CAD and CG, Zhejiang University,State Key Lab of CAD and CG, Zhejiang University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_263&sess=sess150">Motion-example-controlled Co-speech Gesture Generation Leveraging Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a framework to utilize Large Language Models (LLMs) for co-speech gesture generation with motion examples as direct conditions. It enables multi-modal controls over co-speech gesture generation, such as motion clips, a single pose, human video, or even text prompts.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>124
</span></div><div id = 'author'>Authors:<span id = 'author'>Linjun Wu,Xiangjun Tang,Jingyuan Cong,He Wang,Bo Hu,Xu Gong,Songnan Li,Yuchen Liao,Yiqian Wu,Chen Liu,Xiaogang Jin
</span></div><div id = 'affiliation'><span id = 'affiliation'>Zhejiang University,Zhejiang University,University of California San Diego,UCL Centre for Artificial Intelligence, Department of Computer Science, University College London (UCL),Tencent Technology (Shenzhen) Co., Ltd.,Tencent Technology (Shenzhen) Co., Ltd.,Tencent Technology (Shenzhen) Co., Ltd.,Tencent Technology (Shenzhen) Co., Ltd.,Zhejiang University,State Key Lab of CAD and CG, Zhejiang University,Zhejiang University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_410&sess=sess150">Semantically Consistent Text-to-Motion with Unsupervised Styles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel method that integrates unsupervised style from arbitrary references into a text-driven diffusion model to generate semantically consistent stylized human motion. We leverage text as a mediator to capture the temporal correspondences between motion and style, enabling the seamless integration of temporally dynamic style into motion features.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>125
</span></div><div id = 'author'>Authors:<span id = 'author'>Lei Zhong,Chuan Guo,Yiming Xie,Jiawei Wang,Changjian Li
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Edinburgh,Snap Inc.,Northeastern University,University of Edinburgh,University of Edinburgh
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_561&sess=sess150">Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel and first approach - Sketch2Anim, to automatically translate 2D storyboard sketches into high-quality 3D animations through multi-conditional motion generation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>126
</span></div><div id = 'author'>Authors:<span id = 'author'>Qinghe Wang,Yawen Luo,Xiaoyu Shi,Xu Jia,Huchuan Lu,Tianfan Xue,Xintao Wang,Pengfei Wan,Di Zhang,Kun Gai
</span></div><div id = 'affiliation'><span id = 'affiliation'>Dalian University of Technology,The Chinese University of Hong Kong,Kuaishou Technology,Dalian University of Technology,Dalian University of Technology,The Chinese University of Hong Kong,Kuaishou Technology,Kuaishou Technology,Kuaishou Technology,Kuaishou Technology
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1346&sess=sess146">CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A 3D-aware and controllable text-to-video generation method allows users to manipulate objects and camera jointly in 3D space for high-quality cinematic video creation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>127
</span></div><div id = 'author'>Authors:<span id = 'author'>Jinbo Xing,Long Mai,Cusuh Ham,Jiahui Huang,Aniruddha Mahapatra,Chi-Wing Fu,Tien-Tsin Wong,Feng Liu
</span></div><div id = 'affiliation'><span id = 'affiliation'>The Chinese University of Hong Kong,Adobe Research,Adobe Research,Adobe Research,Adobe Research,The Chinese University of Hong Kong,Monash University,Adobe Research
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_217&sess=sess146">MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>MotionCanvas enables intuitive cinematic shot design in image-to-video generation by letting users control both camera movements and object motions in a 3D-aware scene. Combining classical graphics with modern diffusion models, it translates motion intentions into spatiotemporal signals—without costly 3D data—empowering creative video synthesis for diverse editing workflows.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>128
</span></div><div id = 'author'>Authors:<span id = 'author'>Zekai Gu,Rui Yan,Jiahao Lu,Peng Li,Zhiyang Dou,Chenyang Si,Zhen Dong,Qifeng Liu,Cheng Lin,Ziwei Liu,Wenping Wang,Yuan Liu
</span></div><div id = 'affiliation'><span id = 'affiliation'>Hong Kong University of Science and Technology,Zhejiang University,Hong Kong University of Science and Technology,Hong Kong University of Science and Technology,University of Hong Kong,Nanyang Technological University,Wuhan University,Hong Kong University of Science and Technology,University of Hong Kong,Nanyang Technological University,Texas A&M University,Hong Kong University of Science and Technology
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_225&sess=sess146">Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion as Shader (DaS) is a unified approach for controlled video generation that uses 3D tracking videos to enable versatile editing, including animating mesh-to-video, camera control, motion transfer, and object manipulation, while improving temporal consistency.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>129
</span></div><div id = 'author'>Authors:<span id = 'author'>Xiuli Bi,Jianfei Yuan,Bo Liu,Yong Zhang,Xiaodong Cun,Chi Man Pan,Bin Xiao
</span></div><div id = 'affiliation'><span id = 'affiliation'>Chongqing University of Post and Telecommunications,Chongqing University of Post and Telecommunications,Chongqing University of Post and Telecommunications,Meituan,Great Bay University,University of Macau,Chongqing University of Post and Telecommunications
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1197&sess=sess146">Mobius: Text to Seamless Looping Video Generation via Latent Shift</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobius is a novel method to generate seamlessly looping videos from text descriptions directly without any user annotations, thereby creating new visual materials for the multi-media presentation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>130
</span></div><div id = 'author'>Authors:<span id = 'author'>Sihui Ji,Hao Luo,Xi Chen,Yuanpeng Tu,Yiyang Wang,Hengshuang Zhao
</span></div><div id = 'affiliation'><span id = 'affiliation'>The University of Hong Kong,DAMO Academy, Alibaba Group,The University of Hong Kong,The University of Hong Kong,The University of Hong Kong,The University of Hong Kong
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_572&sess=sess146">LayerFlow: A Unified Model for Layer-aware Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose LayerFlow,  a unified framework for layer-aware video generation, enabling seamless creation of transparent foregrounds, clean backgrounds, and blended scenes. With multi-stage training and LoRA techniques improving layer-wise video quality with limited data, it also supports variants like video layer decomposition, generating backgrounds for given foregrounds and vice versa.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>131
</span></div><div id = 'author'>Authors:<span id = 'author'>Ruizhi Shao,Yinghao Xu,Yujun Shen,Ceyuan Yang,Yang Zheng,Changan Chen,Yebin Liu,Gordon Wetzstein
</span></div><div id = 'affiliation'><span id = 'affiliation'>Tsinghua University,Stanford University,Alibaba Group,ByteDance Inc.,Stanford University,Stanford University,Tsinghua University,Stanford University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_548&sess=sess146">Interspatial Attention for Efficient 4D Human Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel interspatial attention (ISA) for diffusion transformers, which maintains identity and ensures motion consistency while allowing precise control of camera and body poses. Combined with a custom video variation autoencoder, our model achieves state-of-the-art performance for photorealistic 4D human video generation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>132
</span></div><div id = 'author'>Authors:<span id = 'author'>Yongtao Ge,Kangyang Xie,Guangkai Xu,Mingyu Liu,Li Ke,Longtao Huang,Hui Xue,Hao Chen,Chunhua Shen
</span></div><div id = 'affiliation'><span id = 'affiliation'>The University of Adelaide,Zhejiang University,Zhejiang University,Zhejiang University,Alibaba Group,Alibaba Group,Alibaba Group,Zhejiang University,Zhejiang University of Technology
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_411&sess=sess146">Generative Video Matting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Limited high-quality ground-truth data hinders traditional video matting's real-world application. This work tackles this by advocating for large-scale training with diverse synthetic segmentation and matting data. A novel generative pipeline is also introduced to predict temporally consistent alpha masks with fine-grained details.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>133
</span></div><div id = 'author'>Authors:<span id = 'author'>Kai Yan,Cheng Zhang,Sébastien Speierer,Guangyan Cai,Yufeng Zhu,Zhao Dong,Shuang Zhao
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of California Irvine,Reality Labs Research, Meta,Reality Labs, Meta,University of California Irvine,Reality Labs, Meta,Reality Labs, Meta,University of California Irvine
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_305&sess=sess131">Image-space Adaptive Sampling for Fast Inverse Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our goal is to accelerate inverse rendering by reducing the sampling budget without sacrificing overall performance. We introduce a novel image-space adaptive sampling framework to accelerate inverse rendering by dynamically adjusting pixel sampling probabilities based on gradient variance and contribution to the loss function.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>134
</span></div><div id = 'author'>Authors:<span id = 'author'>Jeongmin Gu,Bochang Moon
</span></div><div id = 'affiliation'><span id = 'affiliation'>Gwangju Institute of Science and Technology,Gwangju Institute of Science and Technology
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_901&sess=sess131">James-Stein Gradient Combiner for Inverse Monte Carlo Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a gradient combiner that blends unbiased and biased gradients in parameter space using the James-Stein estimator to infer scene parameters (BSDFs and volumes) from images. This approach enhances optimization accuracy compared to relying solely on either unbiased or biased gradients.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>135
</span></div><div id = 'author'>Authors:<span id = 'author'>Lifan Wu,Nathan Morrical,Sai Praveen Bangaru,Rohan Sawhney,Shuang Zhao,Chris Wyman,Ravi Ramamoorthi,Aaron Lefohn
</span></div><div id = 'affiliation'><span id = 'affiliation'>NVIDIA,NVIDIA,NVIDIA,NVIDIA,NVIDIA,NVIDIA,NVIDIA,NVIDIA
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_619&sess=sess131">Unbiased Differential Visibility Using Fixed-Step Walk-on-Spherical-Caps And Closest Silhouettes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Warped-area reparameterization is a powerful technique to compute differential visibility. The key is constructing a velocity field that is continuous in the domain interior and agrees with defined velocities on boundaries. We present a robust and efficient unbiased estimator for differential visibility, using a fixed-step walk-on-spheres and closest silhouette queries.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>136
</span></div><div id = 'author'>Authors:<span id = 'author'>Markus Worchel,Marc Alexa
</span></div><div id = 'affiliation'><span id = 'affiliation'>TU Berlin,TU Berlin
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_886&sess=sess131">Moment Bounds are Differentiable: Efficiently Approximating Measures in Inverse Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Measures can be compactly represented and approximated using the theory of moments. This work proves that such moment-based representations are differentiable, leading to principled and efficient approaches for approximating transmittance and visibility in differentiable rendering.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>137
</span></div><div id = 'author'>Authors:<span id = 'author'>Mariia Soroka,Christoph Peters,Steve Marschner
</span></div><div id = 'affiliation'><span id = 'affiliation'>Cornell University,Delft University of Technology,Cornell University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_116&sess=sess131">Quadric-Based Silhouette Sampling for Differentiable Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physically based differentiable rendering computes gradients of the rendering equation. The task is made difficult by discontinuities in the integrand at object silhouettes. To address this challenge, we propose a novel edge sampling approach that outperforms the state-of-the-art among unidirectional differentiable renderers.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>138
</span></div><div id = 'author'>Authors:<span id = 'author'>Ugo Finnendahl,Markus Worchel,Tobias Jüterbock,Daniel Wujecki,Fabian Brinkmann,Stefan Weinzierl,Marc Alexa
</span></div><div id = 'affiliation'><span id = 'affiliation'>TU Berlin,TU Berlin,TU Berlin,TU Berlin,TU Berlin,TU Berlin,TU Berlin
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_887&sess=sess131">Differentiable Geometric Acoustic Path Tracing using Time-Resolved Path Replay Backpropagation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Introducing differentiable path tracing for geometric acoustics with an efficient gradient algorithm based on path replay backpropagation. The system computes derivatives of output spectrograms with respect to arbitrary scene parameters (materials, geometry, emitters, microphones) within the framework of acoustic ray tracing, with applications demonstrated in various geometric scenarios.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>139
</span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Zhang,Yirui Yuan,Yiren Song,Jiaming Liu
</span></div><div id = 'affiliation'><span id = 'affiliation'>Shanghai Jiao Tong University,Shanghai Tech University,National University of Singapore,Tiamat AI
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_809&sess=sess149">StableMakeup: When Real-World Makeup Transfer Meets Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stable-Makeup is a diffusion-based makeup transfer method. It leverages a Detail-Preserving makeup encoder, and content-structure control modules to preserve facial content and structure during transfer. Extensive experiments show that Stable-Makeup outperforms existing methods, offering robust, generalizable performance.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>140
</span></div><div id = 'author'>Authors:<span id = 'author'>Sihui Ji,Yiyang Wang,Xi Chen,Xiaogang Xu,Hao Luo,Hengshuang Zhao
</span></div><div id = 'affiliation'><span id = 'affiliation'>The University of Hong Kong,The University of Hong Kong,The University of Hong Kong,The Chinese University of Hong Kong,DAMO Academy, Alibaba Group,The University of Hong Kong
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_576&sess=sess149">FashionComposer: Compositional Fashion Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>FashionComposer is a flexible model for compositional fashion image generation, with a universal framework that handles diverse input modalities such as text, human models, and garment images. It personalizes appearance, pose, and human figure, using subject-binding attention to integrate reference features, enabling applications like virtual try-ons and human album generation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>141
</span></div><div id = 'author'>Authors:<span id = 'author'>Liyuan Zhu,Shengqu Cai,Shengyu Huang,Gordon Wetzstein,Naji Khosravan,Iro Armeni
</span></div><div id = 'affiliation'><span id = 'affiliation'>Stanford University,Stanford University,NVIDIA Research,Stanford University,Zillow,Stanford University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_497&sess=sess149">ReStyle3D: Scene-Level Appearance Transfer with Semantic Correspondences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Redesign spaces effortlessly-ReStyle3D transforms indoor scenes by transferring object-specific styles from a single reference image, preserving 3D coherence. Combining semantic-aware diffusion and depth guidance, it enables photo-realistic virtual staging—faithfully redecorating furniture, textures, and decor. Ideal for interior design, our method outperforms existing approaches in realism, detail fidelity, and cross-view consistency.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>142
</span></div><div id = 'author'>Authors:<span id = 'author'>Ipek Oztas,Duygu Ceylan,Aysegul Dundar
</span></div><div id = 'affiliation'><span id = 'affiliation'>Bilkent University,Adobe Research,Bilkent University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_377&sess=sess149">3D Stylization via Large Reconstruction Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given a 3D object representing the source content and a reference style image, our method performs 3D stylization with a large pre-trained reconstruction model. This is achieved in a zero-shot manner, with no training or test time optimization required, while delivering superior visual fidelity and efficiency compared to existing approaches.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>143
</span></div><div id = 'author'>Authors:<span id = 'author'>Peiying Zhang,Nanxuan Zhao,Jing Liao
</span></div><div id = 'affiliation'><span id = 'affiliation'>City University of Hong Kong,Adobe Research,City University of Hong Kong
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_835&sess=sess149">Style Customization of Text-to-Vector Generation with Image Diffusion Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel text-to-vector pipeline with style customization that disentangles content and style in SVG generation. Our method represents the first feed-forward text-to-vector diffusion model capable of generating SVGs in custom styles.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>144
</span></div><div id = 'author'>Authors:<span id = 'author'>Junhao Zhuang,Lingen Li,Xuan Ju,Zhaoyang Zhang,Chun Yuan,Ying Shan
</span></div><div id = 'affiliation'><span id = 'affiliation'>Tsinghua University,Chinese University of Hong Kong,Chinese University of Hong Kong,Chinese University of Hong Kong,Tsinghua University,Tencent
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_539&sess=sess149">Cobra: Efficient Line Art COlorization with BRoAder References</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cobra is a novel efficient long-context fine-grained ID preservation framework for line art colorization, achieving high precision, efficiency, and flexible usability for comic colorization. By effectively integrating extensive contextual references, it transforms black-and-white line art into vibrant illustrations.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>145
</span></div><div id = 'author'>Authors:<span id = 'author'>Kaizhi Yang,Liu Dai,Isabella Liu,Xiaoshuai Zhang,Xiaoyan Sun,Xuejin Chen,Zexiang Xu,Hao Su
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Science and Technology of China,University of California San Diego,University of California San Diego,Hillbot Inc.,University of Science and Technology of China,University of Science and Technology of China,Hillbot Inc.,University of California San Diego
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1183&sess=sess103">IMLS-Splatting: Efficient Mesh Reconstruction from Multi-view Images via Point Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose IMLS-Splatting, an end-to-end multi-view mesh optimization method that leverages point clouds for surface representation. By introducing a splatting-based differentiable IMLS algorithm, our approach efficiently converts point clouds into SDF and texture field, enabling multi-view mesh optimization in approximately 11 minutes and achieving state-of-the-art reconstruction performance.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>146
</span></div><div id = 'author'>Authors:<span id = 'author'>Mingyang Song,Yang Zhang,Marko Mihajlovic,Siyu Tang,Markus Gross,Tunc Ozan Aydin
</span></div><div id = 'affiliation'><span id = 'affiliation'>Disney Research Studios,Disney Research Studios,ETH Zürich,ETH Zürich,ETH Zürich,Disney Research Studios
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_646&sess=sess103">Spline Deformation Field</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We combine splines,  a classical tool from applied mathematics, with implicit Coordinate Neural Networks to model deformation fields, achieving strong performance across multiple datasets. The explicit regularization from spline interpolation enhances spatial coherency in challenging scenarios. We further introduce a metric based on Moran’s I to quantitatively evaluate spatial coherence.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>147
</span></div><div id = 'author'>Authors:<span id = 'author'>Selena Ling,Merlin Nimier-David,Alec Jacobson,Nicholas Sharp
</span></div><div id = 'affiliation'><span id = 'affiliation'>NVIDIA Research,NVIDIA Research,University of Toronto,NVIDIA Research
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_492&sess=sess103">Stochastic Preconditioning for Neural Field Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stochastic preconditioning adds spatial noise to query locations during neural field optimization; it can be formalized as a stochastic estimate for a blur operator. This simple technique eases optimization and significantly improves quality for neural fields optimization, matching or outperforming custom-designed policies and coarse-to-fine schemes.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>148
</span></div><div id = 'author'>Authors:<span id = 'author'>Weizhou Liu,Jiaze Li,Xuhui Chen,Fei Hou,Shiqing Xin,Xingce Wang,Zhongke Wu,Chen Qian,Ying He,Ying He
</span></div><div id = 'affiliation'><span id = 'affiliation'>Beijing Normal University,Nanyang Technological University,Institute of Software, Chinese Academy of Sciences,Institute of Software, Chinese Academy of Sciences,Shandong University,Beijing Normal University,Beijing Normal University,SenseTime Group,Nanyang Technological University  College of Computing and Data Science,Nanyang Technological University, School of Computer Science and Engineering
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_105&sess=sess103">Diffusing Winding Gradients (DWG): A Parallel and Scalable Method for 3D Reconstruction from Unoriented Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusing Winding Gradients (DWG) efficiently reconstructs watertight 3D surfaces from unoriented point clouds. Unlike conventional methods, DWG avoids solving linear systems or optimizing objective functions, enabling simple implementation and parallel execution. Our CUDA implementation on an NVIDIA GTX 4090 GPU runs 30–120x faster than iPSR on large-scale models (10–20 million points).
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>149
</span></div><div id = 'author'>Authors:<span id = 'author'>Jianjun Xia,Tao Ju
</span></div><div id = 'affiliation'><span id = 'affiliation'>Washington University in St. Louis,Washington University in St. Louis
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_881&sess=sess103">Variational Surface Reconstruction Using Natural Neighbors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduced a new surface reconstruction method from points without normals. The method robustly handles undersampled regions and scales to large input sizes.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>150
</span></div><div id = 'author'>Authors:<span id = 'author'>Kaixin Yao,Longwen Zhang,Xinhao Yan,Yan Zeng,Qixuan Zhang,Jiayuan Gu,Wei Yang,Lan Xu,Jingyi Yu
</span></div><div id = 'affiliation'><span id = 'affiliation'>ShanghaiTech University,ShanghaiTech University,ShanghaiTech University,ShanghaiTech University,ShanghaiTech University,ShanghaiTech University,Huazhong University of Science and Technology,ShanghaiTech University,ShanghaiTech University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_364&sess=sess103">CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce CAST, an innovative method for reconstructing high-quality 3D scenes from a single RGB  image. Supporting open-vocabulary reconstruction, CAST excels in managing occlusions, aligning objects accurately, and ensuring physical consistency with the input, unlocking new possibilities in virtual content creation and robotics.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>151
</span></div><div id = 'author'>Authors:<span id = 'author'>Huanyu Chen,Jiahao Wen,Jernej Barbič
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Southern California,University of Southern California,University of Southern California
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1094&sess=sess147">ANIME-Rod: Adjustable Nonlinear Isotropic Materials for Elastic Rods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We derive a nonlinear elastic rod energy, starting from a general 3D volumetric isotropic material. Validated against FEM, we accurately capture rod stretching, bending and twisting, under finite deformations. We also propose how to separately control linear/nonlinear stretchability/bendability/twistability, supporting rod material design for application in computer graphics.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>152
</span></div><div id = 'author'>Authors:<span id = 'author'>Jerry Hsu,Tongtong Wang,Kui Wu,Cem Yuksel
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Utah,LightSpeed Studios,LightSpeed Studios,University of Utah
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_283&sess=sess147">Stable Cosserat Rods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cosserat rods have become increasingly popular for simulating complex thin elastic rods. However, traditional approaches often encounter significant challenges in robustly and efficiently solving for valid quaternion orientations. We introduce Stable Cosserat rods, which can achieve high accuracy with high stiffness levels and maintain stability under large time steps.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>153
</span></div><div id = 'author'>Authors:<span id = 'author'>Ziqiu Zeng,Siyuan Luo,Fan Shi,Zhongkai Zhang
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Strasbourg,National University of Singapore,National University of Singapore,Centre for Artificial Intelligence and Robotics, Hong Kong, CAS
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_308&sess=sess147">Fast But Accurate: A Real-Time Hyperelastic Simulator with Robust Frictional Contact</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a GPU-friendly framework for real-time implicit simulation of hyperelastic materials with frictional contacts. Utilizing a novel splitting strategy and efficient solver, the approach achieves robust, high-performance simulation across various stiffness materials, handling large deformations and precise friction interactions with remarkable efficiency, accuracy, and generality.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>154
</span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Wen,Jernej Barbic,Danny Kaufman
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Southern California,University of Southern California,Adobe Research
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1036&sess=sess147">Optimal r-Adaptive In-Timestep Remeshing for Elastodynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a coupled mesh-adaptation model and physical simulation algorithm to jointly generate, per timestep, optimal adaptive remeshings and implicit solutions for the simulation of frictionally contacting, large-deformation elastica.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>155
</span></div><div id = 'author'>Authors:<span id = 'author'>Leticia Mattos Da Silva,Silvia Sellán,Natalia Pacheco-Tallaj,Justin Solomon
</span></div><div id = 'affiliation'><span id = 'affiliation'>Massachusetts Institute of Technology (MIT),Massachusetts Institute of Technology (MIT),Massachusetts Institute of Technology (MIT),Massachusetts Institute of Technology (MIT)
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1014&sess=sess147">Variational Elastodynamic Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper shows how to express variational time integration for a large class of elastic energies as an optimization problem with a “hidden” convex substructure. Our integrator improves the performance of elastic simulation tasks, while conserving physical invariants up to tolerance/numerical precision.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>156
</span></div><div id = 'author'>Authors:<span id = 'author'>Chris Giles,Elie Diaz,Cem Yuksel
</span></div><div id = 'affiliation'><span id = 'affiliation'>Roblox,University of Utah,University of Utah
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_909&sess=sess147">Augmented Vertex Block Descent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We extend the Vertex Block Descent method for fast and unconditionally stable physics-based simulation using an Augmented Lagrangian formulation to enable simulating hard constraints with infinite stiffness and systems with high stiffness ratios. This allows simulating complex contact scenarios involving rigid bodies with stacking and friction, and articulated joint constraints.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>157
</span></div><div id = 'author'>Authors:<span id = 'author'>John Desnoyers-Stewart,Noah Miller,Bernhard Riecke
</span></div><div id = 'affiliation'><span id = 'affiliation'>Univeristy of British Columbia,Simon Fraser University,Simon Fraser University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=artpl_147&sess=sess109">Synedelica: Mixed Reality Reimagined</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synedelica challenges traditional approaches to mixed reality by transforming physical environments through a synesthetic experience. This artwork emphasizes the potential for immersive technology to mediate reality itself, fostering social interaction and shared experiences. By reimagining how we perceive and interact with our surroundings, Synedelica opens new perspectives at the intersection between virtual and physical. Our approach encourages the SIGGRAPH community to explore the innovative capacity of intuitive and serendipitous design.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>158
</span></div><div id = 'author'>Authors:<span id = 'author'>Yuqian Sun,Chenhang Cheng,Chuyan Xu,Chang Hee Lee,Ali Asadipour
</span></div><div id = 'affiliation'><span id = 'affiliation'>Computer Science Research Centre, Royal College of Art,Individual,Individual,Korea Advanced Institute of Science and Technology (KAIST),Computer Science Research Centre, Royal College of Art
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=artpl_166&sess=sess109">Hyborg Agency: Fostering AI Agents Through Community Conversations in a Digital Forest</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyborg Agency proposes an artistic perspective on AI agents: We can design AI agents that maintain their distinct non-human nature while meaningfully participating in human social contexts.Presenting AI agents as mechanical deer nurtured by community conversations, this computational ecosystem demonstrates how defamiliarized AI agents can enrich human social experiences while promoting transparency about their artificial nature, contributing to more sustainable and ethical human-AI symbiotic relationship.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>159
</span></div><div id = 'author'>Authors:<span id = 'author'>Xueqi Ma,Yilin Liu,Tianlong Gao,Qirui Huang,Hui Huang
</span></div><div id = 'affiliation'><span id = 'affiliation'>Shenzhen University,Shenzhen University,Shenzhen University,Shenzhen University,Shenzhen University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_386&sess=sess128">CLR-Wire: Towards Continuous Latent Representations for 3D Curve Wireframe Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>CLR-Wire is a unified generative framework for 3D curve-based wireframes, jointly modeling geometry and topology in a continuous latent space. Using attention-driven VAEs and flow matching, it enables high-quality, diverse generation from noise, images, or point clouds—advancing CAD design, shape reconstruction, and 3D content creation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>160
</span></div><div id = 'author'>Authors:<span id = 'author'>Jionghao Wang,Cheng Lin,Yuan Liu,Rui Xu,Zhiyang Dou,Xiaoxiao Long,Haoxiang Guo,Taku Komura,Xin Li,Wenping Wang
</span></div><div id = 'affiliation'><span id = 'affiliation'>Texas A&M University,University of Hong Kong,HKUST,University of Hong Kong,University of Hong Kong,Nanjing University,Skywork AI, Kunlun Inc.,University of Hong Kong,Texas A&M University,Texas A&M University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_921&sess=sess128">PDT: Point Distribution Transformation with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>PDT is a novel framework that uses diffusion models to transform unstructured point clouds into semantically meaningful and structured distributions, such as keypoints, joints, and feature lines. Exploring complex point distribution transformation, PDT captures fine-grained geometry and semantics, offering a versatile tool for diverse tasks.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>161
</span></div><div id = 'author'>Authors:<span id = 'author'>Xiao-Lei Li,Hao-Xiang Chen,Yanni Zhang,Kai Ma,Alan Zhao,Tai-Jiang Mu,Haoxiang Guo,Ran Zhang
</span></div><div id = 'affiliation'><span id = 'affiliation'>Tsinghua University,Tsinghua University,Tencent Video AI Center,Tencent PCG,Tencent Video AI Center,Tsinghua University,Skywork AI, Kunlun Inc.,Tencent Video AI Center
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_454&sess=sess128">RELATE3D: REfocusing Latent Adapter for Targeted local Enhancement and Editing in 3D Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The alignment of text,images,and 3D is very challenging,yet it is crucial and beneficial for many tasks.We explore and reveal the characteristics of the native 3D latent space for 3D generation,make it decomposable and low-rank,thereby enabling efficient learning for multimodal local alignment,achieving precise local enhancement and part-level editing of 3D geometry.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>162
</span></div><div id = 'author'>Authors:<span id = 'author'>Yansong Qu,Dian Chen,Xinyang Li,Xiaofan Li,Shengchuan Zhang,Liujuan Cao,Rongrong Ji
</span></div><div id = 'affiliation'><span id = 'affiliation'>Xiamen University,Xiamen University,Xiamen University,Baidu Inc.,Xiamen University,Xiamen University,Xiamen University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_195&sess=sess128">Drag Your Gaussian: Effective Drag-Based Editing  with Score Distillation for 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DYG is a 3D drag-based scene editing method for Gaussian Splatting that enables precise, multi-view consistent geometric edits using 3D masks and control points. It combines implicit triplane representation and a drag-based diffusion model for high-quality, fine-grained results. Visit our project page at \url{https://drag-your-gaussian.github.io/}.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>163
</span></div><div id = 'author'>Authors:<span id = 'author'>Peng Li,Suizhi Ma,Jialiang Chen,Yuan Liu,Congyi Zhang,Wei Xue,Wenhan Luo,Alla Sheffer,Wenping Wang,Yike Guo
</span></div><div id = 'affiliation'><span id = 'affiliation'>Hong Kong University of Science and Technology,Johns Hopkins University,Hong Kong University of Science and Technology,Hong Kong University of Science and Technology,Univeristy of British Columbia,Hong Kong University of Science and Technology,Hong Kong University of Science and Technology,Univeristy of British Columbia,Texas A&M University,Hong Kong University of Science and Technology
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_987&sess=sess128">CMD: Controllable Multiview Diffusion for 3D Editing and Progressive Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>CMD revolutionizes 3D generation by enabling flexible local editing of 3D models from a single rendering, as well as progressive, interactive creation of complex 3D scenes. At its core, CMD leverages a conditional multiview diffusion model to seamlessly modify/add new components—enhancing control, quality, and efficiency in 3D content creation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>164
</span></div><div id = 'author'>Authors:<span id = 'author'>Ellie Arar,Yarden Frenkel,Daniel Cohen-Or,Ariel Shamir,Yael Vinker
</span></div><div id = 'affiliation'><span id = 'affiliation'>Tel Aviv University,Tel Aviv University,Tel Aviv University,Reichman University,Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT)
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_264&sess=sess128">SwiftSketch: A Diffusion Model for Image-to-Vector Sketch Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>SwiftSketch, a diffusion-based model with a transformer-decoder, generates high-quality vector sketches from images in under a second. It progressively denoises stroke coordinates sampled from a Gaussian distribution, effectively generalizing across various object classes. Training uses the ControlSketch Dataset,  a new synthetic high-quality image-sketch pairs created by our ControlSketch optimization method.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>165
</span></div><div id = 'author'>Authors:<span id = 'author'>Sinan Wang,Junwei Zhou,Fan Feng,Zhiqi Li,Yuchen Sun,Duowen Chen,Greg Turk,Bo Zhu
</span></div><div id = 'affiliation'><span id = 'affiliation'>Georgia Institute of Technology,University of Michigan Ann Arbor,Dartmouth College,Georgia Institute of Technology,Georgia Institute of Technology,Georgia Institute of Technology,Georgia Institute of Technology,Georgia Institute of Technology
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_951&sess=sess145">Fluid Simulation on Vortex Particle Flow Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the Vortex Particle Flow Map (VPFM) method, which revitalizes the traditional Vortex-In-Cell approach for computer graphics. By evolving vorticity and higher-order quantities along particle flow maps, our method achieves significantly improved long-term stability and vorticity preservation, enabling high-fidelity simulation of complex vortical fluid motions in 2D and 3D.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>166
</span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqi Li,Candong Lin,Duowen Chen,Xinyi Zhou,Shiying Xiong,Bo Zhu
</span></div><div id = 'affiliation'><span id = 'affiliation'>Georgia Institute of Technology,Georgia Institute of Technology,Georgia Institute of Technology,Georgia Institute of Technology,Zhejiang University,Georgia Institute of Technology
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_903&sess=sess145">Clebsch Gauge Fluid on Particle Flow Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a Clebsch PFM fluid solver that accurately transports wave functions using particle flow maps. Key innovations include a new gauge transformation, improved velocity reconstruction on coarse grids, and better fine-scale structure preservation. Benchmarks show superior performance over impulse- or vortex-based methods, especially for small-scale flow features.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>167
</span></div><div id = 'author'>Authors:<span id = 'author'>Mengdi Wang,Fan Feng,Junlin Li,Bo Zhu
</span></div><div id = 'affiliation'><span id = 'affiliation'>Georgia Institute of Technology,Dartmouth College,Georgia Institute of Technology,Georgia Institute of Technology
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_879&sess=sess145">Cirrus: Adaptive Hybrid Particle-Grid Flow Maps on GPU</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce an adaptive octree-based GPU simulator for large-scale fluid simulation. Our hybrid particle-grid flow map advection scheme effectively preserves vortex details, enabling high-resolution and high-quality results. The source code has been made publicly available at: https://wang-mengdi.github.io/proj/25-cirrus/.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>168
</span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Sun,Junlin Li,Ruicheng Wang,Sinan Wang,Zhiqi Li,Bart G. van Bloemen Waanders,Bo Zhu
</span></div><div id = 'affiliation'><span id = 'affiliation'>Georgia Institute of Technology,Georgia Institute of Technology,Georgia Institute of Technology,Georgia Institute of Technology,Georgia Institute of Technology,Sandia National Laboratories,Georgia Institute of Technology
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_689&sess=sess145">Leapfrog Flow Maps for Real-Time Fluid Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Leapfrog Flow Maps (LFM), a fast hybrid velocity-impulse scheme with leapfrog integration. The computations are further accelerated by a matrix-free AMGPCG solver optimized for GPUs. As a result, LFM achieves high performance and fidelity across diverse examples, including fireballs and wingtip vortices.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>169
</span></div><div id = 'author'>Authors:<span id = 'author'>Duowen Chen,Zhiqi Li,Taiyuan Zhang,Jinjin He,Junwei Zhou,Bart G. van Bloemen Waanders,Bo Zhu
</span></div><div id = 'affiliation'><span id = 'affiliation'>Georgia Institute of Technology,Georgia Institute of Technology,Dartmouth College,Dartmouth College,University of Michigan,Sandia National Laboratories,Georgia Institute of Technology
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_899&sess=sess145">Fluid Simulation on Compressible Flow Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a unified compressible flow map framework based on Lagrangian path integrals, enabling conservative density-energy transport and flexible pressure treatments. Validated on diverse systems—from shocks to shallow water—it captures complex flow features like vortices and wave interactions, broadening flow-map applicability across compressibility regimes and fluid morphologies.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>170
</span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqi Li,Ruicheng Wang,Junlin Li,Duowen Chen,Sinan Wang,Bo Zhu
</span></div><div id = 'affiliation'><span id = 'affiliation'>Georgia Institute of Technology,Georgia Institute of Technology,Georgia Institute of Technology,Georgia Institute of Technology,Georgia Institute of Technology,Georgia Institute of Technology
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_902&sess=sess145">EDGE: Epsilon-Difference Gradient Evolution for Buffer-Free Flow Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents Epsilon Difference Gradient Evolution (EDGE), a novel method for accurate flow-map computation on grids without velocity buffers. EDGE enables large-scale, efficient and high-fidelity fluid simulations that capture and preserve complex vorticity structures while significantly reducing memory usage.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>171
</span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Bonneel,David Coeurjolly,Jean-Claude Iehl,Victor Ostromoukhov
</span></div><div id = 'affiliation'><span id = 'affiliation'>CNRS - LIRIS,CNRS - LIRIS,Université Claude Bernard Lyon 1,Université Claude Bernard Lyon 1
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_126&sess=sess127">Sobol' Sequences with Guaranteed-Quality 2D Projections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of quasi-Monte Carlo rendering, we introduce a new Sobol' construction and demonstrate that particular pairs of polynomials of the form p and p^2+p+1 in Sobol'-based sampling lead to (1, 2)-sequences. They can be combined to form high-dimensional low discrepancy sequences with good 2D projections.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>172
</span></div><div id = 'author'>Authors:<span id = 'author'>Qingqin Hua,Pascal Grittmann,Philipp Slusallek
</span></div><div id = 'affiliation'><span id = 'affiliation'>Saarland University,Saarland University,Saarland University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_117&sess=sess127">Correct your balance heuristic: Optimizing balance-style multiple importance sampling weights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple importance sampling (MIS) is vital to most rendering algorithms. MIS computes a weighted sum of samples from different techniques to handle diverse scene types and lighting effects.We propose a practical weight correction scheme that yields better equal-time performance on bidirectional algorithms and resampled importance sampling for direct illumination.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>173
</span></div><div id = 'author'>Authors:<span id = 'author'>Corentin Salaun,Martin Balint,Laurent Belcour,Eric Heitz,Gurprit Singh,Karol Myszkowski
</span></div><div id = 'affiliation'><span id = 'affiliation'>Max Planck Institute for Informatics,Max Planck Institute for Informatics,Intel,Intel,Max Planck Institute for Informatics,Max Planck Institute for Informatics
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1010&sess=sess127">Histogram Stratification for Spatio-Temporal Reservoir Sampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces stratification into resampled importance sampling (RIS) technique for real-time photorealistic rendering. It organizes sample candidates into local histograms and then employs Quasi Monte Carlo and antithetic patterns for efficient sampling. This low-overhead approach significantly reduces rendering noise, improving visual quality compared to existing methods.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>174
</span></div><div id = 'author'>Authors:<span id = 'author'>Zhimin Fan,Yiming Wang,Chenxi Zhou,Ling-Qi Yan,Yanwen Guo,Jie Guo
</span></div><div id = 'affiliation'><span id = 'affiliation'>Nanjing University,Nanjing University,Nanjing University,University of California Santa Barbara,Nanjing University,Nanjing University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_113&sess=sess127">Multiple Importance Reweighting for Path Guiding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We combine the estimates generated in each guiding iteration, leveraging the importance distributions from multiple guiding iterations. We demonstrate that our path-level reweighting makes guiding algorithms less sensitive to noise and overfitting in distributions.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>175
</span></div><div id = 'author'>Authors:<span id = 'author'>Haolin Lu,Delio Vicini,Wesley Chang,Tzu-Mao Li
</span></div><div id = 'affiliation'><span id = 'affiliation'>UC San Diego,Google Inc.,UC San Diego,UC San Diego
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_635&sess=sess127">Vector-Valued Monte Carlo Integration Using Ratio Control Variates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Variance reduction techniques are widely used to reduce the noise of Monte Carlo integration. However, these techniques are typically designed with the assumption that the integrand is scalar-valued. To address this, we introduce ratio control variations, an estimator that leverages a ratio-based approach instead of the conventional difference-based control variates.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>176
</span></div><div id = 'author'>Authors:<span id = 'author'>Xiaochun Tong,Toshiya Hachisuka
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Waterloo,University of Waterloo
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_702&sess=sess127">Practical Stylized Nonlinear Monte Carlo Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a practical method for rendering scenes with complex, recursive nonlinear stylization applied to physically based rendering. Our approach introduces nonlinear path filtering(NL-PF) and nonlinear neural radiance caching(NL-NRC), which reduce the exponential sampling cost of stylized rendering to polynomial, enabling rendering of nonlinear stylization with significantly improved efficiency.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>177
</span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Wang,Wenjun Liu,Jinwen Wang,Gabriel Vallat,Fan Shi,Stefana Parascho,Maryam Kamgarpour
</span></div><div id = 'affiliation'><span id = 'affiliation'>HKUST,HKUST,EPFL,EPFL,National University of Singapore,EPFL,EPFL
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_150&sess=sess137">Learning to Assemble with Alternative Plans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a reinforcement learning framework for assembling structures composed of rigid parts. A pre-trained policy generates alternative assembly plans, enabling rapid adaptation to unexpected disruptions. Our approach supports efficient and robust planning for multi-robot assembly tasks.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>178
</span></div><div id = 'author'>Authors:<span id = 'author'>Lucas N. Alegre,Agon Serifi,Ruben Grandia,David Müller,Espen Knoop,Moritz Bächer
</span></div><div id = 'affiliation'><span id = 'affiliation'>Instituto de Informática - Universidade Federal do Rio Grande do Sul,Disney Research,Disney Research,Disney Research,Disney Research,Disney Research
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_504&sess=sess137">AMOR: Adaptive Character Control through Multi-Objective Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Presenting AMOR, a policy conditioned on context and a linear combination of reward weights, trained using multi-objective reinforcement learning. Once trained, AMOR allows for on-the-fly adjustments of reward weights, unlocking new possibilities in physics-based and robotic character control.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>179
</span></div><div id = 'author'>Authors:<span id = 'author'>Hewen Xiao,Xiuping Liu,Hang Zhao,Jian Liu,Kai Xu
</span></div><div id = 'affiliation'><span id = 'affiliation'>Dalian University of Technology,Dalian University of Technology,Wuhan University,Shenyang University of Technology,National University of Defense Technology (NUDT)
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_688&sess=sess137">Designing Pin-pression Gripper and Learning its Dexterous Grasping with Online In-hand Adjustment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a pin-pression gripper featuring parallel-jaw fingers with 2D arrays of independently extendable pins, allowing instant shape adaptation to target object geometry and dynamic in-hand re-orientation for enhanced grasp stability. Reinforcement learning with curriculum-based training enables flexible, robust grasping and grasp-while-lift mode, validated by sim-to-real experiments with superior performance.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>180
</span></div><div id = 'author'>Authors:<span id = 'author'>Sean Memery,Kevin Denamganaï,Jiaxin Zhang,Zehai Tu,Yiwen Guo,Kartic Subr
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Edinburgh,University of Edinburgh,Lightspeed Studios,Lightspeed Studios,Independent,University of Edinburgh
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1166&sess=sess137">CueTip: An Interactive and Explainable Physics-aware Pool Assistant</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>CueTip is an interactive and explainable automated coaching assistant for a variant of pool/billiards. CueTip has a natural-language interface, the ability to perform contextual, physics-aware reasoning, and its explanations are rooted in a set of predetermined guidelines developed by domain experts. CueTip matches SOTA performance, with grounded and reliable explanations.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>181
</span></div><div id = 'author'>Authors:<span id = 'author'>Wenbin Song,Heng Zhang,Yang Wang,Xiaopei Liu
</span></div><div id = 'affiliation'><span id = 'affiliation'>ShanghaiTech University,ShanghaiTech University,ShanghaiTech University,ShanghaiTech University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_472&sess=sess137">Creating Fluid-Interactive Virtual Agents by an Efficient Simulator with Local-domain Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel local-domain fluid-solid interaction simulator grounded in a lattice Boltzmann solver. By leveraging an MPC-based domain-tracking approach and an improved convective boundary condition, it offers enhanced stability and efficiency for deriving control policies of virtual agents, holding great promise for applications in both computer animation and robotics.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>182
</span></div><div id = 'author'>Authors:<span id = 'author'>Mingfeng Tang,Ningna Wang,Ziyuan Xie,Jianwei Hu,Ke Xie,Xiaohu Guo,Hui Huang
</span></div><div id = 'affiliation'><span id = 'affiliation'>Shenzhen University,University of Texas at Dallas,Shenzhen University,QiYuan Lab,Shenzhen University,University of Texas at Dallas,Shenzhen University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_387&sess=sess137">Aerial Path Online Planning for Urban Scene Updation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the first scene-update aerial path planning algorithm specifically designed for detecting and updating change areas in urban environments, which paves the way for efficient, scalable, and adaptive UAV-based scene updates in complex urban environments.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>183
</span></div><div id = 'author'>Authors:<span id = 'author'>Yifang Pan,Karan Singh,Luiz Gustavo Hafemann
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Toronto,University of Toronto,Ubisoft
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_614&sess=sess153">Model See Model Do: Speech-Driven Facial Animation with Style Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>ModelSeeModelDo presents a speech-driven 3D facial animation method using a latent diffusion model conditioned on a reference clip to capture nuanced performance styles. A novel "style basis" mechanism extracts key poses to guide generation, achieving expressive, temporally coherent animations with accurate lip-sync and strong stylistic fidelity across diverse speech inputs.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>184
</span></div><div id = 'author'>Authors:<span id = 'author'>Zhen Han,Mattias Teye,Derek Yadgaroff,Judith Bütepage
</span></div><div id = 'affiliation'><span id = 'affiliation'>Electronic Arts,Electronic Arts,Electronic Arts,Electronic Arts
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1216&sess=sess153">Tiny is not small enough: High quality, low-resource facial animation models through hybrid knowledge distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of this work is to train lip sync animation models that can run in real-time and on-device. We design a two-stage knowledge distillation framework to distill large, high-quality models. Our results show that we can train small models with low latency and a comparatively small loss in quality.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>185
</span></div><div id = 'author'>Authors:<span id = 'author'>Luchuan Song,Yang Zhou,Zhan Xu,Yi Zhou,Deepali Aneja,Chenliang Xu
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Rochester,Adobe Research,Adobe Research,Adobe Research,Adobe Research,University of Rochester
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_366&sess=sess153">StreamME: Simplify 3D Gaussian Avatar within Live Stream</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The StreamME takes live stream video as input to enable rapid 3D head avatar reconstruction. It achieves impressive speed, capturing the basic facial appearance within 10-seconds and reaching high-quality fidelity within 5-minutes. StreamME reconstructs facial features through on-the-fly training, allowing simultaneous recording and modeling without the need for pre-cached data.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>186
</span></div><div id = 'author'>Authors:<span id = 'author'>Shivangi Aneja,Sebastian Weiss,Irene Baeza,Prashanth Chandran,Gaspard Zoss,Matthias Niessner,Derek Bradley
</span></div><div id = 'affiliation'><span id = 'affiliation'>Technical University of Munich,DisneyResearch|Studios,DisneyResearch|Studios,DisneyResearch|Studios,DisneyResearch|Studios,Technical University Munich,DisneyResearch|Studios
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1048&sess=sess153">ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>ScaffoldAvatar presents a novel approach for reconstructing ultra-high fidelity animatable head avatars, which can be rendered in real-time. Our method operates on patch-based local expression features and synthesizes 3D Gaussians dynamically by leveraging tiny scaffold MLPs. We employ color-based densification and progressive training to obtain high-quality results and fast convergence.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>187
</span></div><div id = 'author'>Authors:<span id = 'author'>Forrest Iandola,Stanislav Pidhorskyi,Igor Santesteban,Divam Gupta,Anuj Pahuja,Nemanja Bartolovic,Frank Yu,Emanuel Garbin,Tomas Simon,Shunsuke Saito
</span></div><div id = 'affiliation'><span id = 'affiliation'>Meta,Meta,Meta,Meta,Meta,Meta,Meta,Meta,Meta,Meta
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_192&sess=sess153">SqueezeMe: Mobile-Ready Distillation of Gaussian Full-Body Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Gaussian Splatting avatars require desktop GPUs, limiting mobile device use. SqueezeMe converts these avatars into a lightweight representation, enabling real-time animation and rendering on mobile devices. By distilling the corrective decoder into an efficient linear model, SqueezeMe achieves 72 FPS on a Meta Quest 3 VR headset.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>188
</span></div><div id = 'author'>Authors:<span id = 'author'>Wesley Chang,Andrew Russell,Stephane Grabli,Matt Chiang,Christophe Hery,Doug Roble,Ravi Ramamoorthi,Tzu-Mao Li,Olivier Maury
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of California San Diego,University of California San Diego,Meta,Meta,Meta,Meta,University of California San Diego,University of California San Diego,Meta
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_566&sess=sess153">Transforming Unstructured Hair Strands into Procedural Hair Grooms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent methods have been developed to reconstruct 3D hair strand geometry from images. We introduce an inverse hair grooming pipeline to transform these unstructured hair strands into procedural hair grooms controlled by a small set of guide strands and artist-friendly grooming operators, enabling easy editing of hair shape and style.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>189
</span></div><div id = 'author'>Authors:<span id = 'author'>Omer Dahary,Yehonathan Cohen,Or Patashnik,Kfir Aberman,Daniel Cohen-Or
</span></div><div id = 'affiliation'><span id = 'affiliation'>Tel Aviv University,Tel Aviv University,Tel Aviv University,Snap Research,Tel Aviv University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_331&sess=sess152">Be Decisive: Noise-Induced Layouts for Multi-Subject Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-image diffusion models struggle with multi-subject generation due to subject leakage. Prior methods impose external layouts that conflict with the model’s prior, harming alignment and natural composition. We introduce a method that leverages the layout encoded in the initial noise, promoting alignment and natural compositions while preserving the model’s diversity.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>190
</span></div><div id = 'author'>Authors:<span id = 'author'>Amirhossein Alimohammadi,Aryan Mikaeili,Sauradip Nag,Negar Hassanpour,Andrea Tagliasacchi,Ali Mahdavi-Amiri
</span></div><div id = 'affiliation'><span id = 'affiliation'>Simon Fraser University,Simon Fraser University,Simon Fraser University,Huawei Canada,Simon Fraser University,Simon Fraser University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_467&sess=sess152">Cora: Correspondence-aware image editing using few step diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cora is a novel diffusion-based image editing method that achieves complex edits, such as object insertion, background changes, and non-rigid transformations, in only four diffusion steps. By leveraging pixel-wise semantic correspondences between source and target, it preserves key elements of the original image’s structure and appearance while introducing new content.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>191
</span></div><div id = 'author'>Authors:<span id = 'author'>Linjie Lyu,Valentin Deschaintre,Yannick Hold-Geoffroy,Milos Hasan,Jae Shin Yoon,Thomas Leimkuehler,Christian Theobalt,Iliyan Georgiev
</span></div><div id = 'affiliation'><span id = 'affiliation'>Max-Planck-Institute for Informatics & Saarland Informatics Campus,Adobe Research,Adobe Research,Adobe Research,Adobe Research,Max-Planck-Institute for Informatics & Saarland Informatics Campus,Max-Planck-Institute for Informatics & Saarland Informatics Campus,Adobe Research
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_613&sess=sess152">IntrinsicEdit: Precise generative image manipulation in intrinsic space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A generative workflow for precise image editing using an intrinsic-image latent space. Built on RGB-X diffusion, it enables diverse edits—like relighting, color changes, and object manipulation—while preserving identity and ameliorating intrinsic-channel entanglement. All this is done without extra data or fine-tuning, achieving state-of-the-art results.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>192
</span></div><div id = 'author'>Authors:<span id = 'author'>Yen-Chi Cheng,Krishna Kumar Singh,Jae Shin Yoon,Alexander Schwing,Liang-Yan Gui,Matheus Gadelha,Paul Guerrero,Nanxuan Zhao
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Illinois Urbana-Champaign,Adobe Research,Adobe Research,University of Illinois Urbana-Champaign,University of Illinois Urbana-Champaign,Adobe Research,Adobe Research,Adobe Research
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_768&sess=sess152">3D-Fixup: Advancing Photo Editing with 3D Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D-Fixup enables realistic 3D-aware photo editing by leveraging 3D priors and a novel data pipeline that extracts training pairs from real-world videos. Its feed-forward architecture supports efficient, high-quality edits involving complex 3D transformations while preserving object identity, outperforming prior methods in both edit accuracy and user control.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>193
</span></div><div id = 'author'>Authors:<span id = 'author'>Niladri Shekhar Dutt,Duygu Ceylan,Niloy Mitra
</span></div><div id = 'affiliation'><span id = 'affiliation'>University College London (UCL),Adobe,University College London (UCL)
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1195&sess=sess152">MonetGPT: Solving Puzzles Enhances MLLMs=E2=80=99 Image Retouching Skills</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>MonetGPT explores using multimodal large language models (MLLMs) for photo retouching by injecting domain knowledge via visual puzzles. These puzzles help MLLMs understand individual operations,  visual aesthetics, and generate expert plans. Our procedural pipeline enables explainable edits with detailed reasoning for the plan and individual operations.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>194
</span></div><div id = 'author'>Authors:<span id = 'author'>Aleksandar Cvejic,Abdelrahman Eldesokey,Peter Wonka
</span></div><div id = 'affiliation'><span id = 'affiliation'>King Abdullah University of Science and Technology (KAUST),King Abdullah University of Science and Technology (KAUST),King Abdullah University of Science and Technology (KAUST)
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1255&sess=sess152">PartEdit: Fine-Grained Image Editing using Pre-Trained Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PartEdit, a novel diffusion-based system enabling precise, text-based edits of object parts without retraining or manual masks. Optimizing part-aware tokens generates localized non-binary attention maps to guide seamless edits. Our novel blending strategy delivers high-quality visual results and outperforms prior techniques in both synthetic and real-world scenarios.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>195
</span></div><div id = 'author'>Authors:<span id = 'author'>Mia Tang,Yael Vinker,Chuan Yan,Lvmin Zhang,Maneesh Agrawala
</span></div><div id = 'affiliation'><span id = 'affiliation'>Stanford University,Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT),Stanford University,Stanford University,Stanford University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_224&sess=sess152">Instance Segmentation of Scene Sketches Using Natural Image Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>INKi enables instance segmentation for scene sketches by adapting image segmentation models with class-agnostic tuning and depth-based refinement. We introduce a new dataset INK-scene with diverse styles and demonstrate layered sketch organization for advanced editing, including inpainting occluded instances—paving the way for robust, editable sketch understanding.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>196
</span></div><div id = 'author'>Authors:<span id = 'author'>Jiabao Brad Wang,Amir Vaxman
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Edinburgh,University of Edinburgh
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_128&sess=sess118">Power-Linear Polar Directional Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a method for designing smooth directional fields on triangle meshes with precise control over singularities. Our approach uses a power-linear polar representation, allowing singularities of any index to be placed anywhere on the mesh. The resulting fields are smooth, robust to mesh quality, and support N-fold symmetry.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>197
</span></div><div id = 'author'>Authors:<span id = 'author'>Qiujie Dong,Huibiao Wen,Rui Xu,Shuangmin Chen,Jiaran Zhou,Shiqing Xin,Changhe Tu,Taku Komura,Wenping Wang
</span></div><div id = 'affiliation'><span id = 'affiliation'>Shandong University,Shandong University,The University of Hong Kong,Qingdao University of Science and Technology,Ocean University of China,Shandong University,Shandong University,The University of Hong Kong,Texas A&M University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_449&sess=sess118">NeurCross: A Neural Approach to Computing Cross Fields for Quad Mesh Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose NeurCross, a self-supervised framework for quadrilateral mesh generation that jointly optimizes principal curvature direction field and cross field by employing an optimizable neural SDF to approximate the input surface. NeurCross outperforms state-of-the-art methods in terms of singular point placement, robustness to noise and geometric variations, and approximation accuracy.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>198
</span></div><div id = 'author'>Authors:<span id = 'author'>Zhongxuan Liang,Wei Du,Xiao-Ming Fu
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Science and Technology of China,University of Science and Technology of China,University of Science and Technology of China
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_765&sess=sess118">Field Smoothness-Controlled Partition for Quadrangulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our approach proposes a novel partition method for reliable feature-aligned quadrangulation. The core insight is that singularity-distant smooth streamlines are more suitable as patch boundaries. The key implementation confines patch boundaries to high field smoothness regions.Validated on large-scale datasets, our method generates high-quality quad meshes while preserving reliability.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>199
</span></div><div id = 'author'>Authors:<span id = 'author'>Ryan Capouellez,Rodrigo Singh,Martin Heistermann,David Bommes,Denis Zorin
</span></div><div id = 'affiliation'><span id = 'affiliation'>New York University,New York University,University of Bern,University of Bern,New York University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1362&sess=sess118">Feature-Aligned Parametrization in Penner Coordinates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We extend Penner-coordinate-based methods for seamless parametrizations to surfaces with sharp features to which the parametrization needs to be aligned.  We describe a two-phase method to efficiently minimize feature constraint residual errors.  We demonstrate that the resulting algorithm works robustly on the Thingi10k dataset, completing the quad mesh generation pipeline.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>200
</span></div><div id = 'author'>Authors:<span id = 'author'>Etienne Corman,Keenan Crane
</span></div><div id = 'affiliation'><span id = 'affiliation'>CNRS,Carnegie Mellon University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_656&sess=sess118">Rectangular Surface Parameterization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a method for mapping curved surfaces to the plane without shear, enabling rectangular parameterizations. It introduces a novel approach for computing integrable, orthogonal frame fields. The method improves mesh quality, supports rich user control, and outperforms existing techniques in simulation, modeling, retopology, and digital fabrication tasks.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>201
</span></div><div id = 'author'>Authors:<span id = 'author'>Yuan-Yuan Cheng,Qing Fang,Ligang Liu,Xiao-Ming Fu
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Science and Technology of China,University of Science and Technology of China,University of Science and Technology of China,University of Science and Technology of China
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_757&sess=sess118">Divide-and-Conquer Embedding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The paper proposes a construction algorithm based on a divide-and-conquer strategy to map a disk-topology triangular mesh onto any convex polygon., which supports arbitrary numerical precision and exact arithmetic. Under exact arithmetic, it strictly guarantees a bijection for any mesh and convex polygon.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>202
</span></div><div id = 'author'>Authors:<span id = 'author'>Lei Lan,Tianjia Shao,Zixuan Lu,Yu Zhang,Chenfanfu Jiang,Yin Yang
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Utah,Zhejiang University,University of Utah,University of Utah,UCLA,University of Utah
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_716&sess=sess119">3DGS2: Near Second-order Converging 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a nearly second-order convergent training algorithm for 3D Gaussian Splatting that exploits independent kernel attributes and sparse coupling across images. By constructing and solving small Newton systems for parameter groups, it achieves about an-order faster training while maintaining or exceeding SGD-based reconstruction quality.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>203
</span></div><div id = 'author'>Authors:<span id = 'author'>Yunji Seo,Young Sun Choi,HyunSeung Son,Youngjung Uh
</span></div><div id = 'affiliation'><span id = 'affiliation'>Yonsei University,Yonsei University,Yonsei University,Yonsei University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1344&sess=sess119">FLoD: Integrating Flexible Level of Detail into 3D Gaussian Splatting for Customizable Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Flexible Level of Detail (FLoD) integrates the concept of LoD into 3DGS using a multi-level representation built with 3D Gaussian scale constraints and level-by-level training strategy. FLoD enables flexible rendering through single-level or selective rendering for optimal image quality under varying GPU VRAM constraints.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>204
</span></div><div id = 'author'>Authors:<span id = 'author'>Xijie Yang,Linning Xu,Lihan Jiang,Dahua Lin,Bo Dai
</span></div><div id = 'affiliation'><span id = 'affiliation'>Zhejiang University,The Chinese University of Hong Kong,University of Science and Technology of China,The Chinese University of Hong Kong,University of Hong Kong
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_204&sess=sess119">Virtualized 3D Gaussians: Flexible Cluster-based Level-of-Detail System for Real-Time Rendering of Composed Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>V3DG achieves real-time rendering of massive 3D Gaussians in large, composed scenes through a novel LOD approach.Inspired by Nanite, V3DG processes detailed 3D assets into clusters at various granularities offline, and selectively renders 3D Gaussians at runtime—flexibly balancing rendering speed and visual fidelity based on user-defined tolerances.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>205
</span></div><div id = 'author'>Authors:<span id = 'author'>Jorge Condor,Sébastien Speierer,Lukas Bode,Božič Aljaž,Simon Green,Piotr Didyk,Adrián Jarabo,Jorge Condor
</span></div><div id = 'affiliation'><span id = 'affiliation'>Universita della Svizzera Italiana,Meta Reality Labs,Meta Reality Labs,Meta Reality Labs,Meta Reality Labs,Universita della Svizzera Italiana,Meta Reality Labs,Università della Svizzera Italiana
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_109&sess=sess119">Don=E2=80=99t Splat your Gaussians: Volumetric Ray-Traced Primitives for Modeling and Rendering Scattering and Emissive Media</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We formalize the path-tracing of volumes composed of anisotropic kernel mixture models. Our work enables computing physically-based light transport on complex volumetric assets efficiently, on tiny memory budgets. We further introduce Epanechnikov kernels as an efficient alternative in kernel-based rendering, and showcase our method in different forward and inverse volume rendering applications including radiance fields.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>206
</span></div><div id = 'author'>Authors:<span id = 'author'>Keyang Ye,Tianjia Shao,Kun Zhou
</span></div><div id = 'affiliation'><span id = 'affiliation'>Zhejiang University,Zhejiang University,Zhejiang University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1192&sess=sess119">When Gaussian Meets Surfel: Ultra-fast High-fidelity Radiance Field Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Gaussian-enhanced Surfels (GESs), a bi-scale representation combining opaque surfels and Gaussians for high-fidelity radiance field rendering. GES is entirely sorting free, enabling high-fidelity view-consistent rendering with ultra fast speeds.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>207
</span></div><div id = 'author'>Authors:<span id = 'author'>Rong Liu,Dylan Sun,Meida Chen,Yue Wang,Andrew Feng
</span></div><div id = 'affiliation'><span id = 'affiliation'>USC Institute for Creative Technologies (ICT),University of Southern California,USC Institute for Creative Technologies (ICT),University of Southern California,USC Institute for Creative Technologies (ICT)
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_920&sess=sess119">Deformable Beta Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deformable Beta Splatting (DBS) is a novel approach for real-time radiance field rendering that leverages deformable Beta Kernels with adaptive frequency control for both geometry and color encoding. DBS captures complex geometries and lighting with state-of-the-art fidelity, while only using 45% fewer parameters and rendering 1.5x faster than 3DGS-MCMC.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>208
</span></div><div id = 'author'>Authors:<span id = 'author'>Yunxiang Zhang,Bingxuan Li,Alexandr Kuznetsov,Akshay Jindal,Stavros Diolatzis,Kenneth Chen,Anton Sochenov,Anton Kaplanyan,Qi Sun
</span></div><div id = 'affiliation'><span id = 'affiliation'>New York University,New York University,Advanced Micro Devices (AMD),Intel Corporation,Intel Corporation,New York University,Intel Corporation,Intel Corporation,New York University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_170&sess=sess119">Image-GS: Content-Adaptive Image Representation via 2D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Image-GS, a content-adaptive image representation based on colored 2D Gaussians. Image-GS achieves remarkable rate-distortion performance across diverse images and textures while supporting hardware-friendly fast random access and flexible quality control through a smooth level-of-detail hierarchy. We demonstrate its versatility with two applications: semantics-aware compression and image restoration.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>209
</span></div><div id = 'author'>Authors:<span id = 'author'>Mingi Lee,Dongsu Zhang,Clément Jambon,Young Min Kim
</span></div><div id = 'affiliation'><span id = 'affiliation'>Seoul National University,Seoul National University,Massachusetts Institute of Technology (MIT),Seoul National University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_781&sess=sess120">BrepDiff: Single-Stage B-rep Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present BrepDiff, a simple, single-stage diffusion model for generating Boundary Representations (B-reps). Our approach generates B-reps by denoising point-based face samples with a dedicated noise schedule. Unlike multi-stage methods, BrepDiff enables intuitive, editable geometry creation, including completion, merging, and interpolation, while achieving competitive performance on unconditional generation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>210
</span></div><div id = 'author'>Authors:<span id = 'author'>Yingyu Yang,Xiaohong Jia,Bolun Wang,Jieyin Yang,Shiqing Xin,Dong-Ming Yan
</span></div><div id = 'affiliation'><span id = 'affiliation'>State Key Laboratory of Mathematical Sciences, Academy of Mathematics and Systems Science, Chinese Academy of Sciences,State Key Laboratory of Mathematical Sciences, Academy of Mathematics and Systems Science, Chinese Academy of Sciences,Visual Computing Institute, RWTH Aachen University,State Key Laboratory of Mathematical Sciences, Academy of Mathematics and Systems Science, Chinese Academy of Sciences,Shandong University,MAIS, Institute of Automation, Chinese Academy of Sciences
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_974&sess=sess120">Boolean Operation for CAD Models Using a Hybrid Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel algorithm for efficient and accurate Boolean operations on B-Rep models by mapping them bijectively to controllable-error triangle meshes. Using conservative intersection detection on the mesh to locate all surface intersection curves and carefully handling degeneration and topology errors ensure that the results are watertight and correct.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>211
</span></div><div id = 'author'>Authors:<span id = 'author'>Bingchen Yang,Haiyong Jiang,Hao Pan,Guosheng Lin,Jun Xiao,Peter Wonka,Bingchen Yang
</span></div><div id = 'affiliation'><span id = 'affiliation'>School of Artificial Intelligence, University of Chinese Academy of Sciences,School of Artificial Intelligence, University of Chinese Academy of Sciences,Tsinghua University,Nanyang Technological University,School of Artificial Intelligence, University of Chinese Academy of Sciences,KAUST,School of Artificial Intelligence, University of Chinese Academy of Sciences; Nanyang Technological University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_106&sess=sess120">PS-CAD: Local Geometry Guidance via Prompting and Selection for CAD Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose an iterative prompt-and-select architecture to progressively reconstruct the CAD modeling sequence of a target point cloud. We propose the concept of local geometric guidance and come up with three ways to integrate this guidance into iterative reconstruction. Experiments demonstrate the superiority over the current state of the art.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>212
</span></div><div id = 'author'>Authors:<span id = 'author'>Pu Li,Wenhao Zhang,Jinglu Chen,Dongming Yan
</span></div><div id = 'affiliation'><span id = 'affiliation'>Institute of Automation, Chinese Academy Of Sciences,Institute of Automation, Chinese Academy of Sciences,Institute of Automation, Chinese Academy of Sciences,Institute of Automation, Chinese Academy of Sciences
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_546&sess=sess120">Stitch-A-Shape: Bottom-up Learning for B-Rep Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stitch-A-Shape introduces a novel framework for generating B-Rep models by directly addressing both topology and geometry. Using a sequential stitching approach, it assembles 3D shapes from vertices through curves to faces, effectively managing topological and geometric complexities. The framework demonstrates superior performance in shape generation, class-conditional generation, and autocompletion tasks.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>213
</span></div><div id = 'author'>Authors:<span id = 'author'>Jing-En Jiang,Hanxiao Wang,Mingyang Zhao,Dong-Ming Yan,Shuangmin Chen,Shiqing Xin,Changhe Tu,Wenping Wang
</span></div><div id = 'affiliation'><span id = 'affiliation'>School of Computer Science and Technology, Shandong University,Institute of Automation, Chinese Academy of Sciences, Beijing, China     The School of Artificial Intelligence, University of Chinese Academy of Sciences,Academy of Mathematics and Systems Science, Chinese Academy of Sciences, the University of Chinese Academy of Sciences,Institute of Automation, Chinese Academy of Sciences,School of Information and Technology, Qingdao University of Science and Technology,School of Computer Science and Technology, Shandong University,School of Computer Science and Technology,  Shandong University,Computer Science & Engineering,  Texas A&M University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_552&sess=sess120">DeFillet: Detection and Removal of Fillet Regions in Polygonal CAD Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DeFillet, the reverse of CAD filleting, is vital for CAE and redesign but challenging with polygon CAD models. Our algorithm uses Voronoi vertices as rolling-ball center candidates to efficiently identify fillets. Sharp features are then reconstructed via quadratic optimization, validated on diverse models.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>214
</span></div><div id = 'author'>Authors:<span id = 'author'>Yilin Liu,Duoteng Xu,Xingyao Yu,Xiang Xu,Daniel Cohen-Or,Hao Zhang,Hui Huang
</span></div><div id = 'affiliation'><span id = 'affiliation'>Shenzhen University,Shenzhen University,Shenzhen University,Simon Fraser University,Tel Aviv University,Simon Fraser University,Shenzhen University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_385&sess=sess120">HoLa: B-Rep Generation using a Holistic Latent Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel representation for learning and generating Computer-Aided Design (CAD) models in the form of boundary representations (BReps). Our representation unifies the continuous geometric properties of BRep primitives in different orders (e.g., surfaces and curves) and theirdiscrete topological relations in a holistic latent (HoLa) space.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>215
</span></div><div id = 'author'>Authors:<span id = 'author'>Chris Careaga,Yağız Aksoy
</span></div><div id = 'affiliation'><span id = 'affiliation'>Simon Fraser University,Simon Fraser University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_582&sess=sess117">Physically Controllable Relighting of Photographs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a photograph relighting method that enables explicit control over light sources akin to CG pipelines. We achieve this in a pipeline involving mid-level computer vision, physically-based rendering, and neural rendering. We introduce a self-supervised training methodology to train our neural renderer using real-world photograph collections.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>216
</span></div><div id = 'author'>Authors:<span id = 'author'>Nadav Magar,Amir Hertz,Eric Tabellion,Yael Pritch,Alex Rav-Acha,Ariel Shamir,Yedid Hoshen
</span></div><div id = 'affiliation'><span id = 'affiliation'>Tel Aviv University,Google,Google,Google,Google,Reichman University,Hebrew University of Jerusalem
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_770&sess=sess117">LightLab: Controlling Light Sources in Images with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LightLab is a diffusion-based method for parametric control over light sources in an image. Leveraging the linearity of light we create a dataset of controled illumniation changes from a small set of real image pairs and synthetic renders, which is used to fine-tune a model to enable physically plausible edits.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>217
</span></div><div id = 'author'>Authors:<span id = 'author'>Mutian Tong,Rundi Wu,Changxi Zheng
</span></div><div id = 'affiliation'><span id = 'affiliation'>Columbia University,Columbia University,Columbia University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1270&sess=sess117">Spatiotemporally Consistent Indoor Lighting Estimation with Diffusion Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a method for estimating spatiotemporally varying indoor lighting from videos using a continuous light field represented as an MLP. By leveraging 2D diffusion priors fine-tuned to predict lighting jointly at multiple locations, our approach achieves superior performance and zero-shot generalization to in-the-wild scenes.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>218
</span></div><div id = 'author'>Authors:<span id = 'author'>Henglei Lv,Bailin Deng,Jianzhu Guo,Xiaoqiang Liu,Pengfei Wan,Di Zhang,Lin Gao
</span></div><div id = 'affiliation'><span id = 'affiliation'>Institute of Computing Technology, Chinese Academy of Sciences,Cardiff University,Kuaishou Technology,Kuaishou Technology,Kuaishou Technology,Kuaishou Technology,Institute of Computing Technology, Chinese Academy of Sciences
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_269&sess=sess117">GSHeadRelight: Fast Relightability for 3D Gaussian Head Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>GSHeadRelight enables fast, high-quality relightability for 3D Gaussian head synthesis. A linear light model based on learnable radiance transfer is integrated into the native 3DGS rasterization process and supports colored illumination. Without requiring expensive light stage data, our method achieves 240+ FPS rendering speed and offers state-of-the-art relighting results.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>219
</span></div><div id = 'author'>Authors:<span id = 'author'>Philippe Weier,Jérémy Riviere,Ruslan Guseinov,Stephan Garbin,Philipp Slusallek,Bernd Bickel,Thabo Beeler,Delio Vicini
</span></div><div id = 'affiliation'><span id = 'affiliation'>Saarland University,Google,Google,Google,Saarland University,Google,Google,Google
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_521&sess=sess117">Practical Inverse Rendering of Textured and Translucent Appearance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses recovering textured materials using inverse rendering. Our Laplacian mipmapping improves the reconstruction of high-resolution textures. We also propose a novel gradient computation that enables efficiently reconstructing textured, path-traced subsurface scattering. The methods are applied to challenging scenes, including reconstructing realistic human face appearance from sparse captures.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>220
</span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Yang,Jing Tan,Mengchen Zhang,Tong Wu,Gordon Wetzstein,Ziwei Liu,Dahua Lin
</span></div><div id = 'affiliation'><span id = 'affiliation'>Shanghai Jiao Tong University,The Chinese University of Hong Kong,Zhejiang University,The Chinese University of Hong Kong,Stanford University,Nanyang Technological University,The Chinese University of Hong Kong
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_413&sess=sess117">LayerPano3D: Layered 3D Panorama for Hyper-Immersive Scene Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LayerPano3D is a novel framework that generates hyper-immersive 3D panoramic scenes from a single text prompt. By decomposing panoramas into multiple layers and optimizing them as 3D Gaussians, it enables full 360°×180° exploration with consistent visual quality, unlocking new possibilities for virtual reality and scene generation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>221
</span></div><div id = 'author'>Authors:<span id = 'author'>Khoa Do,David Coeurjolly,Pooran Memari,Nicolas Bonneel
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Michigan,CNRS - LIRIS,CNRS - LIX,CNRS - LIRIS
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_122&sess=sess155">Linear-Time Transport with Rectified Flows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a simple, parallelizable algorithm inspired by rectified flows to match probability distributions. With linear-time complexity, it approximates optimal transport by employing summed-area tables and direct particle advection. We illustrate our applications in stippling, mesh parameterization, and shape interpolation in 2D, 3D.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>222
</span></div><div id = 'author'>Authors:<span id = 'author'>Navid Ansari,HANS-PETER SEIDEL,Vahid Babaei
</span></div><div id = 'affiliation'><span id = 'affiliation'>Max Planck Institute for Informatics,Max Planck Institute for Informatics,Max Planck Institute for Informatics
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_867&sess=sess155">Accelerated Gamut Discovery via Massive Parallelization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a scalable framework using Bayesian Neural Networks and a novel 2mD acquisition function to efficiently discover gamut boundaries in performance space. Combining NSGA-II's diversity and Bayesian Optimization's efficiency, the method enables large-batch, parallel optimization, outperforming traditional approaches in real-world engineering and fabrication tasks.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>223
</span></div><div id = 'author'>Authors:<span id = 'author'>Behrooz Zarebavani,Danny M. Kaufman,David I. W. Levin,Maryam Mehri Dehnavi
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Toronto,Adobe Research,University of Toronto,University of Toronto
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_673&sess=sess155">Adaptive Algebraic Reuse of Reordering in Cholesky Factorizations with Dynamic Sparsity Patterns</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Parth delivers adaptive fill-reducing ordering to accelerate Cholesky solvers in simulations with dynamic sparsity patterns, such as contact modelling, achieving up to 255× ordering speedups. With seamless, three-line integration into popular solvers like MKL and Accelerate, Parth ensures reliable, high-performance computations for applications in computer graphics and scientific computing.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>224
</span></div><div id = 'author'>Authors:<span id = 'author'>Federico Sichetti,Enrico Puppo,Zizhou Huang,Marco Attene,Denis Zorin,Daniele Panozzo
</span></div><div id = 'affiliation'><span id = 'affiliation'>Università di Genova,Università di Genova,New York University,CNR IMATI,New York University,New York University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1052&sess=sess155">MiSo: A DSL for robust and efficient MINIMIZE and SOLVE problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many problems in graphics can be formulated as a non-linearly constrained global minimization (MINIMIZE), or solution of a system of non-linear constraints (SOLVE). We introduce MiSo, a domain-specific language and compiler for generating efficient code for low-dimensional MINIMIZE and SOLVE problems, using interval methods to guarantee conservative results.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>225
</span></div><div id = 'author'>Authors:<span id = 'author'>Chunlei Li,Peng Yu,Tiantian Liu,Siyuan Yu,Yuting Xiao,Shuai Li,Aimin Hao,Yang Gao,Qinping Zhao
</span></div><div id = 'affiliation'><span id = 'affiliation'>Beihang University,Beihang University,Taichi Graphics,Zenustech,Beihang University,Beihang University,Beihang University,Beihang University,Beihang University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_955&sess=sess155">MGPBD: A Multigrid Accelerated Global XPBD Solver</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In high-stiffness, high-resolution simulations, while primal space methods typically fail, the dual-space XPBD method produces unphysical softening artifacts due to convergence stall. We design an innovative Algebraic Multigrid method to enhance XPBD, utilizing lazy-update prolongators and near-kernel optimization. Our approach ensures stability, efficiency, and scalability for high-stiffness, high-resolution deformable models.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>226
</span></div><div id = 'author'>Authors:<span id = 'author'>Isabella Liu,Zhan Xu,Yifan Wang,Hao Tan,Zexiang Xu,Xiaolong Wang,Hao Su,Zifan Shi
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of California San Diego,Adobe Research,Adobe Research,Adobe Research,Hillbot Inc.,University of California San Diego,University of California San Diego,Adobe Research
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_181&sess=sess154">RigAnything: Template-Free Autoregressive Rigging for Diverse 3D Assets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RigAnything is a transformer-based model that autoregressively generates 3D rigging without templates. It sequentially predicts joints and skeleton topology while assigning skinning weights, working on objects in any pose. It’s 20× faster than existing methods, completing rigging in under 2 seconds with state-of-the-art quality across diverse object types.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>227
</span></div><div id = 'author'>Authors:<span id = 'author'>Jia-Peng Zhang,Cheng-Feng Pu,Meng-Hao Guo,Yan-Pei Cao,Shi-Min Hu
</span></div><div id = 'affiliation'><span id = 'affiliation'>CS Dept, Tsinghua University,CS Dept, Tsinghua University,CS Dept, Tsinghua University,VAST,CS Dept, Tsinghua University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1237&sess=sess154">One Model to Rig Them All: Diverse Skeleton Rigging with UniRig</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Manual 3D rigging is slow. UniRig introduces a unified learning framework for automatic skeletal rigging. Trained on our large, diverse Rig-XL dataset, it uses an autoregressive model and cross-attention to accurately rig various characters and objects, significantly outperforming prior methods and speeding up animation pipelines.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>228
</span></div><div id = 'author'>Authors:<span id = 'author'>Yufan Deng,Yuhao Zhang,Chen Geng,Shangzhe Wu,Jiajun Wu
</span></div><div id = 'affiliation'><span id = 'affiliation'>Stanford University,Stanford University,Stanford University,Stanford University,Stanford University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1176&sess=sess154">Anymate: A Dataset and Baselines for Learning 3D Object Rigging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the Anymate Dataset, a large-scale dataset of 230K 3D assets paired with expert-crafted rigging and skinning information---70 times larger than existing datasets. Using this dataset, we propose a learning-based auto-rigging framework with three sequential modules for joint, connectivity, and skinning weight prediction.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>229
</span></div><div id = 'author'>Authors:<span id = 'author'>Wenning Xu,Shiyu Fan,Paul Henderson,Edmond S. L. Ho
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Glasgow,University of Glasgow,University of Glasgow,University of Glasgow
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_718&sess=sess154">Multi-Person Interaction Generation from Two-Person Motion Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generate exciting multi-character interactions, such as team fights, with our training-free method! Multi-character interactions can be decomposed into multiple two-person interactions using a directed graph, which enables repurposing large pre-trained two-character motion synthesis models without any multi-character data. You can compose and vary multi-character interactions spatially and temporally!
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>230
</span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Chang,He Wang,George Koulieris,Hubert Shum
</span></div><div id = 'affiliation'><span id = 'affiliation'>Durham University,UCL Centre for Artificial Intelligence, Department of Computer Science, University College London (UCL),Durham University,Durham University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1272&sess=sess154">Large-Scale Multi-Character Interaction Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work introduces a conditional generative framework for large-scale multi-character interaction synthesis by facilitating natural interactive motions and transitions where characters are coordinated for new interactive partners, proposing a coordinatable multi-character interaction space for interaction synthesis and a transition planning network to plan transitions to achieve scalable, transferable multi-character animations.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>231
</span></div><div id = 'author'>Authors:<span id = 'author'>Runyi Yu,Yinhuai Wang,Qihan Zhao,Hok Wai Tsui,Jingbo Wang,Ping Tan,Qifeng Chen
</span></div><div id = 'affiliation'><span id = 'affiliation'>Hong Kong University of Science and Technology,Hong Kong University of Science and Technology,Hong Kong University of Science and Technology,Hong Kong University of Science and Technology,Shanghai Aritificial Intelligence Laboratory,Hong Kong University of Science and Technology,Hong Kong University of Science and Technology
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_407&sess=sess154">SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from Sparse and Noisy Demonstrations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses the challenge of learning robust interaction skills from limited demonstrations. By introducing novel data augmentation techniques for skill transitions and recovery patterns, combined with enhanced reinforcement imitation learning methods, we achieve superior performance in learning interaction skills, demonstrating improved generalization and recovery capabilities across diverse manipulation tasks.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>232
</span></div><div id = 'author'>Authors:<span id = 'author'>Minghao Yin,Yukang Cao,Songyou Peng,Kai Han
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Hong Kong,Nanyang Technological University, Singapore,Google DeepMind,University of Hong Kong
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1280&sess=sess125">Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and Spatially Consistent Content Creation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Splat4D generates high-fidelity 4D content from monocular videos by integrating multi-view rendering, inconsistency identification, a video diffusion model, and asymmetric U-Net refinement. Our framework maintains spatial-temporal consistency while preserving details and following user guidance, achieving state-of-the-art benchmark performance. Applications include text/image-conditioned generation, 4D human modeling, and text-guided content editing.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>233
</span></div><div id = 'author'>Authors:<span id = 'author'>Pinxuan Dai,Peiquan Zhang,Zheng Dong,Ke Xu,Yifan Peng,Dandan Ding,Yujun Shen,Yin Yang,Xinguo Liu,Rynson W.H. Lau,Weiwei Xu
</span></div><div id = 'affiliation'><span id = 'affiliation'>Zhejiang University,Zhejiang University,Zhejiang University,City University of Hong Kong,The University of Hong Kong,Hangzhou Normal University,Ant Group,The University of Utah,Zhejiang University,City University of Hong Kong,State Key Lab CAD&CG, Zhejiang University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_853&sess=sess125">4D Gaussian Videos with Motion Layering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present 4D Gaussian Video (4DGV) for high-quality, low-storage volumetric video reconstruction and real-time streaming. Our method effectively handles complex motion and enables effective motion compression, achieving superior performance in both reconstruction quality and storage efficiency.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>234
</span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoyang Lv,Maurizio Monge,Ka Chen,Yufeng Zhu,Michael Goesele,Jakob Engel,Zhao Dong,Richard Newcombe
</span></div><div id = 'affiliation'><span id = 'affiliation'>Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1286&sess=sess125">Photoreal Scene Reconstruction from an Egocentric Device</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates photorealistic scene reconstruction using videos captured from an egocentric device in high dynamic range. It presents a novel system utilizing visual-inertial bundle adjustment and a physical image formation model that handles camera motion artifacts. The experiments using Project Aria and Quest3 show substantial improvements in visual quality.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>235
</span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Meuleman,Ishaan Shah,Alexandre Lanvin,Bernhard Kerbl,George Drettakis
</span></div><div id = 'affiliation'><span id = 'affiliation'>INRIA, Université Côte d'Azur,INRIA, Université Côte d'Azur,INRIA, Université Côte d'Azur,TU Wien,INRIA, Université Côte d'Azur
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1024&sess=sess125">On-the-fly Reconstruction for Large-Scale Novel View Synthesis from Unposed Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a fast, on-the-fly 3D Gaussian Splatting method that jointly estimates poses and reconstructs scenes. Through fast pose initialization, direct primitive sampling, and scalable clustering and merging, it efficiently handles diverse ordered image sequences of arbitrary length.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>236
</span></div><div id = 'author'>Authors:<span id = 'author'>Songyin Wu,Zhaoyang Lv,Yufeng Zhu,Duncan Frost,Zhengqin Li,Ling-Qi Yan,Carl Ren,Richard Newcombe,Zhao Dong
</span></div><div id = 'affiliation'><span id = 'affiliation'>Meta Reality Labs Research,Meta Reality Labs Research,Meta Reality Labs Research,Meta Reality Labs Research,Meta Reality Labs Research,University of California Santa Barbara,Meta Reality Labs Research,Meta Reality Labs Research,Meta Reality Labs Research
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_533&sess=sess125">Monocular Online Reconstruction with Enhanced Detail Preservation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a high-quality online reconstruction pipeline for monocular input streams, reconstructing environments with detail across multiple levels while maintaining high speed.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>237
</span></div><div id = 'author'>Authors:<span id = 'author'>Letian Huang,Dongwei Ye,Jialin Dan,Chengzhi Tao,Huiwen Liu,Kun Zhou,Bo Ren,Yuanqi Li,Yanwen Guo,Jie Guo
</span></div><div id = 'affiliation'><span id = 'affiliation'>State Key Lab for Novel Software Technology, Nanjing University,State Key Lab for Novel Software Technology, Nanjing University,State Key Lab for Novel Software Technology, Nanjing University,State Key Lab for Novel Software Technology, Nanjing University,TMCC, College of Computer Science, Nankai University,State Key Lab of CAD&CG, Zhejiang University,TMCC, College of Computer Science, Nankai University,State Key Lab for Novel Software Technology, Nanjing University,State Key Lab for Novel Software Technology, Nanjing University,State Key Lab for Novel Software Technology, Nanjing University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_825&sess=sess125">TransparentGS: Fast Inverse Rendering of Transparent Objects with Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose TransparentGS, a fast inverse rendering pipeline for transparent objects based on 3D-GS. The main contributions are three-fold: efficient transparent Gaussian primitives for specular refraction, GaussProbe to encode ambient light and nearby contents, and the IterQuery algorithm to reduce parallax errors in our probe-based framework.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>238
</span></div><div id = 'author'>Authors:<span id = 'author'>Elad Richardson,Yuval Alaluf,Ali Mahdavi-Amiri,Daniel Cohen-Or
</span></div><div id = 'affiliation'><span id = 'affiliation'>Tel Aviv University,Tel Aviv University,Simon Fraser University,Tel Aviv University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_270&sess=sess142">pOps: Photo-Inspired Diffusion Operators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>pOps is a framework for learning semantic manipulations in CLIP’s image embedding space. Built on a Diffusion Prior model, it enables concept manipulation by training operators directly on image embeddings. This approach enhances semantic control and integrates easily with diffusion models for image generation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>239
</span></div><div id = 'author'>Authors:<span id = 'author'>Lvmin Zhang,Chuan Yan,Yuwei Guo,Jinbo Xing,Maneesh Agrawala
</span></div><div id = 'affiliation'><span id = 'affiliation'>Stanford University,Stanford University,CUHK,CUHK,Stanford University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_476&sess=sess142">Generating Past and Future in Digital Painting Processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A framework to generate past and future processes for drawing process videos.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>240
</span></div><div id = 'author'>Authors:<span id = 'author'>Eric Chen,Žiga Kovačič,Madhav Aggarwal,Abe Davis
</span></div><div id = 'affiliation'><span id = 'affiliation'>Cornell University,Cornell University,Cornell University,Cornell University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_140&sess=sess142">Pocket Time-Lapse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pocket Time-Lapse is a system to record, explore and visualize long-term changes in the environment, based on data that a user can capture with the phone they carry. Our contributions include a process to conveniently capture a scene, and novel techniques for registering and visualizing panoramic time-lapse data.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>241
</span></div><div id = 'author'>Authors:<span id = 'author'>Etai Sella,Yanir Kleiman,Hadar Averbuch-Elor
</span></div><div id = 'affiliation'><span id = 'affiliation'>Tel Aviv University,Meta,Cornell Tech
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_265&sess=sess142">InstanceGen: Image Generation with Instance-level Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose InstanceGen - a new technique for improving Text-to-Image models ability to generate images for prompts describing multiple objects, attributes and spatial relationships. InstanceGen requires no training or additional user inputs and achieves state-of-the art results in terms of both accuracy and visual quality on these highly challenging prompts.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>242
</span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Zhang,Minyan Luo,Weiming Dong,Xiao Yang,Haibin Huang,Chongyang Ma,Oliver Deussen,Tong-Yee Lee,Changsheng Xu
</span></div><div id = 'affiliation'><span id = 'affiliation'>MAIS, Institute of Automation, Chinese Academy of Sciences,MAIS, Institute of Automation, Chinese Academy of Sciences,MAIS, Institute of Automation, Chinese Academy of Sciences,ByteDance Inc.,ByteDance Inc.,ByteDance Inc.,University of Konstanz,National Cheng-Kung University,MAIS, Institute of Automation, Chinese Academy of Sciences
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_599&sess=sess142">IP-Prompter: Training-Free Theme-Specific Image Generation via Dynamic Visual Prompting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents T-Prompter, a method for visually prompting generative models to enable continuous image generation for specific themes, characters, and scenes. It introduces Dynamic Visual Prompting to enhance generation accuracy and quality, outperforming existing methods in maintaining character identity, style consistency, and text alignment.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>243
</span></div><div id = 'author'>Authors:<span id = 'author'>Yuanpeng Tu,Xi Chen,Ser-Nam Lim,Hengshuang Zhao
</span></div><div id = 'affiliation'><span id = 'affiliation'>The University of Hong Kong,The University of Hong Kong,UCF,The University of Hong Kong
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_687&sess=sess142">DreamMask: Boosting Open-vocabulary Panoptic Segmentation with Synthetic Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To address a lack of generalization to novel classes, we propose DreamMask, which systematically explores data generation in the open-vocabulary setting, and how to train the model with both real and synthetic data. It significantly simplifies the collection of large-scale training data, serving as a plug-and-play enhancement for existing methods.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>244
</span></div><div id = 'author'>Authors:<span id = 'author'>Sara Dorfman,Dana Cohen-Bar,Rinon Gal,Daniel Cohen-Or
</span></div><div id = 'affiliation'><span id = 'affiliation'>Tel Aviv University,Tel Aviv University,NVIDIA Research,Tel Aviv University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_297&sess=sess142">IP-Composer: Semantic Composition of Visual Concepts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>IP-Composer is a novel, training-free method for compositional image generation from multiple reference images. Extending IP-Adapter, it uses natural language to identify concept-specific subspaces in CLIP, projects input images into these subspaces to extract targeted concepts, and fuses them into composite embeddings—enabling fine-grained, controllable generation across diverse visual concepts.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>245
</span></div><div id = 'author'>Authors:<span id = 'author'>Tianyang Xue,Longdu Liu,Lin Lu,Paul Henderson,Pengbin Tang,Haochen Li,Jikai Liu,Haisen Zhao,Hao Peng,Bernd Bickel
</span></div><div id = 'affiliation'><span id = 'affiliation'>Shandong University,Shandong University,Shandong University,University of Glasgow,ETH Zürich,Shandong University,Shandong University,Shandong University,CrownCAD,ETH Zürich
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_679&sess=sess124">MIND: Microstructure INverse Design with Generative Hybrid Neural Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MIND, a novel generative framework for inverse-designing diverse, tileable 3D microstructures. Leveraging latent diffusion and our hybrid neural representation, MIND precisely achieves targeted physical properties, ensures geometric validity, and enables seamless boundary compatibility—opening new avenues for advanced metamaterial design and manufacturing applications.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>246
</span></div><div id = 'author'>Authors:<span id = 'author'>Tao Liu,Tianyu Zhang,Yongxue Chen,Weiming Wang,Yu Jiang,Yuming Huang,Charlie C.L. Wang
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Manchester,University of Manchester,University of Manchester,University of Manchester,University of Manchester,University of Manchester,University of Manchester
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1122&sess=sess124">Neural Co-Optimization of Structural Topology, Manufacturable Layers, and Path Orientations for Fiber-Reinforced Composites</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a computational framework that co-optimizes structural topology, curved layers, and fiber orientations for manufacturable, high-strength composites. Using implicit neural fields, our method integrates design and fabrication objectives into a unified optimization process, achieving up to 33.1% improvement in failure load for multi-axis 3D printed fiber-reinforced thermoplastics.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>247
</span></div><div id = 'author'>Authors:<span id = 'author'>Maxine Perroni-Scharf,Zachary Ferguson,Thomas Butruille,Carlos Portela,Mina Konaković Luković
</span></div><div id = 'affiliation'><span id = 'affiliation'>Massachusetts Institute of Technology (MIT),Massachusetts Institute of Technology (MIT),Massachusetts Institute of Technology (MIT),Massachusetts Institute of Technology (MIT),Massachusetts Institute of Technology (MIT)
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1431&sess=sess124">Data-Efficient Discovery of Hyperelastic TPMS Metamaterials with Extreme Energy Dissipation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a method for discovering novel microscale TPMS structures with high-energy dissipation. By combining a parametric design space, empirical testing, and uncertainty-aware deep ensembles with Bayesian optimization, we efficiently explore and discover structures with extreme energy dissipation capabilities.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>248
</span></div><div id = 'author'>Authors:<span id = 'author'>Pengbin Tang,Bernhard Thomaszewski,Stelian Coros,Bernd Bickel
</span></div><div id = 'affiliation'><span id = 'affiliation'>ETH Zürich,ETH Zürich,ETH Zürich,ETH Zürich
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_640&sess=sess124">Inverse Design of Discrete Interlocking Materials with Desired Mechanical Behavior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a computational approach for designing Discrete Interlocking Materials (DIM) with desired mechanical properties. We demonstrate the effectiveness of our method by designing discrete interlocking materials with diverse limit profiles for in- and out-of-plane deformation and validate our method on fabricated physical prototypes.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>249
</span></div><div id = 'author'>Authors:<span id = 'author'>Di Zhang,Ligang Liu
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Science and Technology of China,University of Science and Technology of China
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_750&sess=sess124">Asymptotic analysis and design of linear elastic shell lattice metamaterials</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel asymptotic directional stiffness (ADS) metric to analyze the contribution of middle surface geometry on the stiffness of shell lattice metamaterials, focusing on Triply Periodic Minimal Surfaces (TPMS). It provides a theoretical framework and optimization techniques, advancing the understanding of TPMS shell lattices.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>250
</span></div><div id = 'author'>Authors:<span id = 'author'>Aviv Segall,Jing Ren,Martin Schwarz,Olga Sorkine-Hornung
</span></div><div id = 'affiliation'><span id = 'affiliation'>ETH Zurich,ETH Zurich,University of Basel,ETH Zurich
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_457&sess=sess124">Computational Modeling of Gothic Microarchitecture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gothic microarchitecture—a prevalent feature of late medieval art—comprises sculptural works that replicate monumental Gothic forms, though its original construction techniques remain historically undocumented. Leveraging insights from 15th-century Basel goldsmith drawings, we present an interactive framework for reconstructing these intricate designs from 2D projections into accurate 3D forms.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>251
</span></div><div id = 'author'>Authors:<span id = 'author'>András Simon,Danwu Chen,Philipp Urban,Vincent Duveiller,Henning Lübbe
</span></div><div id = 'affiliation'><span id = 'affiliation'>Fraunhofer IGD,Fraunhofer IGD,Fraunhofer IGD,VITA Zahnfabrik H. Rauter GmbH & Co. KG,VITA Zahnfabrik H. Rauter GmbH & Co. KG
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_845&sess=sess124">Color Matching and Biomimicry for Multi-material Dental 3D Printing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a practical method for dental layer biomimicry and multi-spot shade matching using multi-material 3D printing. It integrates seamlessly into workflows combining dental CAD tools and industrial multi-material slicers.We validated it by printing multiple dentures and teeth with varying inner structures and translucencies to match VITA classical shades.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>252
</span></div><div id = 'author'>Authors:<span id = 'author'>Michele Rocca,Sune Darkner,Kenny Erleben,Sheldon Andrews,Michele Rocca
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Copenhagen,University of Copenhagen,University of Copenhagen,École de technologie supérieure,University of Copenhagen
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_102&sess=sess156">Policy-Space Diffusion for Physics-Based Character Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a new perspective on physics-based character animation. Assuming policies for similar motions should have similar weights, we introduce regularization during RL training to preserve weight similarity. By modeling the weights’ manifold with a diffusion model, we generate a continuum of policies adapting to novel character morphologies and tasks.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>253
</span></div><div id = 'author'>Authors:<span id = 'author'>Jungnam Park,Euikyun Jung,Jehee Lee,Jungdam Won
</span></div><div id = 'affiliation'><span id = 'affiliation'>Seoul National University,Seoul National University,Seoul National University,Seoul National University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_277&sess=sess156">MAGNET: Muscle Activation Generation Networks for Diverse Human Movement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MAGNET (Muscle Activation Generation Networks), a scalable framework for reconstructing full-body muscle activations across diverse human movements, which also includes distilled models for solving downstream tasks or generating real-time muscle activations—even on edge devices. The efficacy is demonstrated through examples of daily life and challenging behaviors.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>254
</span></div><div id = 'author'>Authors:<span id = 'author'>Michael Xu,Yi Shi,KangKang Yin,Xue Bin Peng
</span></div><div id = 'affiliation'><span id = 'affiliation'>Simon Fraser University,Simon Fraser University,Simon Fraser University,Simon Fraser University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_276&sess=sess156">PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>PARC is a framework that enhances terrain traversal with machine learning and physics-based simulation. By iteratively training a kinematic motion generator and simulated motion tracker, PARC produces a character controller capable of traversing complex environments using highly agile motor skills, overcoming the challenges of limited motion capture data.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>255
</span></div><div id = 'author'>Authors:<span id = 'author'>Minsu Kim,Eunho Jung,Yoonsang Lee
</span></div><div id = 'affiliation'><span id = 'affiliation'>Hanyang University,Hanyang University,Hanyang University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_334&sess=sess156">PhysicsFC: Learning User-Controlled Skills for a Physics-Based Football Player Controller</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>PhysicsFC introduces a breakthrough in interactive football simulation—enabling real-time control of physically simulated players that perform complex skills with smooth transitions. It combines skill-specific learning, physics-informed rewards, latent-guided training, and transition-aware state initialization, achieving agile, lifelike football behaviors in scenarios ranging from 1v1 play to full 11v11 matches.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>256
</span></div><div id = 'author'>Authors:<span id = 'author'>Minseok Kim,Wonjeong Seo,Sung-Hee Lee,Jungdam Won
</span></div><div id = 'affiliation'><span id = 'affiliation'>Seoul National University,Seoul National University,Korea Advanced Institute of Science and Technology (KAIST),Seoul National University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_278&sess=sess156">ViSA: Physics-based Virtual Stunt Actors for Ballistic Stunts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce ViSA (Virtual Stunt Actors), an interactive animation system using deep reinforcement learning to generate realistic ballistic stunt actions. It efficiently produces dynamic scenes commonly seen in films and TV dramas, such as traffic accidents and stairway falls. A novel action space design enables scene generation within minutes.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>257
</span></div><div id = 'author'>Authors:<span id = 'author'>Jinseok Bae,Younghwan Lee,Donggeun Lim,Young Min Kim
</span></div><div id = 'affiliation'><span id = 'affiliation'>Seoul National University,Seoul National University,Seoul National University,Seoul National University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_378&sess=sess156">PLT: Part-Wise Latent Tokens as Adaptable Motion Priors for Physically Simulated Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a physically-based character animation framework that exploits part-wise latent tokens. The novel structured decomposition enables dynamic exploration to stably adapt to diverse unseen scenarios. Additional refinement networks improve overall motion quality. We show superior performance on multi-body tracking, motion adaptation, and locomotion with damaged body parts.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>258
</span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Huang,Takara Truong,Yunbo Zhang,Fangzhou Yu,Jean Pierre Sleiman,Jessica Hodgins,Koushil Sreenath,Farbod Farshidian
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of California Berkeley,Stanford University,Robotics and AI Institute,Robotics and AI Institute,Robotics and AI Institute,Robotics and AI Institute,Robotics and AI Institute,Robotics and AI Institute
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1047&sess=sess156">Diffuse-CLoC: Guided Diffusion for Physics-based Character Look-ahead Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Meet Diffuse-CLoC—a powerful unification of intuitive steering in kinematic motion generation and physics-based character control. By guiding diffusion over joint state-action spaces, it enables agile, steerable, and physically realistic motions across diverse downstream tasks—from obstacle avoidance to task-space control and motion in-betweening—all from a single model, with no fine-tuning required.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>259
</span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohe Ma,Valentin Deschaintre,Milos Hasan,Fujun Luan,Kun Zhou,Hongzhi Wu,Yiwei Hu
</span></div><div id = 'affiliation'><span id = 'affiliation'>State Key Lab of CAD&CG, Zhejiang University,Adobe Research,Adobe Research,Adobe Research,State Key Lab of CAD&CG, Zhejiang University,State Key Lab of CAD&CG, Zhejiang University,Adobe Research
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_984&sess=sess129">MaterialPicker: Multi-Modal DiT-Based Material Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>MaterialPicker is a multi-modal material generation model that creates high-quality material maps from images and/or text by fine-tuning a video diffusion model. It robustly extracts materials from real-world photos, even with distortion or occlusion, enhancing fidelity, diversity, and efficiency in material synthesis.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>260
</span></div><div id = 'author'>Authors:<span id = 'author'>Michael Birsak,John Femiani,Biao Zhang,Peter Wonka
</span></div><div id = 'affiliation'><span id = 'affiliation'>King Abdullah University of Science and Technology (KAUST),Miami University,King Abdullah University of Science and Technology (KAUST),King Abdullah University of Science and Technology (KAUST)
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1157&sess=sess129">MatCLIP: Light- and Shape-Insensitive Assignment of PBR Material Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>MatCLIP assigns realistic PBR materials to 3D models using shape- and lighting-invariant descriptors derived from images, including LDM outputs and photos. It outperforms prior methods by over 15%, enabling consistent material predictions across varied geometry and lighting, with applications to large-scale 3D datasets like ShapeNet and Objaverse.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>261
</span></div><div id = 'author'>Authors:<span id = 'author'>Mengqi Xia,Zhaoyang Zhang,Sumit Chaturvedi,Yutong Yi,Rundong Wu,Holly Rushmeier,Julie Dorsey
</span></div><div id = 'affiliation'><span id = 'affiliation'>Yale University,Yale University,Yale University,Yale University,ByteDance Inc.,Yale University,Yale University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_672&sess=sess129">Predicting Fabric Appearance Through Thread Scattering and Inversion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel pipeline to digitize physical threads and predict fabric appearance before fabricating cloth samples, addressing a real need in the fashion industry. It enables designers to make more informed material choices, thereby promoting sustainable production, reducing costs, and fostering innovation in fabric design.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>262
</span></div><div id = 'author'>Authors:<span id = 'author'>Liwen Wu,Fujun Luan,Miloš Hašan,Ravi Ramamoorthi
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of California San Diego,Adobe Research,Adobe Research,University of California San Diego
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_358&sess=sess129">Position-Normal Manifold for Efficient Glint Rendering on High-Resolution Normal Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate modeling of normal distribution functions (NDF) over a high-resolution normal map enables intriguing glinty appearance but is inefficient. We present a manifold-based glint formulation, transferring the glint NDF computation to mesh intersections. This framework accelerates glint rendering, as well as providing a closed-form shadow-masking derivation for normal-mapped diffuse surfaces.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>263
</span></div><div id = 'author'>Authors:<span id = 'author'>Laurent Belcour,Alban Fichet,Pascal Barla
</span></div><div id = 'affiliation'><span id = 'affiliation'>Intel Labs,Intel Labs,Inria - LaBRI
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_969&sess=sess129">A Fluorescent Material Model for Non-Spectral Editing & Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a material model for diffuse fluorescence that is compatible with RGB and spectral rendering. This models builds on an analytical integrable Gaussian-based model of the spectral reradiation that is efficient enough to permits real-time rendering and editing of such appearance.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>264
</span></div><div id = 'author'>Authors:<span id = 'author'>Zhengze Liu,Yuchi Huo,Yifan Peng,Rui Wang
</span></div><div id = 'affiliation'><span id = 'affiliation'>State Key Lab of CAD & CG, Zhejiang University,State Key Lab of CAD & CG, Zhejiang University,University of Hong Kong,State Key Lab of CAD & CG, Zhejiang University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_198&sess=sess129">A Fully-statistical Wave Scattering Model for Heterogeneous Surfaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a statistical wave-scattering model for surfaces with nanoscale mixtures in geometry and material. It predicts average appearance (BRDF) and draws realistic speckles directly from surface statistics, without explicit definitions. The proposed model demonstrates various applications including corrosion (natural), particle deposition (man-made) and height-correlated mixture (artistic).
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>265
</span></div><div id = 'author'>Authors:<span id = 'author'>Louis Sugy
</span></div><div id = 'affiliation'><span id = 'affiliation'>NVIDIA
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_846&sess=sess157">A Fast Parallel Median Filtering Algorithm Using Hierarchical Tiling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel median filtering algorithm, using hierarchical tiling to reduce redundant computations and achieve better complexity than prior sorting-based methods. The paper discusses two implementations, for both small and larger kernel sizes, that outperform the state of the art by up to 5x on modern GPUs.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>266
</span></div><div id = 'author'>Authors:<span id = 'author'>Ben Weiss
</span></div><div id = 'affiliation'><span id = 'affiliation'>Google Research
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1541&sess=sess157">Fast Isotropic Median Filtering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The median filter is a staple of computational image processing. Existing efficient methods share a common flaw, which is that they use a square kernel, producing visual artifacts. Our method overcomes this limitation, enabling fast and high-quality circular-kernel median filtering, across multiple platforms and image types.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>267
</span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Zhang,Yang Zhang,Lukas Mehl,Markus Gross,Christopher Schroers
</span></div><div id = 'affiliation'><span id = 'affiliation'>ETH Zürich,DisneyResearch|Studios,DisneyResearch|Studios,ETH Zürich,DisneyResearch|Studios
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_595&sess=sess157">High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SplatDiff, a pixel-splatting-guided diffusion model for single-image novel view synthesis (NVS). Leveraging pixel splatting and video diffusion, SplatDiff generates high-quality novel views with consistent geometry and high-fidelity details. SplatDiff achieves state-of-the-art results in single-view NVS and demonstrates remarkable zero-shot performance on sparse-view NVS and stereo video conversion.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>268
</span></div><div id = 'author'>Authors:<span id = 'author'>Youngsik Yun,Jeongmin Bae,Hyunseung Son,Seoha Kim,Hahyun Lee,Gun Bang,Youngjung Uh
</span></div><div id = 'affiliation'><span id = 'affiliation'>Yonsei University,Yonsei University,Yonsei University,Electronics and Telecommunications Research Institute,Electronics and Telecommunications Research Institute,Electronics and Telecommunications Research Institute,Yonsei University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_655&sess=sess157">Compensating Spatiotemporally Inconsistent Observations for Online Dynamic 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We reveal that existing online reconstruction of dynamic scenes with 3D Gaussian Splatting produces temporally inconsistent results, led by inevitable noise in real-world recordings. To address this, we decompose the rendered images into the ideal signal and the errors during optimization, achieving temporally consistent results across various baselines.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>269
</span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Kong,Le Li,Yong Zhang,Feng Gao,Shaoshu Yang,Tao Wang,Kaihao Zhang,Zhuoliang Kang,Xiaoming Wei,Guanying Chen,Wenhan Luo
</span></div><div id = 'affiliation'><span id = 'affiliation'>Sun Yat-sen University,Tianjin University,Meituan,Meituan,School of Artificial Intelligence, University of Chinese Academy of Sciences,Nanjing University,Harbin Institute of Technology,Meituan,Meituan,Sun Yat-sen University,The Hong Kong University of Science and Technology
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_787&sess=sess157">DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose DAM-VSR, an appearance and motion disentanglement framework for video super-resolution. Appearance enhancement is achieved through reference image super-resolution, while motion control is achieved through video ControlNet. Additionally, we propose a motion-aligned bidirectional sampling strategy to support the generation of long videos.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>270
</span></div><div id = 'author'>Authors:<span id = 'author'>Janghyeok Han,Gyujin Sim,Geonung Kim,Hyun-Seung Lee,Kyuha Choi,Youngseok Han,Sunghyun Cho
</span></div><div id = 'affiliation'><span id = 'affiliation'>POSTECH,POSTECH,POSTECH,Samsung Electronics,Samsung Electronics,Samsung Electronics,POSTECH
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_932&sess=sess157">DC-VSR: Spatially and Temporally Consistent Video Super-Resolution with Video Diffusion Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose DC-VSR, a novel video super-resolution approach based on a video diffusion prior. DC-VSR leverages Spatial and Temporal Attention Propagation (SAP and TAP) to ensure spatio-temporally consistent results and Detail-Suppression Self-Attention Guidance (DSSAG) to enhance high-frequency details. DC-VSR restores videos with realistic textures while maintaining spatial and temporal coherence.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>271
</span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed H. Mahmoud,Serban D. Porumbescu,John D. Owens
</span></div><div id = 'affiliation'><span id = 'affiliation'>Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT),University of California, Davis,University of California, Davis
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_496&sess=sess134">Dynamic Mesh Processing on the GPU</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Introducing the first GPU-based system for dynamic triangle mesh processing, delivering order-of-magnitude speedups over CPU solutions across diverse applications. Our system uses patch-based data structure, speculative conflict handling, and a novel programming model, enabling robust, high-performance, and fully dynamic mesh operations directly on the GPU.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>272
</span></div><div id = 'author'>Authors:<span id = 'author'>Abhishek Madan,Nicholas Sharp,Francis Williams,Ken Museth,David I.W. Levin
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Toronto,NVIDIA,NVIDIA,NVIDIA,University of Toronto
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1013&sess=sess134">Stochastic Barnes-Hut Approximation for Fast Summation on the GPU</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel stochastic version of the Barnes-Hut approximation. Regarding the level-of-detail (LOD) family of approximations as control variates, we construct an unbiased estimator of the kernel sum being approximated. Through several examples in graphics, we demonstrate that our method outperforms a GPU-optimized implementation of the deterministic Barnes-Hut approximation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>273
</span></div><div id = 'author'>Authors:<span id = 'author'>Kai Li,Xiaohong Jia,Falai Chen,Kai Li
</span></div><div id = 'affiliation'><span id = 'affiliation'>State Key Laboratory of Mathematical Sciences, Academy of Mathematics and Systems Science, Chinese Academy of Sciences,State Key Laboratory of Mathematical Sciences, Academy of Mathematics and Systems Science, Chinese Academy of Sciences,University of Science and Technology of China,Key Laboratory of Mathematics Mechanization, Academy of Mathematics and Systems Science, Chinese Academy of Sciences; University of Chinese Academy of Sciences
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_107&sess=sess134">Fast Determination and Computation of Self-intersections for NURBS Surfaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting surface self-intersections is crucial for CAD modeling to prevent issues in simulation and manufacturing. This paper presents an algebraic signature-based algorithm for fast determining self-intersections of NURBS surfaces. This signature is then recursively cross-used to compute the self-intersection locus, guaranteeing robustness in critical cases including tangency and small loops.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>274
</span></div><div id = 'author'>Authors:<span id = 'author'>Hugo Schott,Théo Thonat,Thibaud Lambert,Eric Guérin,Eric Galin,Axel Paris
</span></div><div id = 'affiliation'><span id = 'affiliation'>INSA, Lyon,Adobe,Adobe,INSA, Lyon,Université Claude Bernard Lyon 1,Adobe
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_420&sess=sess134">Sphere Carving: Bounding Volumes for Signed Distance Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Sphere Carving, a method for automatically computing bounding volumes for conservative implicit surface. SDF queries define a set of spheres, from which we extract intersection points, used to compute a bounding volume with guarantees. Sphere Carving is conceptually simple and independent of the function representation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>275
</span></div><div id = 'author'>Authors:<span id = 'author'>Cyprien Plateau--Holleville,Benjamin Stamm,Vincent Nivoliers,Maxime Maria,Stéphane Mérillou
</span></div><div id = 'affiliation'><span id = 'affiliation'>Université de Limoges,Universität Stuttgart,Université Claude Bernard Lyon 1,Université de Limoges,Université de Limoges
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_610&sess=sess134">In Search of Empty Spheres: 3D Apollonius Diagrams on GPU</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel construction algorithm of 3D Apollonius diagrams designed for GPUs. Our method features a fast execution while allowing a comprehensive computation. This is made possible thanks to a light data structure, a cell update procedure and a spacial exploration strategy all designed to support the diagram properties.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>276
</span></div><div id = 'author'>Authors:<span id = 'author'>Huadong Zhang,Lizhou Cao,Chao Peng
</span></div><div id = 'affiliation'><span id = 'affiliation'>Rochester Institute of Technology,Rochester Institute of Technology,Rochester Institute of Technology
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_832&sess=sess134">UltraMeshRenderer: Efficient Structure and Management of GPU Out-of-core Memory for Real-time Rendering of Gigantic 3D Meshes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents UltraMeshRenderer, a GPU out-of-core method for real-time rendering of 3D scenes with billions of vertices and triangles. It features a balanced hierarchical mesh, coherence-based LOD selection, and parallel in-place GPU memory management, achieving efficient data transfer and memory use with significant improvements over existing out-of-core techniques.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>277
</span></div><div id = 'author'>Authors:<span id = 'author'>Zhengming Yu,Tianye Li,Jingxiang Sun,Omer Shapira,Seonwook Park,Michael Stengel,Matthew Chan,Xin Li,Wenping Wang,Koki Nagano,Shalini De Mello
</span></div><div id = 'affiliation'><span id = 'affiliation'>Texas A&M University,NVIDIA,Tsinghua University,NVIDIA,NVIDIA,NVIDIA,NVIDIA,Texas A&M University,Texas A&M University,NVIDIA,NVIDIA
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1111&sess=sess122">GAIA: Generative Animatable Interactive Avatars with Expression-conditioned Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present GAIA (Generative Animatable Interactive Avatars) for high-fidelity 3D head avatar generation. GAIA learns dynamic details with expression-conditioned Gaussians, while being animatable consistently with an underlying morphable model. With a novel two-branch architecture, GAIA disentangles identity and expression. GAIA achieves state-of-the-art realism and supports interactive rendering and animation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>278
</span></div><div id = 'author'>Authors:<span id = 'author'>Gengyan S. Li,Paulo Gotardo,Timo Bolkart,Stephan Garbin,Kripasindhu Sarkar,Abhimitra Meka,Alexandros Lattas,Thabo Beeler
</span></div><div id = 'affiliation'><span id = 'affiliation'>ETH Zürich,Google,Google,Google,Google,Google,Google,Google
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_858&sess=sess122">TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>By combining a continuous, UVD tangent space 3DGS model with a UNet deformation network while maintaining adaptive densification, we present a novel high-detail 3D head avatar model that preserves even finer detail like pores and eyelashes at 4K resolution.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>279
</span></div><div id = 'author'>Authors:<span id = 'author'>Luchao Qi,Jiaye Wu,Bang Gong,Annie Wang,David Jacobs,Roni Sengupta
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of North Carolina at Chapel Hill (UNC),University of Maryland College Park,University of North Carolina Chapel Hill,University of North Carolina at Chapel Hill (UNC),University of Maryland College Park,University of North Carolina at Chapel Hill (UNC)
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_611&sess=sess122">MyTimeMachine: Personalized Facial Age Transformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We personalize a pre-trained global aging prior using 50 personal selfies, allowing age regression (de-aging) and age progression (aging) with high fidelity and identity preservation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>280
</span></div><div id = 'author'>Authors:<span id = 'author'>Howard Zhang,Yuval Alaluf,Sizhuo Ma,Achuta Kadambi,Jian Wang,Kfir Aberman
</span></div><div id = 'affiliation'><span id = 'affiliation'>Snap,Tel Aviv University,Snap,University of California Los Angeles,Snap,Snap
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_307&sess=sess122">InstantRestore: Single-Step Personalized Face Restoration with Shared-Image Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>InstantRestore is a fast, personalized face restoration framework that uses a single-step diffusion model with an extended self-attention mechanism to match low-quality image patches to high-quality reference patches. Leveraging implicit correspondences in the denoising network, we efficiently transfer identity details in one pass, enabling real-time, identity-preserving restoration without per-identity tuning.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>281
</span></div><div id = 'author'>Authors:<span id = 'author'>Shaofei Wang,Tomas Simon,Igor Santesteban,Timur Bagautdinov,Junxuan Li,Vasu Agrawal,Fabian Prada,Shoou-I Yu,Pace Nalbone,Matt Gramlich,Roman Lubachersky,Chenglei Wu,Javier Romero,Jason Saragih,Michael Zollhoefer,Andreas Geiger,Siyu Tang,Shunsuke Saito
</span></div><div id = 'affiliation'><span id = 'affiliation'>ETH Zürich,Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta,Reality Labs Research, Meta,University of Tübingen,ETH Zürich,Reality Labs Research, Meta
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1137&sess=sess122">Relightable Full-Body Gaussian Codec Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the first drivable full-body avatar model that reconstructs perceptually realistic relightable appearance.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>282
</span></div><div id = 'author'>Authors:<span id = 'author'>Hendrik Junkawitsch,Guoxing Sun,Heming Zhu,Christian Theobalt,Marc Habermann
</span></div><div id = 'affiliation'><span id = 'affiliation'>Max Planck Institute for Informatics,Max Planck Institute for Informatics,Max Planck Institute for Informatics,Max Planck Institute for Informatics,Max Planck Institute for Informatics
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_651&sess=sess122">EVA: Expressive Virtual Avatars from Multi-view Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we introduce Expressive Virtual Avatars (EVA), an actor-specific, fully controllable and expressive human avatar framework that achieves high-fidelity, lifelike renderings in real-time, while enabling independent control of facial expressions, body movements, and hand gestures.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>283
</span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Shen,Tianjia Shao,Kun Zhou,Chenfanfu Jiang,Sheldon Andrews,Victor Zordan,Yin Yang
</span></div><div id = 'affiliation'><span id = 'affiliation'>Zhejiang University,Zhejiang University,Zhejiang University,UCLA,École de technologie supérieure (ÉTS),Roblox,University of Utah
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_698&sess=sess111">Elastic Locomotion with Mixed Second-order Differentiation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our framework enables realistic and interesting elastic body locomotion by determining optimal muscle activations to achieve desired movements. It combines interior-point method for contact modeling with a novel mixed second-order differentiation algorithm that merges analytic and numerical approaches, allowing Newton's method optimization to create diverse soft body animations.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>284
</span></div><div id = 'author'>Authors:<span id = 'author'>Quentin Becker,Oliver Gross,Mark Pauly
</span></div><div id = 'affiliation'><span id = 'affiliation'>EPFL,University of California San Diego,EPFL
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_840&sess=sess111">Inverse Geometric Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a computational framework for optimizing shape sequences to achieve user-defined motion objectives in deformable bodies undergoing geometric locomotion. Through a reduced spatiotemporal parameterization of the shape sequences, our method is able to efficiently capture the complex coupling between shape changes and motion in different environments.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>285
</span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Yi,Shaohua Pan,Feng Xu
</span></div><div id = 'affiliation'><span id = 'affiliation'>Tsinghua University,Tsinghua University,Tsinghua University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_130&sess=sess111">Improving Global Motion Estimation in Sparse IMU-based Motion Capture with Physics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a physics-driven approach to IMU-based motion capture, improving global motion estimation with 3D contact modeling and gravity awareness. Our method estimates world-aligned 3D motion, contact points, contact forces, joint torques, and proxy surface interactions using only six IMUs in real time.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>286
</span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wang,Taogang Hou,Tianhui Liu,Jiaxin Li,Tianmiao Wang,Hao Wang,Taogang Hou
</span></div><div id = 'affiliation'><span id = 'affiliation'>Beihang University,Beijing Jiaotong University,Beijing Jiaotong  University,Beijing Jiaotong University,Beihang University,Biehang University, Beihang University,Beijing Jiaotong  University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_115&sess=sess111">Encoded Marker Clusters for Auto-Labeling in Optical Motion Capture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Marker-based optical motion capture (MoCap) is critical for virtual production and movement sciences. We propose a novel framework for MoCap auto-labeling and matching using uniquely coded clusters of reflective markers (AEMCs). Compared to commercial software, our method achieves higher labeling accuracy for heterogeneous targets and unknown marker layouts.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>287
</span></div><div id = 'author'>Authors:<span id = 'author'>KyeongMin Kim,SeungWon Seo,DongHeun Han,HyeongYeop Kang,KyeongMin Kim
</span></div><div id = 'affiliation'><span id = 'affiliation'>Korea University,Korea University,KyungHee University,Korea University,Korea University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_120&sess=sess111">DAMO: A Deep Solver for Arbitrary Marker Configuration in Optical Motion Capture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces DAMO, a Deep solver for Arbitrary Marker configuration in Optical motion capture. DAMO directly infers the relationship between each raw marker point and 3D model joint, without using predefined marker labels and configuration information.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>288
</span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Yu,Zhe Li,Hujun Bao,Can Yang,Xiaowei Zhou
</span></div><div id = 'affiliation'><span id = 'affiliation'>Department of Mathematics, Hong Kong University of Science and Technology,Huawei,State Key Laboratory of CAD&CG, Zhejiang University,Department of Mathematics, Hong Kong University of Science and Technology,State Key Laboratory of CAD&CG, Zhejiang Univerisity
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_222&sess=sess111">HumanRAM: Feed-forward Human Reconstruction and Animation Model using Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing avatar methods typically require sophisticated dense-view capture and/or time-consuming per-subject optimization processes. HumanRAM proposes a feed-forward approach for generalizable human reconstruction and animation from monocular or sparse human images. Experiments show that HumanRAM achieves state-of-the-art results in terms of reconstruction accuracy, animation fidelity, and generalization performance.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>289
</span></div><div id = 'author'>Authors:<span id = 'author'>Wonjong Jang,Yucheol Jung,Gyeongmin Lee,Seungyong Lee
</span></div><div id = 'affiliation'><span id = 'affiliation'>POSTECH,POSTECH,POSTECH,POSTECH
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_596&sess=sess116">Instant Self-Intersection Repair for 3D Meshes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel framework that instantly (< 1 sec) repairs self-intersections in static surface meshes, which commonly occur during the 3D modeling process.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>290
</span></div><div id = 'author'>Authors:<span id = 'author'>Huibiao Wen,Guilong He,Rui Xu,Shuangmin Chen,Shiqing Xin,Zhenyu Shu,Taku Komura,Jieqing Feng,Wenping Wang,Changhe Tu
</span></div><div id = 'affiliation'><span id = 'affiliation'>Shandong University,Shandong University,University of Hong Kong,Qingdao University of Science and Technology,Shandong University,NingboTech University,University of Hong Kong,State Key Laboratory of CAD & CG, Zhejiang University,Texas A&M University,Shandong University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_612&sess=sess116">Feature-Preserving Mesh Repair via Restricted Power Diagram</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a unified mesh repair framework using a manifold wrap surface to fix diverse imperfections while preserving sharp features. By optimizing projected samples and leveraging adaptive weighting, our method ensures watertightness, manifoldness, and high geometric fidelity, outperforming existing approaches in both topology correction and feature preservation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>291
</span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Zint,Zhouyuan Chen,Yifei Zhu,Denis Zorin,Teseo Schneider,Daniele Panozzo
</span></div><div id = 'affiliation'><span id = 'affiliation'>New York University/ Courant,New York University/ Courant,New York University/ Courant,New York University/ Courant,University of Victoria,New York University/ Courant
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_348&sess=sess116">Topological Offsets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Topological Offsets is a method for generating offset surfaces that are topologically equivalent to an offset infinitesimally close to the surface. By construction, the offsets are manifold, watertight, self-intersection-free, and strictly enclose the input. Tested on Thingi10k, it supports applications like manifold extraction, layered offsets, and robust finite offset computation.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>292
</span></div><div id = 'author'>Authors:<span id = 'author'>Julian Knodt
</span></div><div id = 'affiliation'><span id = 'affiliation'>LightSpeed Studios
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_104&sess=sess116">Single Edge Collapse Quad-Dominant Mesh Reduction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A simple and robust modification to triangle mesh reduction bridges the gap for what artists want in quad-dominant mesh reduction, preserving symmetry, topology, and joints without sacrificing geometric quality, allowing for high-quality level-of-detail meshes at no cost compared to what was done before.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>293
</span></div><div id = 'author'>Authors:<span id = 'author'>Alexandre Binninger,Ruben Wiersma,Philipp Herholz,Olga Sorkine-Hornung
</span></div><div id = 'affiliation'><span id = 'affiliation'>ETH Zurich,ETH Zurich,Independent Contributor,ETH Zurich
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_493&sess=sess116">TetWeave: Isosurface Extraction using On-The-Fly Delaunay Tetrahedral Grids for Gradient-Based Mesh Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>TetWeave is a novel isosurface representation that jointly optimizes a tetrahedral grid and directional distances for gradient-based mesh processing like multi-view 3D reconstruction. It dynamically builds adaptive grids via Delaunay triangulation, ensuring watertight, manifold meshes. By resampling high-error regions and promoting fairness, it achieves high-quality results with minimal memory requirements.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>294
</span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Kohler,Martin Heistermann,David Bommes
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Bern,University of Bern,University of Bern
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1379&sess=sess116">HexHex: Highspeed Extraction of Hexahedral Meshes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present HexHex, which extracts a hexahedral mesh from a locally injective integer-grid map. Key contributions include a conservative rasterization technique and a novel mesh data structure called propeller. Our algorithm is significantly faster and uses less memory than the previous state-of-the-art method, especially for large hex-to-tet ratios.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>295
</span></div><div id = 'author'>Authors:<span id = 'author'>Timo Probst,Matthias Teschner,Timo Probst
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Freiburg,University of Freiburg,University of Freiburg
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_110&sess=sess136">Unified Pressure, Surface Tension and Friction for SPH Fluids</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a new SPH approach to replicate the behavior of droplets and other smaller scale fluid bodies. For this, we develop a new implicit surface tension formulation and implement a Coulomb friction force at the fluid-solid interface. A strong coupling between both forces and pressure is achieved through a unified solving mechanism.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>296
</span></div><div id = 'author'>Authors:<span id = 'author'>Mengyun Liu,Kai Bai,Xiaopei Liu
</span></div><div id = 'affiliation'><span id = 'affiliation'>ShanghaiTech University,ShanghaiTech University,ShanghaiTech University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_237&sess=sess136">A Hybrid Near-wall Model for Kinetic Simulation of Turbulent Boundary Layer Flows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an innovative hybrid near-wall model for the multi-resolution lattice Boltzmann solver to effectively enable simulations of high Reynolds number turbulent boundary layer flows. For the first time, it strikes an excellent balance between the precision demanded by industrial computational design and the efficiency required for various visual animations.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>297
</span></div><div id = 'author'>Authors:<span id = 'author'>Yijie Liu,Taiyuan Zhang,Xiaoxiao Yan,Nuoming Liu,Bo Ren
</span></div><div id = 'affiliation'><span id = 'affiliation'>TMCC, College of Computer Science, Nankai University,Dartmouth College,TMCC, College of Computer Science, Nankai University,TMCC, College of Computer Science, Nankai University,TMCC, College of Computer Science, Nankai University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_577&sess=sess136">Controllable Complex Freezing Dynamics Simulation on Thin Films</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a physics-based method for simulating intricate freezing dynamics on thin films. Our novel Phase Map method integrated with MELP particles reproduces Marangoni freezing dynamics and the "Snow-Globe Effect". The framework captures soap bubble freezing dynamics while ensuring stability in complex scenarios and enabling precise pattern control.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>298
</span></div><div id = 'author'>Authors:<span id = 'author'>Filipe Nascimento,Fabricio S. Sousa,Afonso Paiva
</span></div><div id = 'affiliation'><span id = 'affiliation'>Universidade de São Paulo - USP,Universidade de São Paulo - USP,Universidade de São Paulo - USP
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_564&sess=sess136">Digital Animation of Powder-Snow Avalanches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Powder-snow avalanches are natural phenomena that result from an instability in the snow cover on a mountain relief. This paper introduces a physically-based framework to simulate powder-snow avalanches under complex terrains, allowing us to animate the turbulent snow cloud dynamics within the avalanche in a visually realistic way.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>299
</span></div><div id = 'author'>Authors:<span id = 'author'>Zhanyu Yang,Aryamaan Jain,Guillaume Cordonnier,Marie-Paule Cani,Zhaopeng Wang,Bedrich Benes
</span></div><div id = 'affiliation'><span id = 'affiliation'>Purdue University,Inria, Université Côte d'Azur,Inria, Université Côte d'Azur,Centre National de la Recherche Scientifique - Laboratoire d'informatique de l'École Polytechnique (LIX),Purdue University,Purdue University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_988&sess=sess136">Arenite: A Physics-based Sandstone Simulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Arenite is a novel, physics-based simulation method for generating realistic sandstone structures. It combines fabric interlocking, multi-factor erosion, and particle-based deposition. Our GPU-based implementation produces detailed 3D shapes such as arches, alcoves, hoodoos, and buttes in minutes and provides real-time control.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>300
</span></div><div id = 'author'>Authors:<span id = 'author'>Michael Liu,Xinlei Wang,Minchen Li
</span></div><div id = 'affiliation'><span id = 'affiliation'>Carnegie Mellon University,NetEase Games Messiah Engine,Carnegie Mellon University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_332&sess=sess136">CK-MPM: A Compact-Kernel Material Point Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a compact, C2-continuous kernel for MPM that reduces numerical diffusion and improves efficiency—without sacrificing stability. Built on a dual-grid framework and compatible with APIC and MLS, our method enables high-fidelity, large-scale simulations, further pushing the limits of MPM.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>301
</span></div><div id = 'author'>Authors:<span id = 'author'>Yuanpeng Tu,Luo Hao,Chen Xi,Sihui Ji,Xiang Bai,Zhao Hengshuang
</span></div><div id = 'affiliation'><span id = 'affiliation'>The University of Hong Kong,DAMO Academy, Alibaba Group,The University of Hong Kong,The University of Hong Kong,Huazhong University of Science and Technology,The University of Hong Kong
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_445&sess=sess141">VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose VideoAnydoor, a zero-shot video object insertion framework with high-fidelity detail preservation and precise motion control, where a pixel warper and a image-video mix-training strategy are designed to warp the pixel details according to the trajectories. VideoAnydoor demonstrates significant superiority over existing methods and naturally supports various downstream applications.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>302
</span></div><div id = 'author'>Authors:<span id = 'author'>Feng-Lin Liu,Shi-Yang Li,Yan-Pei Cao,Hongbo Fu,Lin Gao
</span></div><div id = 'affiliation'><span id = 'affiliation'>Institute of Computing Technology, Chinese Academy of Sciences,Institute of Computing Technology, Chinese Academy of Sciences,VAST,Hong Kong University of Science and Technology,Institute of Computing Technology, Chinese Academy of Sciences
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_296&sess=sess141">Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Sketch3DVE, a sketch-based 3D-aware video editing method to enable detailed local manipulation of videos with significant viewpoint changes. Our approach leverages detailed analysis and editing of underlying 3D scene representations, combined with a diffusion model to synthesize realistic and temporally coherent edited videos.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>303
</span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Bian,Zhaoyang Zhang,Xuan Ju,Mingdeng Cao,Liangbin Xie,Ying Shan,Qiang Xu
</span></div><div id = 'affiliation'><span id = 'affiliation'>The Chinese University of Hong Kong,Tencent,The Chinese University of Hong Kong,The University of Tokyo,University of Macau,Tencent,The Chinese University of Hong Kong
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_626&sess=sess141">VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>VideoPainter introduces a dual-branch framework for video inpainting with a lightweight context encoder that integrates with pre-trained diffusion transformers. Its ID resampling strategy maintains identity consistency across any-length videos, while VPData and VPBench provide the largest segmentation-mask dataset with captions. The system achieves state-of-the-art performance in video inpainting and editing.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>304
</span></div><div id = 'author'>Authors:<span id = 'author'>Hongbo Zhao,Jiaxing Li,Peiyi Zhang,Peng Xiao,Jianxin Lin,Yijun Wang
</span></div><div id = 'affiliation'><span id = 'affiliation'>Hunan University,Hunan University,Hunan University,Hunan University,Hunan University,Hunan University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1108&sess=sess141">ColorSurge: Bringing Vibrancy and Efficiency to Automatic Video Colorization via Dual-Branch Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose ColorSurge, a lightweight dual-branch network for end-to-end video colorization. It delivers vivid, accurate, and real-time results from grayscale input, and is easily extensible for high-quality performance at low computational cost.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>305
</span></div><div id = 'author'>Authors:<span id = 'author'>Karlis Martins Briedis,Abdelaziz Djelouah,Raphaël Ortiz,Markus Gross,Christopher Schroers
</span></div><div id = 'affiliation'><span id = 'affiliation'>DisneyResearch|Studios,DisneyResearch|Studios,DisneyResearch|Studios,DisneyResearch|Studios,DisneyResearch|Studios
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_191&sess=sess141">Controllable Tracking-Based Video Frame Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a tracking-based video frame interpolation method, optionally guided by user inputs. It utilizes sparse point tracks, first estimated using existing point tracking methods and then optionally refined by the user. Without any user input, it already achieves state-of-the-art results, with further significant improvements possible through user interactions.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>306
</span></div><div id = 'author'>Authors:<span id = 'author'>Manuel Kansy,Jacek Naruniec,Christopher Schroers,Markus Gross,Romann Weber
</span></div><div id = 'affiliation'><span id = 'affiliation'>ETH Zürich,Disney Research Studios,Disney Research Studios,ETH Zürich,Disney Research Studios
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_591&sess=sess141">Reenact Anything: Semantic Video Motion Transfer Using Motion-Textual Inversion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reenact Anything introduces a unified framework for semantic motion transfer, covering applications from full-body and face reenactment to controlling the motion of inanimate objects and the camera. Thereby, motions are represented using text/image embeddings of an image-to-video diffusion model and are optimized based on a given motion reference video.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>307
</span></div><div id = 'author'>Authors:<span id = 'author'>Bosheng Li,Nikolas Schwarz,Wojtek Palubicki,Sören Pirk,Dominik L. Michels,Bedrich Benes
</span></div><div id = 'affiliation'><span id = 'affiliation'>Purdue University,Kiel University,AMU,Kiel University,King Abdullah University of Science and Technology (KAUST),Purdue University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1219&sess=sess158">Stressful Tree Modeling: Breaking Branches with Strands</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A novel approach for the computational modeling of lignified tissues, such as those found in tree branches and timber, extends strand-based representation to describe biophysical processes at short and long time scales. The computationally fast simulation leverages Cosserat rod physics and enables the interactive exploration of branches and wood breaking.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>308
</span></div><div id = 'author'>Authors:<span id = 'author'>Hao Xu,Yinqiao Wang,Niloy Mitra,Shuaicheng Liu,Pheng Ann Heng,Chi-Wing Fu
</span></div><div id = 'affiliation'><span id = 'affiliation'>The Chinese University of Hong Kong,The Chinese University of Hong Kong,University College London (UCL),University of Electronic Science and Technology of China,Chinese University of Hong Kong,The Chinese University of Hong Kong
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_329&sess=sess158">Hand-Shadow Poser</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We solve an inverse hand-shadow problem: finding poses of left and right hands that together produce a shadow resembling the target 2D input, e.g., animals, letters, and everyday objects. Our three-stage pipeline decouples the anatomical constraints and semantic constraints, and our benchmark provides 210 diverse shadow shapes of varying complexity.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>309
</span></div><div id = 'author'>Authors:<span id = 'author'>Rahul Mitra,Mattéo Couplet,Tongtong Wang,Megan Hoffman,Kui Wu,Edward Chien
</span></div><div id = 'affiliation'><span id = 'affiliation'>Boston University,Boston University,LightSpeed Studios,Northeastern University,LightSpeed Studios,Boston University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_906&sess=sess158">Curl Quantization for Automatic Placement of Knit Singularities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a method for the automatic placement of knit singularities based on curl quantization. Our method generates knit graphs that maintain all structural manufacturing constraints as well as any additional user constraints. This approach allows for simulation-free previews of rendered knits and also extends to the popular cut-and-sew setting.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>310
</span></div><div id = 'author'>Authors:<span id = 'author'>Klara Mundilova,Michele Vidulis,Quentin Becker,Florin Isvoranu,Mark Pauly
</span></div><div id = 'affiliation'><span id = 'affiliation'>EPFL,EPFL,EPFL,EPFL,EPFL
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1261&sess=sess158">C-Tubes: Design of Tubular Structures From Developable Strips</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>C-tubes are 3D tubular structures made of developable strips. We introduce an algorithm to construct C-tubes while guaranteeing exact surface developability and an optimization method for design exploration. Applications span architecture, engineering, and product design. We present prototypes showcasing cost-effective fabrication of complex geometries using different materials.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>311
</span></div><div id = 'author'>Authors:<span id = 'author'>Fanchao Zhong,Yang Wang,Peng-Shuai Wang,Lin Lu,Haisen Zhao
</span></div><div id = 'affiliation'><span id = 'affiliation'>Shandong University,Shandong University,Peking University,Shandong University,Shandong University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_512&sess=sess158">DeepMill: Neural Accessibility Learning for Subtractive Manufacturing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proposed neural network, DeepMill, can efficiently predict inaccessible and occlusion regions in subtractive manufacturing. By utilizing a cutter-aware dual-head octree-based convolutional architecture, it overcomes the computational inefficiency of traditional geometric methods and is capable of real-time prediction of inaccessible and occlusion regions during the 3D shape design phase.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>312
</span></div><div id = 'author'>Authors:<span id = 'author'>Weizheng Zhang,Hao Pan,Lin Lu,Xiaowei Duan,Xin Yan,Ruonan Wang,Qiang Du
</span></div><div id = 'affiliation'><span id = 'affiliation'>Shandong University,School of Software, Tsinghua University,Shandong University,Shandong University,Shandong University,Institute of Engineering Thermophysics, Chinese Academy of Sciences,Institute of Engineering Thermophysics, Chinese Academy of Sciences
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_790&sess=sess158">DualMS: Implicit Dual-Channel Minimal Surface Optimization for Heat Exchanger Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DualMS is a novel framework for designing high-performance heat exchangers by directly optimizing the separation surface of two fluids using dual skeleton optimization and neural implicit functions. It offers greater topological flexibility than TPMS and achieves superior thermal performance with lower pressure drop while maintaining comparable heat exchange rates.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>313
</span></div><div id = 'author'>Authors:<span id = 'author'>Zilin Xu,Xiang Chen,Chen Liu,Beibei Wang,Lu Wang,Zahra Montazeri,Ling-Qi Yan
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of California Santa Barbara,Shandong University,Zhejiang Lingdi Digital Technology Co.,Ltd,Nanjing University,Shandong University,University of Manchester,University of California Santa Barbara
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_303&sess=sess123">Towards Comprehensive Neural Materials: Dynamic Structure-Preserving Synthesis with Accurate Silhouette at Instant Inference Speed</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We challenge the comprehensive neural material representation by thoroughly considering the essential aspects of the complete appearance. We introduce an int8-quantized model that keeps high fidelity while achieving an order of magnitude speedup compared to previous methods, and a controllable structure-preserving synthesis strategy, along with accurate displacement effects.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>314
</span></div><div id = 'author'>Authors:<span id = 'author'>Nithin Raghavan,Krishna Mullia,Alexander Trevithick,Fujun Luan,Miloš Hašan,Ravi Ramamoorthi
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of California San Diego,Adobe Research,University of California San Diego,Adobe Research,Adobe Research,University of California San Diego
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1249&sess=sess123">Generative Neural Materials</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the first generative model for neural BTFs, enabling single-shot generation from arbitrary text or image prompts. To achieve this, we introduce a universal neural material basis and train a conditional diffusion model to generate materials in this basis from flash images, natural images and text prompts.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>315
</span></div><div id = 'author'>Authors:<span id = 'author'>Liwen Wu,Sai Bi,Zexiang Xu,Hao Tan,Kai Zhang,Fujun Luan,Haolin Lu,Ravi Ramamoorthi
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of California San Diego,Adobe Research,Hillbot,Adobe Research,Adobe Research,Adobe Research,Max Planck Institute for Informatics,University of California San Diego
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_665&sess=sess123">Neural BRDF Importance Sampling by Reparameterization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a reparameterization-based formulation of neural BRDF importance sampling. Comparing to previous methods that construct a probability transform to the BRDF through multi-step invertible neural networks, our BRDF sampling is in single step without needing network invertibility, achieving higher inference speed with the best variance reduction.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>316
</span></div><div id = 'author'>Authors:<span id = 'author'>Krishna Mullia,Fujun Luan,Xin Sun,Miloš Hašan,Krishna Mullia
</span></div><div id = 'affiliation'><span id = 'affiliation'>Adobe Research,Adobe Research,Adobe Research,Adobe Research,Adobe Research
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_114&sess=sess123">RNA: Relightable Neural Assets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a neural representation for 3D assets with complex shading. We precompute shading and scattering on ground-truth geometry, enabling high-fidelity rendering with full relightability, eliminating complex shading models and multiple scattering paths, offering significant speed-ups and seamless integration into existing rendering pipelines.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>317
</span></div><div id = 'author'>Authors:<span id = 'author'>Saeed Hadadan,Benedikt Bitterli,Tizian Zeltner,Jan Novák,Fabrice Rousselle,Jacob Munkberg,Jon Hasselgren,Bartlomiej Wronski,Matthias Zwicker
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Maryland College Park,NVIDIA Research,NVIDIA Research,NVIDIA Research,NVIDIA Research,NVIDIA Research,NVIDIA Research,NVIDIA Research,University of Maryland College Park
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1278&sess=sess123">Generative detail enhancement for physically based materials</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a tool for enhancing the detail of physically based materials using an off-the-shelf diffusion model and inverse rendering. Our goal is to enhance the visual fidelity of materials with detail that is often tedious to author, by adding signs of wear, aging, weathering, etc.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>318
</span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhou,Tao Huang,Ravi Ramamoorthi,Pradeep Sen,Ling-Qi Yan,Ling-Qi Yan
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of California Santa Barbara,University of California Santa Barbara,University of California San Diego,University of California Santa Barbara,University of California Santa Barbara,University of California Santa Barbara
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_118&sess=sess123">Appearance-Preserving Scene Aggregation for Level-of-Detail Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel volumetric representation for the aggregated appearance of complex scenes and a pipeline for level-of-detail generation and rendering. Our representation preserves accurate far-field appearance and spatial correlation from scene geometry. Our method faithfully reproduces appearance and achieves higher quality than existing scene filtering methods.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>319
</span></div><div id = 'author'>Authors:<span id = 'author'>Hossein Baktash,Nicholas Sharp,Qingnan Zhou,Alec Jacobson,Keenan Crane
</span></div><div id = 'affiliation'><span id = 'affiliation'>Carnegie Mellon University,NVIDIA Research,Adobe Research,Adobe Research,Carnegie Mellon University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1035&sess=sess135">Putting Rigid Bodies to Rest</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We identify stable orientations of any rigid shape, and the probability that it will rest at these orientations if randomly dropped on the ground. We use a differentiable inverse version of our method to design and fabricate shapes with target resting behavior, such as dice with target, nonuniform probabilities.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>320
</span></div><div id = 'author'>Authors:<span id = 'author'>Moritz Bächer,Ruben Grandia,Espen Knoop,Guirec Maloisel,Christian Schumacher
</span></div><div id = 'affiliation'><span id = 'affiliation'>Disney Research,Disney Research,Disney Research,Disney Research,Disney Research
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_633&sess=sess135">A Versatile Quaternion-Based Constrained Rigid Body Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an implicitly-integrated, quaternion-based constrained Rigid Body Dynamics (RBD) that guarantees satisfaction of kinematic constraints, unifying the solution strategy for complex mechanical systems with arbitrary kinematic structures, by navigating subspaces spanned by constraint forces and torques for systems with redundant constraints, over actuation, and passive degrees of freedom.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>321
</span></div><div id = 'author'>Authors:<span id = 'author'>Magí Romanyà,Miguel A. Otaduy
</span></div><div id = 'affiliation'><span id = 'affiliation'>Universidad Rey Juan Carlos,Universidad Rey Juan Carlos
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1511&sess=sess135">Painless Differentiable Rotation Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work introduces a forward and differentiable rigid-body dynamics framework using Lie-algebra rotation derivatives. The approach offers simplified, compact derivatives, improved conditioning, and higher efficiency compared to traditional methods. Applications include fundamental rigid-body problems and Cosserat rods, showcasing its potential for multi-rigid-body dynamics and incremental-potential formulations.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>322
</span></div><div id = 'author'>Authors:<span id = 'author'>Zizhou Huang,Maxwell Paik,Zachary Ferguson,Daniele Panozzo,Denis Zorin
</span></div><div id = 'affiliation'><span id = 'affiliation'>New York University,New York University,Massachusetts Institute of Technology,New York University,New York University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_103&sess=sess135">Geometric Contact Potential</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a systematic derivation of a continuum potential defined for smooth and piecewise smooth surfaces, by identifying a set of natural requirements for contact potentials. Our potential is formulated independently of surface discretization and addresses the shortcomings of existing potential-based methods while retaining their advantages.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>323
</span></div><div id = 'author'>Authors:<span id = 'author'>Xiaodi Yuan,Fanbo Xiang,Yin Yang,Hao Su
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of California San Diego,Hillbot Inc.,University of Utah,University of California San Diego
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_256&sess=sess135">C5D: Sequential Continuous Convex Collision Detection Using Cone Casting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a fast, single-threaded continuous collision detection (CCD) algorithm for convex shapes under affine motion. By combining conservative advancement with a cone-casting approach, it avoids primitive-level overhead and enables efficient integration into intersection-free simulation methods such as ABD.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>324
</span></div><div id = 'author'>Authors:<span id = 'author'>Anka H. Chen,Jerry Hsu,Ziheng Liu,Miles Macklin,Yin Yang,Cem Yuksel
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Utah,University of Utah,University of Utah,NVIDIA,University of Utah,University of Utah
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1039&sess=sess135">Offset Geometric Contact</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Offset Geometric Contact (OGC), a groundbreaking method offering "penetration-free for free" simulations of codimensional objects.  OGC efficiently constructs offset volumetric shapes to ensure stable, artifact-free collisions. Leveraging parallel GPU computations, it delivers real-time simulations at speeds over 100× faster than previous methods, eliminating costly collision detection and global-synchronization.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>325
</span></div><div id = 'author'>Authors:<span id = 'author'>Nuri Ryu,Jiyun Won,Jooeun Son,Minsu Gong,Joo-Haeng Lee,Sunghyun Cho
</span></div><div id = 'affiliation'><span id = 'affiliation'>POSTECH,POSTECH,POSTECH,POSTECH,Pebblous,POSTECH
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_807&sess=sess130">Elevating 3D Models: High-Quality Texture and Geometry Refinement from a Low-Quality Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Elevate3D transforms low-quality 3D models into high-quality assets through iterative texture and geometry refinement. At its core, HFS-SDEdit refines textures generatively while preserving the input’s identity leveraging high-frequency guidance. The resulting texture then guides geometry refinement, allowing Elevate3D to deliver high-quality results with well-aligned texture and geometry.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>326
</span></div><div id = 'author'>Authors:<span id = 'author'>Julian Knodt,Xifeng Gao,Julian Knodt
</span></div><div id = 'affiliation'><span id = 'affiliation'>LightSpeed Studios,LightSpeed Studios,Lightspeed Studios, Tencent America
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_108&sess=sess130">Texture Size Reduction Through Symmetric Overlap and Texture Carving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We develop a method to compress textures and UVs for meshes in a content-aware way. We combine this with overlapping and folding symmetric UV charts, and demonstrate our approach on a dataset from Sketchfab. We outperform prior work in visual similarity to the original mesh.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>327
</span></div><div id = 'author'>Authors:<span id = 'author'>Yuqing Zhang,Hao Xu,Yiqian Wu,Sirui Chen,Sirui Lin,Xiang Li,Xifeng Gao,Xiaogang Jin
</span></div><div id = 'affiliation'><span id = 'affiliation'>Zhejiang University,Zhejiang University,Zhejiang University,Zhejiang University,Zhejiang University,Shenzhen University,Lightspeed Studios, Tencent America,Zhejiang University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_402&sess=sess130">AlignTex: Pixel-Precise Texture Generation from Multi-view Artwork</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AlignTex is a novel framework for generating high-quality textures from 3D meshes and multi-view artwork. It improves texture generation by ensuring both appearance detail and geometric consistency, outpacing traditional methods in quality and efficiency, making it a valuable tool for 3D asset creation in gaming and film production.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>328
</span></div><div id = 'author'>Authors:<span id = 'author'>Maria Larsson,Hodaka Yamaguchi,Ehsan Pajouheshgar,I-Chao Shen,Kenji Tojo,Chia-Ming Chang,Lars Hansson,Olof Broman,Takashi Ijiri,Ariel Shamir,Wenzel Jakob,Takeo Igarashi
</span></div><div id = 'affiliation'><span id = 'affiliation'>The University of Tokyo,Gifu Prefecture Research Institute for Human Life Technology,École Polytechnique Féderale de Lausanne (EPFL),The University of Tokyo,The University of Tokyo,National Taiwan University of Arts,Luleå University of Technology,Luleå University of Technology,Shibaura Institute of Technology,Reichman University,The University of Tokyo,The University of Tokyo
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_650&sess=sess130">The Mokume Dataset and Inverse Modeling of Solid Wood Textures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the Mokume dataset for solid wood texturing, comprising nearly 190 samples from various species. Using this dataset, we propose an inverse modeling pipeline to infer volumetric wood textures from surface photographs, employing inverse procedural texturing and neural cellular automata (NCA).
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>329
</span></div><div id = 'author'>Authors:<span id = 'author'>Crane He Chen,Vladimir Kim
</span></div><div id = 'affiliation'><span id = 'affiliation'>Industrial Light & Magic,Adobe
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_677&sess=sess130">Escher Tile Deformation via Closed-Form Solution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A real-time deformation method for Escher tiles --- interlocking organic forms that seamlessly tessellate the plane following symmetry rules. Rather than treating tiles as mere boundaries, we consider them as textured shapes, ensuring that both the boundary and interior deform simultaneously. The deformation is achieved via a closed-form solution.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>330
</span></div><div id = 'author'>Authors:<span id = 'author'>Marco Maida,Alberto Crescini,Marco Perronet,Elena Camuffo
</span></div><div id = 'affiliation'><span id = 'affiliation'>Independent Researcher,Independent Researcher,Independent Researcher,Independent Researcher
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_503&sess=sess130">Claycode: Stylable and Deformable 2D Scannable Codes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel scannable 2D code where the payload is stored in the topology of nested color regions, abandoning traditional matrix-based approaches (e.g., QRCodes). Claycodes can be largely deformed, styled, and animated. We present a mapping between bits and topologies, shape-constrained rendering, and a robust real-time decoding pipeline.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>331
</span></div><div id = 'author'>Authors:<span id = 'author'>Cédric Zanni,Cédric Zanni
</span></div><div id = 'affiliation'><span id = 'affiliation'>Université de Lorraine CNRS, Inria, LORIA,Université de Lorraine, CNRS, Inria, LORIA, F-54000 Nancy, France; LORIA
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_121&sess=sess133">Synchronized tracing of primitive-based implicit volumes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The paper presents a tile-based rendering pipeline for modeling with implicit volumes, using blobtrees and smooth CSG operators. It requires no preprocessing when updating primitives and ensures efficient ray processing with sphere tracing. The method uses a low-resolution A-buffer and bottom-up tree traversal for scalable performance.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>332
</span></div><div id = 'author'>Authors:<span id = 'author'>Kechun Wang,Renjie Chen
</span></div><div id = 'affiliation'><span id = 'affiliation'>University of Science and Technology of China,University of Science and Technology of China
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_526&sess=sess133">PaRas: A Rasterizer for Large-Scale Parametric Surfaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Higher-order surfaces enable compact, smooth geometry but require efficient rendering. We introduce PaRas, a GPU-based rasterizer that directly renders parametric surfaces, avoiding costly tessellation. It integrates seamlessly into existing pipelines, outperforming traditional methods for quartic triangular and bicubic rational Bézier patches. Experimental results confirm its superior efficiency and accuracy.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>333
</span></div><div id = 'author'>Authors:<span id = 'author'>Pengfei Zhu,Jie Guo,Yifan Liu,Qi Sun,Yanxiang Wang,Keheng Xu,Ligang Liu,Yanwen Guo
</span></div><div id = 'affiliation'><span id = 'affiliation'>Nanjing University,Nanjing University,Nanjing University,Nanjing University,Nanjing University,Nanjing University,University of Science and Technology of China,Nanjing University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_924&sess=sess133">Appearance-aware Multi-view SVBRDF Reconstruction via Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces an appearance-aware adaptive sampling method using deep reinforcement learning to optimize the reconstruction of spatially-varying BRDFs from minimal images. By modeling the sampling as a sequential decision-making problem, the method identifies the next best view-lighting pair, outperforming heuristic sampling strategies for heterogeneous materials.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>334
</span></div><div id = 'author'>Authors:<span id = 'author'>Ruben Wiersma,Julien Philip,Miloš Hašan,Krishna Mullia,Fujun Luan,Elmar Eisemann,Valentin Deschaintre
</span></div><div id = 'affiliation'><span id = 'affiliation'>ETH Zürich,Netflix Eyeline Studios,Adobe Research,Adobe Research,Adobe Research,Delft University of Technology,Adobe Research
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_129&sess=sess133">Uncertainty for SVBRDF Acquisition using Frequency Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We quantify uncertainty for SVBRDF acquisition from multi-view captures using entropy. The otherwise heavy computation is accelerated in the frequency domain, yielding a practical, efficient method. We apply uncertainty to improve SVBRDF capture by guiding camera placement, inpainting uncertain regions, and sharing information from certain regions on the object.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>335
</span></div><div id = 'author'>Authors:<span id = 'author'>Hangming Fan,Yuchi Huo,Chuankun Zheng,Chonghao Hu,Yazhen Yuan,Rui Wang
</span></div><div id = 'affiliation'><span id = 'affiliation'>State Key Laboratory of CAD & CG, Zhejiang University,State Key Laboratory of CAD & CG, Zhejiang University,State Key Laboratory of CAD & CG, Zhejiang University,State Key Laboratory of CAD & CG, Zhejiang University,Game Engine Department, CROS, Tencent, China,State Key Laboratory of CAD & CG, Zhejiang University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_686&sess=sess133">Streaming-Aware Neural Monte Carlo Rendering Framework with Unified Denoising-Compression and Client Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To reduce the high rendering costs and transmission bandwidth requirements of path tracing-based cloud rendering, we propose a novel streaming-aware rendering framework that is able to learn a joint optimal model integrating two path-tracing acceleration (adaptive sampling and denoising) and video compression technique with client side G-buffer collaboration.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>336
</span></div><div id = 'author'>Authors:<span id = 'author'>Rachel McDonnell,Bharat Vyas,Uros Sikimic,Pisut Wisessing
</span></div><div id = 'affiliation'><span id = 'affiliation'>Trinity College Dublin,Trinity College Dublin,Epic Games,CMKL University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1040&sess=sess133">Feeling Blue or Seeing Red? Investigating the effect of light color, shadow and realism on the perception of emotion of real and virtual humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study explores how light color influences the perception of emotion of virtual characters. By analyzing various lighting conditions, including red and blue hues, we reveal how light affects emotion intensity, recognition, and genuineness. Findings show that lighting, realism, and shadows are key factors in enhancing the emotional impact.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>337
</span></div><div id = 'author'>Authors:<span id = 'author'>Jingwen Ye,Yuze He,Yanning Zhou,Yiqin Zhu,Kaiwen Xiao,Yong-Jin Liu,Wei Yang,Xiao Han
</span></div><div id = 'affiliation'><span id = 'affiliation'>Tencent AIPD,Tencent AIPD,Tencent AIPD,Tencent AIPD,Tencent AIPD,Tsinghua University,Tencent AIPD,Tencent AIPD
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_1074&sess=sess132">PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with Auto-Regressive transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PrimitiveAnything, a novel framework that reformulates shape primitive abstraction as a primitive assembly generation task. PrimitiveAnything can generate 3D high-quality primitive assemblies that better align with human perception while maintaining geometric fidelity across diverse shape categories, which benefits various 3D applications.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>338
</span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Wang,Min Lu
</span></div><div id = 'affiliation'><span id = 'affiliation'>Shenzhen University,Shenzhen University
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_740&sess=sess132">Image-Space Collage and Packing with Differentiable Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work introduces an efficient image-space collage technique that optimizes geometric layouts using a differential renderer and hierarchical resolution strategy. Our approach simplifies complex shape handling in image-space optimization, offering fixed computational complexity. Experiments show our method is an order of magnitude faster than state-of-the-art while supporting diverse visual expressions.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>339
</span></div><div id = 'author'>Authors:<span id = 'author'>Junming Huang,Chi Wang,Letian Li,Changxin Huang,Qiang Dai,Weiwei Xu
</span></div><div id = 'affiliation'><span id = 'affiliation'>State Key Laboratory of CAD & CG, Zhejiang University,State Key Laboratory of CAD & CG, Zhejiang University,State Key Laboratory of CAD & CG, Zhejiang University,State Key Laboratory of CAD & CG, Zhejiang University,LIGHTSPEED,State Key Lab CAD&CG, Zhejiang University, ZJU-Tencent Game and Intelligent Graphics Innovation Technology Joint Lab
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_828&sess=sess132">BuildingBlock: A Hybrid Approach for Structured Building Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose BuildingBlock, a hybrid approach integrating generative models, PCG, and LLMs for diverse and structured 3D building generation. A Transformer-based diffusion model generates layouts, which LLMs refine into hierarchical designs. PCG then constructs high-quality buildings, achieving state-of-the-art results and enabling scalable architectural workflows.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>340
</span></div><div id = 'author'>Authors:<span id = 'author'>Jiepeng Wang,Hao Pan,Yang Liu,Xin Tong,Taku Komura,Wenping Wang,Jiepeng Wang
</span></div><div id = 'affiliation'><span id = 'affiliation'>The University of Hong Kong,Microsoft Research Asia,Microsoft Research Asia,Microsoft Research Asia,The University of Hong Kong,Texas A&M University,University of Hong Kong, Microsoft Research Asia
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=paperstog_116&sess=sess132">StructRe: Rewriting for Structured Shape Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The paper presents StructRe, a structure rewriting system for 3D shape modeling. It uses an iterative process to rewrite objects, either upwards to more concise structures or downwards to more detailed ones, generating hierarchies. This localized rewriting approach enables probabilistic modeling of ambiguous structures and robust generalization across object categories.
</p></br></br><p id = 'section'><div id = 'pid'>Paperid:<span id = 'pid'>341
</span></div><div id = 'author'>Authors:<span id = 'author'>Davide Sforza,Marzia Riso,Filippo Muzzini,Nicola Capodieci,Fabio Pellacini
</span></div><div id = 'affiliation'><span id = 'affiliation'>Sapienza University of Rome,Sapienza University of Rome,University of Modena and Reggio Emilia,University of Modena and Reggio Emilia,University of Modena and Reggio Emilia
</span></div><div id="title">Title: <a href="https://s2025.conference-schedule.org/?post_type=page&p=14&id=papers_588&sess=sess132">Interactive Optimization of Scaffolded Procedural Patterns</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a method for interactive design of procedural patterns, allowing users to sketch content incrementally in a level-by-level fashion. Each level, or scaffold, builds on the previous one, making optimization more responsive and controllable. A comprehensive validation demonstrates improved editing experience compared to conventional techniques.
